{"config":{"indexing":"full","lang":["es"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apuntes y pr\u00e1cticas del m\u00f3dulo Despliegue de aplicaciones web de 2\u00ba de DAW Introducci\u00f3n Arquitectura web - Implantaci\u00f3n y admnistraci\u00f3n de servidores web Servidores de aplicaciones Servicios de red implicados en el despliegue de aplicaciones web Control de versiones Contenedores CI/CD (Integraci\u00f3n y despliegue continuo)","title":"Home"},{"location":"ArqWeb/","text":"Arquitectura Web. Implantaci\u00f3n y administraci\u00f3n de servidores web Introducci\u00f3n Con la evoluci\u00f3n y el acceso libre a Internet, uno de los principales alicientes que han surgido es la publicaci\u00f3n de p\u00e1ginas web donde se pueden almacenar unos contenidos bastante atractivos para nosotros y que, al mismo tiempo, pueden ser consultados desde cualquier del mundo para todos. Cabe decir que, con la popularizaci\u00f3n de Internet, tanto empresas como usuarios han visto la necesidad de establecer un punto desde donde anunciar sus productos, o bien, a t\u00edtulo particular, dar publicidad a las aficiones o capacidades personales mediante la publicaci\u00f3n de p\u00e1ginas web. Las p\u00e1ginas web, en su mayor\u00eda en formato HTML, requieren ser alojadas en m\u00e1quinas que dispongan de espacio en disco para almacenar archivos HTML, im\u00e1genes, bloques de c\u00f3digo o archivos de v\u00eddeo en directorios espec\u00edficos y, al mismo tiempo, deben ser capaces de entender todo tipo de extensi\u00f3n de los archivos que son enviados en ambos sentidos de la comunicaci\u00f3n. Paralelamente, no podemos dejar de lado la importancia de las medidas de seguridad ante los peligros existentes en Internet. Para ello, las p\u00e1ginas deber\u00e1n estar dise\u00f1adas considerando la incorporaci\u00f3n de protocolos de comunicaci\u00f3n seguros como, por ejemplo, los desarrollados con el protocolo seguro de transferencia de hipertexto (HTTPS, Hyper Text Transfer Protocol secure) que utilizan claves y estrategias de cifrado propias de las herramientas del protocolo de capa de conexi\u00f3n segura (SSL, secure sockets layer). Las m\u00e1quinas que alojan las p\u00e1ginas web reciben la categor\u00eda de servidores web. Desde el punto de vista de los servidores, los requerimientos m\u00e1s relevantes son el espacio de disco necesario para poder almacenar la estructura de la p\u00e1gina web y una buena conexi\u00f3n de red para que el consumo de la unidad de procesamiento central (CPU, central processing unit ) sea bastante bajo. El funcionamiento de los servidores web es especial ya que, como si se tratara de un diente de sierra, tienen consumos de recursos puntuales porque podemos estar un tiempo sin peticiones y, de repente, tener una avalancha de peticiones. Esto hace que los servidores web suelan tener un n\u00famero bajo de procesos en espera. A medida que resultan necesarios, se van arrancando nuevos. Cabe decir que no todas las peticiones consumen el mismo, y, por ejemplo, aquellas p\u00e1ginas web que ejecuten programas de interacci\u00f3n con el usuario o requieran cifrado (HTTPS) consumen m\u00e1s recursos que otras p\u00e1ginas web con menos interacci\u00f3n. \u00bfQu\u00e9 es un servidor web? Los servidores web sirven para almacenar contenidos de Internet y facilitar su disponibilidad de forma constante y segura. Cuando visitas una p\u00e1gina web desde tu navegador, es en realidad un servidor web el que env\u00eda los componentes individuales de dicha p\u00e1gina directamente a tu ordenador. Esto quiere decir que para que una p\u00e1gina web sea accesible en cualquier momento, el servidor web debe estar permanentemente online. Toda p\u00e1gina accesible en Internet necesita un servidor especial para sus contenidos web. A menudo, las grandes empresas y organizaciones cuentan con un servidor web propio para disponer sus contenidos en Intranet e Internet. Sin embargo, la mayor\u00eda de administradores recurren a los centros de datos de proveedores de alojamiento web para sus proyectos. Independientemente de si tienes un servidor web propio o de si alquilas uno externo, siempre necesitar\u00e1s un software para gestionar los datos de tu p\u00e1gina y mantenerla actualizada. En este sentido, tienes la posibilidad de elegir entre varias soluciones de software para servidores web dise\u00f1adas para diferentes aplicaciones y sistemas operativos. Tecnolog\u00eda de servidores web Principalmente, el software de un servidor HTTP es el encargado de proporcionar los datos para la visualizaci\u00f3n del contenido web. Para abrir una p\u00e1gina web, el usuario solo tiene que escribir el URL correspondiente en la barra de direcciones de su navegador web. El navegador env\u00eda una solicitud al servidor web, quien responde, por ejemplo, entregando una p\u00e1gina HTML. Esta puede estar alojada como un documento est\u00e1tico en el host o ser generada de forma din\u00e1mica, lo que significa que el servidor web tiene que ejecutar un c\u00f3digo de programa (p. ej., Java o PHP) antes de tramitar su respuesta. El navegador interpreta la respuesta, lo que suele generar autom\u00e1ticamente m\u00e1s solicitudes al servidor a prop\u00f3sito de, por ejemplo, im\u00e1genes integradas o archivos CSS (hojas de estilos). El protocolo utilizado para la transmisi\u00f3n es HTTP (o su variante cifrada HTTPS), que se basa, a su vez, en los protocolos de red IP y TCP (y muy rara vez en UDP). Un servidor web puede entregar los contenidos simult\u00e1neamente a varios ordenadores o navegadores web. La cantidad de solicitudes (requests) y la velocidad con la que pueden ser procesadas depende, entre otras cosas, del hardware y la carga (n\u00famero de solicitudes) del host. Sin embargo, la complejidad del contenido tambi\u00e9n juega un papel importante: los contenidos web din\u00e1micos necesitan m\u00e1s recursos que los contenidos est\u00e1ticos. La selecci\u00f3n del equipo adecuado para el servidor y la decisi\u00f3n de si este debe ser dedicado, virtual o en la nube, se debe hacer pensando siempre en evitar sobrecargas en el servidor. Aunque se haya encontrado un servidor web que se adapta perfectamente a las necesidades del proyecto, siempre se corre el riesgo de que se presenten fallos en \u00e9l como consecuencia de imprecisiones t\u00e9cnicas o cortes de energ\u00eda en el centro de datos del host. Aunque no es muy frecuente, durante un per\u00edodo de inactividad de este tipo (downtime), la web no estar\u00e1 disponible. Otras funciones de los servidores web Aunque su principal funci\u00f3n es la transferencia de contenido web, muchos programas de servidor web ofrecen caracter\u00edsticas adicionales: Seguridad Cifrado de la comunicaci\u00f3n entre el servidor web y el cliente v\u00eda HTTPS Autenticaci\u00f3n del usuario Autenticaci\u00f3n HTTP para \u00e1reas espec\u00edficas de una aplicaci\u00f3n web Redirecci\u00f3n Redirecci\u00f3n de una solicitud de documento por medio de Rewrite Engine Redirecci\u00f3n Almacenamiento en cach\u00e9 de documentos din\u00e1micos para la respuesta eficiente de solicitudes y para evitar una sobrecarga del servidor web Asignaci\u00f3n de cookies Env\u00edo y procesamiento de cookies HTTP Adem\u00e1s del software del servidor, un host puede contener otro tipo de programas, como por ejemplo un servidor FTP para la carga de archivos o un servidor de base de datos para contenidos din\u00e1micos. En general, existen diferentes tipos de servidores web que pueden ser utilizados para numerosos prop\u00f3sitos, por ejemplo, los servidores de correo, los servidores de juegos o los servidores proxy. El protocolo HTTP Historia El protocolo de transferencia de hipertexto (HTTP, Hypertext Transfer Protocol) es el motor que da vida a Internet, ya que es la base para la web (www, world wide web). Desde un punto de vista hist\u00f3rico, la web fue creada en 1989 en el Consejo Europeo para la Investigaci\u00f3n Nuclear (CERN, Centro Europeene pour la Recherche Nucl\u00e9aire), con sede en Ginebra, justo en la frontera entre Suiza y Francia. Cabe decir que este organismo dispon\u00eda (y dispone) de una amplia plantilla de cient\u00edficos de diferentes pa\u00edses de Europa que trabajan en sus aceleradores de part\u00edculas. En consecuencia, muchos equipos de trabajadores est\u00e1n integrados por miembros de nacionalidades diferentes. Adem\u00e1s, muchos de los experimentos que se realizan destacan por su complejidad y requieren a\u00f1os y a\u00f1os de planificaci\u00f3n y de construcci\u00f3n de equipamientos. Fue a ra\u00edz de la necesidad de disponer de m\u00faltiples grupos de cient\u00edficos repartidos por el mundo y colaborando entre ellos (envi\u00e1ndose informes, dibujos, esquemas, fotos y todo tipo de documentos) que naci\u00f3 la web. Es en los inicios del protocolo HTTP, a mediados del a\u00f1o 1990, cuando encontramos la versi\u00f3n 0.9. Esta versi\u00f3n ten\u00eda como \u00fanica finalidad transferir datos por Internet en forma de p\u00e1ginas web escritas en lenguaje de marcado de hipertexto (HTML, HyperText Markup Language). A partir de la versi\u00f3n 1.0 del protocolo surgi\u00f3 la posibilidad de transferir mensajes con encabezados que describ\u00edan el contenido de los mensajes. Versiones La primera versi\u00f3n: HTTP/1 La historia de HTTP empez\u00f3 en 1989, cuando Tim Berners-Lee y su equipo del CERN (Suiza) empezaron a desarrollar la World Wide Web. La versi\u00f3n inicial de HTTP fue bautizada con el n\u00famero de versi\u00f3n 0.9, consist\u00eda en una sola l\u00ednea y solo permit\u00eda solicitar un archivo HTML del servidor cada vez. El servidor entonces no hac\u00eda m\u00e1s que transferir el archivo solicitado, de manera que esta versi\u00f3n del protocolo solo pod\u00eda manejar archivos HTML. El primer est\u00e1ndar oficial: HTTP/1.1 HTTP/1.1 aclar\u00f3 ambig\u00fcedades y a\u00f1adi\u00f3 numerosas mejoras: Una conexi\u00f3n pod\u00eda ser reutilizada, ahorrando as\u00ed el tiempo de re-abrirla repetidas veces. Enrutamiento('Pipelining' en ingl\u00e9s) se a\u00f1adi\u00f3 a la especificaci\u00f3n, permitiendo realizar una segunda petici\u00f3n de datos, antes de que fuera respondida la primera, disminuyendo de este modo la latencia de la comunicaci\u00f3n. Se permiti\u00f3 que las respuestas a peticiones, pod\u00edan ser divididas en sub-partes. La negociaci\u00f3n de contenido, incluyendo el lenguaje, el tipo de codificaci\u00f3n, o tipos, se a\u00f1adieron a la especificaci\u00f3n, permitiendo que servidor y cliente, acordasen el contenido m\u00e1s adecuado a intercambiarse. Gracias a la cabecera, Host, pudo ser posible alojar varios dominios en la misma direcci\u00f3n IP. Un protocolo de mayor rendimiento HTTP/2 Seg\u00fan pasaban los a\u00f1os, las p\u00e1ginas web se volv\u00edan cada vez m\u00e1s amplias y complejas. Para cargar una web moderna en el navegador, este tiene que solicitar muchos megabytes de datos y enviar hasta cien solicitudes HTTP. HTTP/1.1 est\u00e1 pensado para procesar solicitudes una tras otra en una misma conexi\u00f3n, de manera que cuanto m\u00e1s compleja sea una p\u00e1gina web, m\u00e1s tardar\u00e1 en cargarse y mostrarse. Por esta raz\u00f3n, Google desarroll\u00f3 un nuevo y experimental protocolo, el SPDY o Speedy, que despert\u00f3 un gran inter\u00e9s entre los desarrolladores y permiti\u00f3 que en 2015 se publicara la versi\u00f3n HTTP/2 del protocolo. Este est\u00e1ndar incluye m\u00faltiples mejoras que tienen como objetivo acelerar la carga de las p\u00e1ginas web. La versi\u00f3n HTTP/2 se extendi\u00f3 r\u00e1pidamente y las p\u00e1ginas web con mucho tr\u00e1fico fueron de las primeras en adoptarla. Actualmente (con fecha de enero de 2020), seg\u00fan W3Techs, un 42 % de las p\u00e1ginas web utilizan la versi\u00f3n HTTP/2. El futuro: HTTP/3 Un punto d\u00e9bil de todas las versiones de HTTP usadas hasta ahora es el protocolo de control de transmisi\u00f3n (TCP) en el que se basan. Este protocolo requiere que el receptor de cada paquete de datos confirme la recepci\u00f3n antes de que pueda enviarse el siguiente paquete. De este modo, basta con que se pierda un paquete para que todos los dem\u00e1s tengan que esperar a que dicho paquete sea transmitido de nuevo. Para evitarlos, la nueva versi\u00f3n HTTP/3 no funcionar\u00e1 con TCP, sino con UDP, que no aplica este tipo de medidas correctivas. A partir de UDP, se ha creado el protocolo QUIC (Quick UDP Internet Connections), que ser\u00e1 la base de HTTP/3. Funcionamiento del protocolo HTTP Ya hemos comentado que el protocolo HTTP tiene un funcionamiento bastante sencillo basado en el env\u00edo de mensajes entre cliente y servidor. Gr\u00e1ficamente podemos resumir el proceso de comunicaci\u00f3n HTTP como sigue: Un usuario accede a una URL, seleccionando un enlace de un documento HTML o introduci\u00e9ndola directamente en el campo correspondiente del cliente Web. El cliente Web descodifica la URL, separando sus diferentes partes: el protocolo de acceso, la direcci\u00f3n DNS o IP del servidor, el posible puerto opcional (el valor por defecto es 80) y el objeto requerido del servidor. http://direccion[:puerto][path] Ejemplo: http://www.miweb.com/documento.html Se abre una conexi\u00f3n TCP/IP con el servidor, llamando al puerto TCP correspondiente. En ese momento, se realiza la petici\u00f3n HTTP. Para ello, se env\u00eda el comando necesario (GET, POST, HEAD,...), la direcci\u00f3n del objeto requerido (el contenido de la URL que sigue a la direcci\u00f3n del servidor), la versi\u00f3n del protocolo HTTP empleada y un conjunto variable de informaci\u00f3n, que incluye datos sobre las capacidades del navegador (browser), datos opcionales para el servidor, etc. El servidor devuelve la respuesta al cliente. Consiste en un c\u00f3digo de estado y el tipo de dato MIME de la informaci\u00f3n de retorno, seguido de la propia informaci\u00f3n. Se cierra la conexi\u00f3n TCP. Este proceso se repite en cada acceso al servidor HTTP. Por ejemplo, si se recoge un documento HTML en cuyo interior est\u00e1n insertadas 2 im\u00e1genes y 1 v\u00eddeo, el proceso anterior se repite cuatro veces, una para el documento HTML y tres m\u00e1s para los recursos (la dos im\u00e1genes y el v\u00eddeo). Comandos o m\u00e9todos HTTP HTTP define un conjunto de m\u00e9todos de petici\u00f3n para indicar la acci\u00f3n que se desea realizar para un recurso determinado. El est\u00e1ndar HTTP/1.0 recoge \u00fanicamente tres comandos, que representan las operaciones de recepci\u00f3n y env\u00edo de informaci\u00f3n y chequeo de estado: GET : se utiliza para solicitar cualquier tipo de informaci\u00f3n o recurso al servidor. Cada vez que se pulsa sobre un enlace o se teclea directamente a una URL se usa este comando. Como resultado, el servidor HTTP enviar\u00e1 el recurso correspondiente. HEAD : se utiliza para solicitar informaci\u00f3n sobre el recurso: su tama\u00f1o, su tipo, su fecha de modificaci\u00f3n\u2026 Es usado por los gestores de cach\u00e9s de p\u00e1ginas o los servidores proxy, para conocer cu\u00e1ndo es necesario actualizar la copia que se mantiene del recurso. Con HEAD se podr\u00e1 comprobar la \u00faltima fecha de modificaci\u00f3n de un recurso antes de traer una nueva copia del mismo. POST : sirve para enviar informaci\u00f3n al servidor, por ejemplo, los datos contenidos en un formulario. El servidor pasar\u00e1 esta informaci\u00f3n a un proceso encargado de su tratamiento. La versi\u00f3n 1.1 del protocolo incorpora unos pocos comandos m\u00e1s como son: OPTIONS, PUT, DELETE, TRACE y CONNECT. Veamos algunos de ellos: OPTIONS : Devuelve los m\u00e9todos HTTP que el servidor soporta para una URL espec\u00edfica. Esto puede ser utilizado para comprobar la funcionalidad de un servidor web mediante petici\u00f3n en lugar de un recurso espec\u00edfico. DELETE : sirve para eliminar un recurso especificado en la URL, aunque pocas veces sera permitido por un servidor web. TRACE : comando que permite hacer un sondeo para saber todos los dispositivos de la red por los que pasa nuestra petici\u00f3n. As\u00ed podremos descubrir si la petici\u00f3n pasa a trav\u00e9s dispositivos intermedios o proxys antes de llegar al servidor Web. PUT : puede verse como el comando inverso a GET. Nos permite escribir datos en el servidor o, lo que es lo mismo, poner un recurso en la URL que se especifique. Si el recurso no existe lo crea sino lo reemplaza. La diferencia con POST puede ser algo confusa; mientras que POST est\u00e1 orientado a la creaci\u00f3n de nuevos contenidos, PUT est\u00e1 m\u00e1s orientado a la actualizaci\u00f3n de los mismos (aunque tambi\u00e9n podr\u00eda crearlos). HTTP/2 no incluye m\u00e9todos nuevos. Ejemplo de petici\u00f3n y respuesta Una solicitud HTTP es un conjunto de l\u00edneas que el navegador env\u00eda al servidor. Incluye: El recurso solicitado, el m\u00e9todo que se aplicar\u00e1 y la versi\u00f3n del protocolo utilizada. Los campos del encabezado de solicitud: es un conjunto de l\u00edneas opcionales que permiten aportar informaci\u00f3n adicional sobre la solicitud y/o el cliente (navegador, sistema operativo, etc.). Cada una de estas l\u00edneas est\u00e1 formada por un nombre que describe el tipo de encabezado, seguido de dos puntos (:) y el valor del encabezado. El cuerpo de la solicitud: es un conjunto de l\u00edneas opcionales que deben estar separadas de las l\u00edneas precedentes por una l\u00ednea en blanco y que, por ejemplo, permiten la transmisi\u00f3n de datos al servidor de un formulario a trav\u00e9s del m\u00e9todo POST. La sintaxis de una respuesta HTTP es un conjunto de l\u00edneas que el servidor env\u00eda al navegador. Incluye: Una l\u00ednea de estado donde figura el versi\u00f3n del protocolo usada, un c\u00f3digo de estado/error y un texto con el significado de dicho c\u00f3digo. Los posibles c\u00f3digos de estado se identifican con n\u00fameros de tres cifras y se clasifican en cinco grupos seg\u00fan sean informativos (1xx), de \u00e9xito en la solicitud (2xx), para redireccionar la solicitud (3xx), por error generado en el cliente (4xx) o bien por errores generados en el servidor (5xx) \u2192 C\u00f3digos de estado/error Los campos del encabezado de la respuesta. Conjunto de lineas opcionales que aportan informaci\u00f3n adicional sobre la respuesta y/o el servidor. El cuerpo de la respuesta que contiene el recurso (objeto) solicitado Cabeceras HTTP Las cabeceras HTTP son los par\u00e1metros que se env\u00edan en una petici\u00f3n o respuesta HTTP al cliente o al servidor para proporcionar informaci\u00f3n esencial sobre la transacci\u00f3n en curso. Estas cabeceras proporcionan informaci\u00f3n mediante la sintaxis 'Cabecera: Valor' y son enviadas autom\u00e1ticamente por el navegador o el servidor Web. \u2192 Cabeceras HTTP Tipos MIME El protocolo HTTP fue dise\u00f1ado para transportar por red ficheros en formato ASCII, formados por texto plano. Con el paso del tiempo, surgi\u00f3 la necesidad de incluir diferentes tipos de ficheros no ASCII en las aplicaciones por Internet (im\u00e1genes, v\u00eddeos, sonidos, etc.) y, como consecuencia de ello, fue necesario buscar una soluci\u00f3n: hab\u00eda que transformar estos formatos a tipo ASCII (u otros juegos de caracteres compatibles) para su correcta recepci\u00f3n en el navegador web. Este problema ya hab\u00eda surgido en las aplicaciones de correo electr\u00f3nico, cuando se necesit\u00f3 enviar por MAIL ficheros no formados por texto plano, y por tanto, no compatibles con los juegos de caracteres permitidos. Para solucionar este problema se crearon los tipos MIME (Multipurpose Internet Mail Extensions), especificaciones para dar formato a mensajes no-ASCII, de forma que pudieran ser enviados por Internet e interpretados correctamente por los programas de correo locales. Tipos de medios de Internet, previamente conocido como \"tipos \" o \"tipos de contenido\", es un est\u00e1ndar dise\u00f1ado para indicar el tipo de informaci\u00f3n que presenta un archivo o un conjunto de datos. En , este identificador puede ser \u00fatil para conocer el tipo de un archivo antes de descarcarglo y tener acceso a \u00e9l. Es una buena p\u0155actica proveer informaci\u00f3n de tipos de medios siempre que sea posible, como en el caso de los elementos que cuentan con atributos como type, enctype, formenctype y accept. Todo identificador de tipo de medio de Internet debe ajustarse al siguiente formato: As\u00ed pues, el \"tipo\" y el \"subtipo\" deben estar presentes en cualquier tipo de medio de Internet. En la lista siguiente hay algunos ejemplos que contienen cada una da las partes delineadas anteriormente. HTTPS El Protocolo seguro de transferencia de hipertexto (en ingl\u00e9s, Hypertext Transfer Protocol Secure o HTTPS) es un protocolo de aplicaci\u00f3n basado en el protocolo HTTP, destinado a la transferencia segura de datos de hipertexto, es decir, es la versi\u00f3n segura de HTTP. La web es insegura por naturaleza. Cuando se dise\u00f1aron los protocolos en los que est\u00e1 basada (TCP/IP) no se tuvieron en cuenta muchos de los problemas que tiene la Internet moderna. Y el protocolo HTTP, para transferir p\u00e1ginas web, no a\u00f1adi\u00f3 nada al respecto tampoco hasta mucho despu\u00e9s, con la introducci\u00f3n del protocolo HTTPS (la \"ese\" es de \"Seguro\") all\u00e1 por 1994 por la empresa Netscape. El protocolo HTTPS original utilizaba SSL (Secure Sockets Layer) como protocolo seguro de intercambio de claves y cifrado, pero en la actualidad est\u00e1 obsoleto y se emplea TLS (Transport Layer Security, que va por su versi\u00f3n 1.3). El est\u00e1ndar de HTTP sobre TLS, en realidad, no se configur\u00f3 hasta mayo del a\u00f1o 2000. Tradicionalmente, los navegadores le han indicado a sus usuarios que se estaban conectando a un sitio seguro utilizando un iconito, generalmente uno con un candado . Seg\u00fan el navegador el aspecto cambia un poco, pero todos muestran el proverbial \"candadito\" al lado de la direcci\u00f3n: Es decir, lo importante aqu\u00ed es que hasta ahora los navegadores consideran HTTP como la norma, y HTTPS como la excepci\u00f3n, y por eso lo marcan de esta manera. Funcionamiento de HTTPS Servidores web: Apache vs Nginx Cuando vamos a poner en marcha un servidor web, lo primero que necesitamos es utilizar un sistema operativo sobre el cual vamos a ejecutar los diferentes servicios, sistema operativo que en m\u00e1s del 95% de las ocasiones suele ser un sistema Linux, as\u00ed como un software que se encargue de la gesti\u00f3n de las bases de datos, MySQL habitualmente, y un software para gestionar el contenido din\u00e1micos de las webs, que suele ser PHP. Adem\u00e1s de este software esencial, otra de las partes m\u00e1s importantes del servidor suele ser la elecci\u00f3n del servidor web, y aqu\u00ed es donde entran las dudas. Cuando buscamos montar una web podemos elegir una gran cantidad de servidores web diferentes, desde Apache y Nginx, los m\u00e1s conocidos y utilizados con m\u00e1s de un 85% de uso entre ambos, hasta otros servidores menos conocidos como Microsoft IIS (si usamos un servidor Windows), LiteSpeed, Node.js, etc. Los dos servidores m\u00e1s utilizados para montar p\u00e1ginas web hoy en d\u00eda son Apache y Nginx, sin embargo, es imposible decir que uno es mejor que otro ya que cada uno de ellos tiene sus propias fortalezas y debilidades y puede mejorar mejor bajo ciertas circunstancias o simplemente ser m\u00e1s sencillo de utilizar. Nginx est\u00e1 orientado a mejorar el rendimiento, soportando mayores cargas de tr\u00e1fico y usuarios que Apache (Problema C10K), adem\u00e1s de ofrecer otras funcionalidades como hacer de proxy. En sus or\u00edgenes era especialmente eficiente ofreciendo contenido est\u00e1tico . Despu\u00e9s de ser lanzado, Nginx fue usado principalmente para servir archivos est\u00e1ticos y como un balanceador de carga o proxy inverso en frente de instalaciones Apache. Ejemplos de servicios de despliegue de p\u00e1ginas est\u00e1ticas: Netlify Surge GitHub Pages GitLab Pages Firebase Vercel Neocities Mientras evolucionaba la red, y la necesidad de exprimir hasta la \u00faltima gota de la velocidad y eficiencia de uso de hardware con este, m\u00e1s sitios empezaron a reemplazar Apache con Nginx por completo, gracias a un software mucho m\u00e1s maduro. Razones para usar Nginx Es ligero Nginx reduce el consumo de RAM. Es multiplataforma y f\u00e1cil de instalar La mayor\u00eda de las grandes distribuciones de GNU/Linux, tienen Nginx en sus repositorios. \u00a1Se puede usar junto a Apache! S\u00ed, como lo lees, algunas empresas solo usan Nginx para servir contenido est\u00e1tico y Apache para el contenido din\u00e1mico. Cach\u00e9 Puedes usar Nginx como cach\u00e9, con algo de configuraci\u00f3n, permitiendo mejorar la eficiencia de tu aplicaci\u00f3n sin tocar la programaci\u00f3n de la misma. Balanceador de carga Este servidor web puede funcionar como balanceador de carga, distribuyendo el tr\u00e1fico entre varios servidores, permitiendo mayor escalabilidad. Soporte comunitario y profesional Nginx, Inc est\u00e1 detr\u00e1s del desarrollo de Nginx, adem\u00e1s de la comunidad en general, permitiendo tener un soporte tanto profesional como comunitario. Compatibilidad con las aplicaciones web m\u00e1s populares Nginx es compatible con una gran cantidad de CMS existentes en el mercado, y hay un muchos tutoriales y documentaci\u00f3n para instalar estos bajo Nginx, como por ejemplo: Wordpress, Joomla, Drupal, phpBB \u00a1y m\u00e1s!","title":"Tema 2 - Arquitectura Web. Implantaci\u00f3n y administraci\u00f3n de servidores web"},{"location":"ArqWeb/#arquitectura-web-implantacion-y-administracion-de-servidores-web","text":"","title":"Arquitectura Web. Implantaci\u00f3n y administraci\u00f3n de servidores web"},{"location":"ArqWeb/#introduccion","text":"Con la evoluci\u00f3n y el acceso libre a Internet, uno de los principales alicientes que han surgido es la publicaci\u00f3n de p\u00e1ginas web donde se pueden almacenar unos contenidos bastante atractivos para nosotros y que, al mismo tiempo, pueden ser consultados desde cualquier del mundo para todos. Cabe decir que, con la popularizaci\u00f3n de Internet, tanto empresas como usuarios han visto la necesidad de establecer un punto desde donde anunciar sus productos, o bien, a t\u00edtulo particular, dar publicidad a las aficiones o capacidades personales mediante la publicaci\u00f3n de p\u00e1ginas web. Las p\u00e1ginas web, en su mayor\u00eda en formato HTML, requieren ser alojadas en m\u00e1quinas que dispongan de espacio en disco para almacenar archivos HTML, im\u00e1genes, bloques de c\u00f3digo o archivos de v\u00eddeo en directorios espec\u00edficos y, al mismo tiempo, deben ser capaces de entender todo tipo de extensi\u00f3n de los archivos que son enviados en ambos sentidos de la comunicaci\u00f3n. Paralelamente, no podemos dejar de lado la importancia de las medidas de seguridad ante los peligros existentes en Internet. Para ello, las p\u00e1ginas deber\u00e1n estar dise\u00f1adas considerando la incorporaci\u00f3n de protocolos de comunicaci\u00f3n seguros como, por ejemplo, los desarrollados con el protocolo seguro de transferencia de hipertexto (HTTPS, Hyper Text Transfer Protocol secure) que utilizan claves y estrategias de cifrado propias de las herramientas del protocolo de capa de conexi\u00f3n segura (SSL, secure sockets layer). Las m\u00e1quinas que alojan las p\u00e1ginas web reciben la categor\u00eda de servidores web. Desde el punto de vista de los servidores, los requerimientos m\u00e1s relevantes son el espacio de disco necesario para poder almacenar la estructura de la p\u00e1gina web y una buena conexi\u00f3n de red para que el consumo de la unidad de procesamiento central (CPU, central processing unit ) sea bastante bajo. El funcionamiento de los servidores web es especial ya que, como si se tratara de un diente de sierra, tienen consumos de recursos puntuales porque podemos estar un tiempo sin peticiones y, de repente, tener una avalancha de peticiones. Esto hace que los servidores web suelan tener un n\u00famero bajo de procesos en espera. A medida que resultan necesarios, se van arrancando nuevos. Cabe decir que no todas las peticiones consumen el mismo, y, por ejemplo, aquellas p\u00e1ginas web que ejecuten programas de interacci\u00f3n con el usuario o requieran cifrado (HTTPS) consumen m\u00e1s recursos que otras p\u00e1ginas web con menos interacci\u00f3n.","title":"Introducci\u00f3n"},{"location":"ArqWeb/#que-es-un-servidor-web","text":"Los servidores web sirven para almacenar contenidos de Internet y facilitar su disponibilidad de forma constante y segura. Cuando visitas una p\u00e1gina web desde tu navegador, es en realidad un servidor web el que env\u00eda los componentes individuales de dicha p\u00e1gina directamente a tu ordenador. Esto quiere decir que para que una p\u00e1gina web sea accesible en cualquier momento, el servidor web debe estar permanentemente online. Toda p\u00e1gina accesible en Internet necesita un servidor especial para sus contenidos web. A menudo, las grandes empresas y organizaciones cuentan con un servidor web propio para disponer sus contenidos en Intranet e Internet. Sin embargo, la mayor\u00eda de administradores recurren a los centros de datos de proveedores de alojamiento web para sus proyectos. Independientemente de si tienes un servidor web propio o de si alquilas uno externo, siempre necesitar\u00e1s un software para gestionar los datos de tu p\u00e1gina y mantenerla actualizada. En este sentido, tienes la posibilidad de elegir entre varias soluciones de software para servidores web dise\u00f1adas para diferentes aplicaciones y sistemas operativos.","title":"\u00bfQu\u00e9 es un servidor web?"},{"location":"ArqWeb/#tecnologia-de-servidores-web","text":"Principalmente, el software de un servidor HTTP es el encargado de proporcionar los datos para la visualizaci\u00f3n del contenido web. Para abrir una p\u00e1gina web, el usuario solo tiene que escribir el URL correspondiente en la barra de direcciones de su navegador web. El navegador env\u00eda una solicitud al servidor web, quien responde, por ejemplo, entregando una p\u00e1gina HTML. Esta puede estar alojada como un documento est\u00e1tico en el host o ser generada de forma din\u00e1mica, lo que significa que el servidor web tiene que ejecutar un c\u00f3digo de programa (p. ej., Java o PHP) antes de tramitar su respuesta. El navegador interpreta la respuesta, lo que suele generar autom\u00e1ticamente m\u00e1s solicitudes al servidor a prop\u00f3sito de, por ejemplo, im\u00e1genes integradas o archivos CSS (hojas de estilos). El protocolo utilizado para la transmisi\u00f3n es HTTP (o su variante cifrada HTTPS), que se basa, a su vez, en los protocolos de red IP y TCP (y muy rara vez en UDP). Un servidor web puede entregar los contenidos simult\u00e1neamente a varios ordenadores o navegadores web. La cantidad de solicitudes (requests) y la velocidad con la que pueden ser procesadas depende, entre otras cosas, del hardware y la carga (n\u00famero de solicitudes) del host. Sin embargo, la complejidad del contenido tambi\u00e9n juega un papel importante: los contenidos web din\u00e1micos necesitan m\u00e1s recursos que los contenidos est\u00e1ticos. La selecci\u00f3n del equipo adecuado para el servidor y la decisi\u00f3n de si este debe ser dedicado, virtual o en la nube, se debe hacer pensando siempre en evitar sobrecargas en el servidor. Aunque se haya encontrado un servidor web que se adapta perfectamente a las necesidades del proyecto, siempre se corre el riesgo de que se presenten fallos en \u00e9l como consecuencia de imprecisiones t\u00e9cnicas o cortes de energ\u00eda en el centro de datos del host. Aunque no es muy frecuente, durante un per\u00edodo de inactividad de este tipo (downtime), la web no estar\u00e1 disponible.","title":"Tecnolog\u00eda de servidores web"},{"location":"ArqWeb/#otras-funciones-de-los-servidores-web","text":"Aunque su principal funci\u00f3n es la transferencia de contenido web, muchos programas de servidor web ofrecen caracter\u00edsticas adicionales: Seguridad Cifrado de la comunicaci\u00f3n entre el servidor web y el cliente v\u00eda HTTPS Autenticaci\u00f3n del usuario Autenticaci\u00f3n HTTP para \u00e1reas espec\u00edficas de una aplicaci\u00f3n web Redirecci\u00f3n Redirecci\u00f3n de una solicitud de documento por medio de Rewrite Engine Redirecci\u00f3n Almacenamiento en cach\u00e9 de documentos din\u00e1micos para la respuesta eficiente de solicitudes y para evitar una sobrecarga del servidor web Asignaci\u00f3n de cookies Env\u00edo y procesamiento de cookies HTTP Adem\u00e1s del software del servidor, un host puede contener otro tipo de programas, como por ejemplo un servidor FTP para la carga de archivos o un servidor de base de datos para contenidos din\u00e1micos. En general, existen diferentes tipos de servidores web que pueden ser utilizados para numerosos prop\u00f3sitos, por ejemplo, los servidores de correo, los servidores de juegos o los servidores proxy.","title":"Otras funciones de los servidores web"},{"location":"ArqWeb/#el-protocolo-http","text":"","title":"El protocolo HTTP"},{"location":"ArqWeb/#historia","text":"El protocolo de transferencia de hipertexto (HTTP, Hypertext Transfer Protocol) es el motor que da vida a Internet, ya que es la base para la web (www, world wide web). Desde un punto de vista hist\u00f3rico, la web fue creada en 1989 en el Consejo Europeo para la Investigaci\u00f3n Nuclear (CERN, Centro Europeene pour la Recherche Nucl\u00e9aire), con sede en Ginebra, justo en la frontera entre Suiza y Francia. Cabe decir que este organismo dispon\u00eda (y dispone) de una amplia plantilla de cient\u00edficos de diferentes pa\u00edses de Europa que trabajan en sus aceleradores de part\u00edculas. En consecuencia, muchos equipos de trabajadores est\u00e1n integrados por miembros de nacionalidades diferentes. Adem\u00e1s, muchos de los experimentos que se realizan destacan por su complejidad y requieren a\u00f1os y a\u00f1os de planificaci\u00f3n y de construcci\u00f3n de equipamientos. Fue a ra\u00edz de la necesidad de disponer de m\u00faltiples grupos de cient\u00edficos repartidos por el mundo y colaborando entre ellos (envi\u00e1ndose informes, dibujos, esquemas, fotos y todo tipo de documentos) que naci\u00f3 la web. Es en los inicios del protocolo HTTP, a mediados del a\u00f1o 1990, cuando encontramos la versi\u00f3n 0.9. Esta versi\u00f3n ten\u00eda como \u00fanica finalidad transferir datos por Internet en forma de p\u00e1ginas web escritas en lenguaje de marcado de hipertexto (HTML, HyperText Markup Language). A partir de la versi\u00f3n 1.0 del protocolo surgi\u00f3 la posibilidad de transferir mensajes con encabezados que describ\u00edan el contenido de los mensajes.","title":"Historia"},{"location":"ArqWeb/#versiones","text":"","title":"Versiones"},{"location":"ArqWeb/#la-primera-version-http1","text":"La historia de HTTP empez\u00f3 en 1989, cuando Tim Berners-Lee y su equipo del CERN (Suiza) empezaron a desarrollar la World Wide Web. La versi\u00f3n inicial de HTTP fue bautizada con el n\u00famero de versi\u00f3n 0.9, consist\u00eda en una sola l\u00ednea y solo permit\u00eda solicitar un archivo HTML del servidor cada vez. El servidor entonces no hac\u00eda m\u00e1s que transferir el archivo solicitado, de manera que esta versi\u00f3n del protocolo solo pod\u00eda manejar archivos HTML.","title":"La primera versi\u00f3n: HTTP/1"},{"location":"ArqWeb/#el-primer-estandar-oficial-http11","text":"HTTP/1.1 aclar\u00f3 ambig\u00fcedades y a\u00f1adi\u00f3 numerosas mejoras: Una conexi\u00f3n pod\u00eda ser reutilizada, ahorrando as\u00ed el tiempo de re-abrirla repetidas veces. Enrutamiento('Pipelining' en ingl\u00e9s) se a\u00f1adi\u00f3 a la especificaci\u00f3n, permitiendo realizar una segunda petici\u00f3n de datos, antes de que fuera respondida la primera, disminuyendo de este modo la latencia de la comunicaci\u00f3n. Se permiti\u00f3 que las respuestas a peticiones, pod\u00edan ser divididas en sub-partes. La negociaci\u00f3n de contenido, incluyendo el lenguaje, el tipo de codificaci\u00f3n, o tipos, se a\u00f1adieron a la especificaci\u00f3n, permitiendo que servidor y cliente, acordasen el contenido m\u00e1s adecuado a intercambiarse. Gracias a la cabecera, Host, pudo ser posible alojar varios dominios en la misma direcci\u00f3n IP.","title":"El primer est\u00e1ndar oficial: HTTP/1.1"},{"location":"ArqWeb/#un-protocolo-de-mayor-rendimiento-http2","text":"Seg\u00fan pasaban los a\u00f1os, las p\u00e1ginas web se volv\u00edan cada vez m\u00e1s amplias y complejas. Para cargar una web moderna en el navegador, este tiene que solicitar muchos megabytes de datos y enviar hasta cien solicitudes HTTP. HTTP/1.1 est\u00e1 pensado para procesar solicitudes una tras otra en una misma conexi\u00f3n, de manera que cuanto m\u00e1s compleja sea una p\u00e1gina web, m\u00e1s tardar\u00e1 en cargarse y mostrarse. Por esta raz\u00f3n, Google desarroll\u00f3 un nuevo y experimental protocolo, el SPDY o Speedy, que despert\u00f3 un gran inter\u00e9s entre los desarrolladores y permiti\u00f3 que en 2015 se publicara la versi\u00f3n HTTP/2 del protocolo. Este est\u00e1ndar incluye m\u00faltiples mejoras que tienen como objetivo acelerar la carga de las p\u00e1ginas web. La versi\u00f3n HTTP/2 se extendi\u00f3 r\u00e1pidamente y las p\u00e1ginas web con mucho tr\u00e1fico fueron de las primeras en adoptarla. Actualmente (con fecha de enero de 2020), seg\u00fan W3Techs, un 42 % de las p\u00e1ginas web utilizan la versi\u00f3n HTTP/2.","title":"Un protocolo de mayor rendimiento HTTP/2"},{"location":"ArqWeb/#el-futuro-http3","text":"Un punto d\u00e9bil de todas las versiones de HTTP usadas hasta ahora es el protocolo de control de transmisi\u00f3n (TCP) en el que se basan. Este protocolo requiere que el receptor de cada paquete de datos confirme la recepci\u00f3n antes de que pueda enviarse el siguiente paquete. De este modo, basta con que se pierda un paquete para que todos los dem\u00e1s tengan que esperar a que dicho paquete sea transmitido de nuevo. Para evitarlos, la nueva versi\u00f3n HTTP/3 no funcionar\u00e1 con TCP, sino con UDP, que no aplica este tipo de medidas correctivas. A partir de UDP, se ha creado el protocolo QUIC (Quick UDP Internet Connections), que ser\u00e1 la base de HTTP/3.","title":"El futuro: HTTP/3"},{"location":"ArqWeb/#funcionamiento-del-protocolo-http","text":"Ya hemos comentado que el protocolo HTTP tiene un funcionamiento bastante sencillo basado en el env\u00edo de mensajes entre cliente y servidor. Gr\u00e1ficamente podemos resumir el proceso de comunicaci\u00f3n HTTP como sigue: Un usuario accede a una URL, seleccionando un enlace de un documento HTML o introduci\u00e9ndola directamente en el campo correspondiente del cliente Web. El cliente Web descodifica la URL, separando sus diferentes partes: el protocolo de acceso, la direcci\u00f3n DNS o IP del servidor, el posible puerto opcional (el valor por defecto es 80) y el objeto requerido del servidor. http://direccion[:puerto][path] Ejemplo: http://www.miweb.com/documento.html Se abre una conexi\u00f3n TCP/IP con el servidor, llamando al puerto TCP correspondiente. En ese momento, se realiza la petici\u00f3n HTTP. Para ello, se env\u00eda el comando necesario (GET, POST, HEAD,...), la direcci\u00f3n del objeto requerido (el contenido de la URL que sigue a la direcci\u00f3n del servidor), la versi\u00f3n del protocolo HTTP empleada y un conjunto variable de informaci\u00f3n, que incluye datos sobre las capacidades del navegador (browser), datos opcionales para el servidor, etc. El servidor devuelve la respuesta al cliente. Consiste en un c\u00f3digo de estado y el tipo de dato MIME de la informaci\u00f3n de retorno, seguido de la propia informaci\u00f3n. Se cierra la conexi\u00f3n TCP. Este proceso se repite en cada acceso al servidor HTTP. Por ejemplo, si se recoge un documento HTML en cuyo interior est\u00e1n insertadas 2 im\u00e1genes y 1 v\u00eddeo, el proceso anterior se repite cuatro veces, una para el documento HTML y tres m\u00e1s para los recursos (la dos im\u00e1genes y el v\u00eddeo).","title":"Funcionamiento del protocolo HTTP"},{"location":"ArqWeb/#comandos-o-metodos-http","text":"HTTP define un conjunto de m\u00e9todos de petici\u00f3n para indicar la acci\u00f3n que se desea realizar para un recurso determinado. El est\u00e1ndar HTTP/1.0 recoge \u00fanicamente tres comandos, que representan las operaciones de recepci\u00f3n y env\u00edo de informaci\u00f3n y chequeo de estado: GET : se utiliza para solicitar cualquier tipo de informaci\u00f3n o recurso al servidor. Cada vez que se pulsa sobre un enlace o se teclea directamente a una URL se usa este comando. Como resultado, el servidor HTTP enviar\u00e1 el recurso correspondiente. HEAD : se utiliza para solicitar informaci\u00f3n sobre el recurso: su tama\u00f1o, su tipo, su fecha de modificaci\u00f3n\u2026 Es usado por los gestores de cach\u00e9s de p\u00e1ginas o los servidores proxy, para conocer cu\u00e1ndo es necesario actualizar la copia que se mantiene del recurso. Con HEAD se podr\u00e1 comprobar la \u00faltima fecha de modificaci\u00f3n de un recurso antes de traer una nueva copia del mismo. POST : sirve para enviar informaci\u00f3n al servidor, por ejemplo, los datos contenidos en un formulario. El servidor pasar\u00e1 esta informaci\u00f3n a un proceso encargado de su tratamiento. La versi\u00f3n 1.1 del protocolo incorpora unos pocos comandos m\u00e1s como son: OPTIONS, PUT, DELETE, TRACE y CONNECT. Veamos algunos de ellos: OPTIONS : Devuelve los m\u00e9todos HTTP que el servidor soporta para una URL espec\u00edfica. Esto puede ser utilizado para comprobar la funcionalidad de un servidor web mediante petici\u00f3n en lugar de un recurso espec\u00edfico. DELETE : sirve para eliminar un recurso especificado en la URL, aunque pocas veces sera permitido por un servidor web. TRACE : comando que permite hacer un sondeo para saber todos los dispositivos de la red por los que pasa nuestra petici\u00f3n. As\u00ed podremos descubrir si la petici\u00f3n pasa a trav\u00e9s dispositivos intermedios o proxys antes de llegar al servidor Web. PUT : puede verse como el comando inverso a GET. Nos permite escribir datos en el servidor o, lo que es lo mismo, poner un recurso en la URL que se especifique. Si el recurso no existe lo crea sino lo reemplaza. La diferencia con POST puede ser algo confusa; mientras que POST est\u00e1 orientado a la creaci\u00f3n de nuevos contenidos, PUT est\u00e1 m\u00e1s orientado a la actualizaci\u00f3n de los mismos (aunque tambi\u00e9n podr\u00eda crearlos). HTTP/2 no incluye m\u00e9todos nuevos.","title":"Comandos o m\u00e9todos HTTP"},{"location":"ArqWeb/#ejemplo-de-peticion-y-respuesta","text":"Una solicitud HTTP es un conjunto de l\u00edneas que el navegador env\u00eda al servidor. Incluye: El recurso solicitado, el m\u00e9todo que se aplicar\u00e1 y la versi\u00f3n del protocolo utilizada. Los campos del encabezado de solicitud: es un conjunto de l\u00edneas opcionales que permiten aportar informaci\u00f3n adicional sobre la solicitud y/o el cliente (navegador, sistema operativo, etc.). Cada una de estas l\u00edneas est\u00e1 formada por un nombre que describe el tipo de encabezado, seguido de dos puntos (:) y el valor del encabezado. El cuerpo de la solicitud: es un conjunto de l\u00edneas opcionales que deben estar separadas de las l\u00edneas precedentes por una l\u00ednea en blanco y que, por ejemplo, permiten la transmisi\u00f3n de datos al servidor de un formulario a trav\u00e9s del m\u00e9todo POST. La sintaxis de una respuesta HTTP es un conjunto de l\u00edneas que el servidor env\u00eda al navegador. Incluye: Una l\u00ednea de estado donde figura el versi\u00f3n del protocolo usada, un c\u00f3digo de estado/error y un texto con el significado de dicho c\u00f3digo. Los posibles c\u00f3digos de estado se identifican con n\u00fameros de tres cifras y se clasifican en cinco grupos seg\u00fan sean informativos (1xx), de \u00e9xito en la solicitud (2xx), para redireccionar la solicitud (3xx), por error generado en el cliente (4xx) o bien por errores generados en el servidor (5xx) \u2192 C\u00f3digos de estado/error Los campos del encabezado de la respuesta. Conjunto de lineas opcionales que aportan informaci\u00f3n adicional sobre la respuesta y/o el servidor. El cuerpo de la respuesta que contiene el recurso (objeto) solicitado","title":"Ejemplo de petici\u00f3n y respuesta"},{"location":"ArqWeb/#cabeceras-http","text":"Las cabeceras HTTP son los par\u00e1metros que se env\u00edan en una petici\u00f3n o respuesta HTTP al cliente o al servidor para proporcionar informaci\u00f3n esencial sobre la transacci\u00f3n en curso. Estas cabeceras proporcionan informaci\u00f3n mediante la sintaxis 'Cabecera: Valor' y son enviadas autom\u00e1ticamente por el navegador o el servidor Web. \u2192 Cabeceras HTTP","title":"Cabeceras HTTP"},{"location":"ArqWeb/#tipos-mime","text":"El protocolo HTTP fue dise\u00f1ado para transportar por red ficheros en formato ASCII, formados por texto plano. Con el paso del tiempo, surgi\u00f3 la necesidad de incluir diferentes tipos de ficheros no ASCII en las aplicaciones por Internet (im\u00e1genes, v\u00eddeos, sonidos, etc.) y, como consecuencia de ello, fue necesario buscar una soluci\u00f3n: hab\u00eda que transformar estos formatos a tipo ASCII (u otros juegos de caracteres compatibles) para su correcta recepci\u00f3n en el navegador web. Este problema ya hab\u00eda surgido en las aplicaciones de correo electr\u00f3nico, cuando se necesit\u00f3 enviar por MAIL ficheros no formados por texto plano, y por tanto, no compatibles con los juegos de caracteres permitidos. Para solucionar este problema se crearon los tipos MIME (Multipurpose Internet Mail Extensions), especificaciones para dar formato a mensajes no-ASCII, de forma que pudieran ser enviados por Internet e interpretados correctamente por los programas de correo locales. Tipos de medios de Internet, previamente conocido como \"tipos \" o \"tipos de contenido\", es un est\u00e1ndar dise\u00f1ado para indicar el tipo de informaci\u00f3n que presenta un archivo o un conjunto de datos. En , este identificador puede ser \u00fatil para conocer el tipo de un archivo antes de descarcarglo y tener acceso a \u00e9l. Es una buena p\u0155actica proveer informaci\u00f3n de tipos de medios siempre que sea posible, como en el caso de los elementos que cuentan con atributos como type, enctype, formenctype y accept. Todo identificador de tipo de medio de Internet debe ajustarse al siguiente formato: As\u00ed pues, el \"tipo\" y el \"subtipo\" deben estar presentes en cualquier tipo de medio de Internet. En la lista siguiente hay algunos ejemplos que contienen cada una da las partes delineadas anteriormente.","title":"Tipos MIME"},{"location":"ArqWeb/#https","text":"El Protocolo seguro de transferencia de hipertexto (en ingl\u00e9s, Hypertext Transfer Protocol Secure o HTTPS) es un protocolo de aplicaci\u00f3n basado en el protocolo HTTP, destinado a la transferencia segura de datos de hipertexto, es decir, es la versi\u00f3n segura de HTTP. La web es insegura por naturaleza. Cuando se dise\u00f1aron los protocolos en los que est\u00e1 basada (TCP/IP) no se tuvieron en cuenta muchos de los problemas que tiene la Internet moderna. Y el protocolo HTTP, para transferir p\u00e1ginas web, no a\u00f1adi\u00f3 nada al respecto tampoco hasta mucho despu\u00e9s, con la introducci\u00f3n del protocolo HTTPS (la \"ese\" es de \"Seguro\") all\u00e1 por 1994 por la empresa Netscape. El protocolo HTTPS original utilizaba SSL (Secure Sockets Layer) como protocolo seguro de intercambio de claves y cifrado, pero en la actualidad est\u00e1 obsoleto y se emplea TLS (Transport Layer Security, que va por su versi\u00f3n 1.3). El est\u00e1ndar de HTTP sobre TLS, en realidad, no se configur\u00f3 hasta mayo del a\u00f1o 2000. Tradicionalmente, los navegadores le han indicado a sus usuarios que se estaban conectando a un sitio seguro utilizando un iconito, generalmente uno con un candado . Seg\u00fan el navegador el aspecto cambia un poco, pero todos muestran el proverbial \"candadito\" al lado de la direcci\u00f3n: Es decir, lo importante aqu\u00ed es que hasta ahora los navegadores consideran HTTP como la norma, y HTTPS como la excepci\u00f3n, y por eso lo marcan de esta manera.","title":"HTTPS"},{"location":"ArqWeb/#funcionamiento-de-https","text":"","title":"Funcionamiento de HTTPS"},{"location":"ArqWeb/#servidores-web-apache-vs-nginx","text":"Cuando vamos a poner en marcha un servidor web, lo primero que necesitamos es utilizar un sistema operativo sobre el cual vamos a ejecutar los diferentes servicios, sistema operativo que en m\u00e1s del 95% de las ocasiones suele ser un sistema Linux, as\u00ed como un software que se encargue de la gesti\u00f3n de las bases de datos, MySQL habitualmente, y un software para gestionar el contenido din\u00e1micos de las webs, que suele ser PHP. Adem\u00e1s de este software esencial, otra de las partes m\u00e1s importantes del servidor suele ser la elecci\u00f3n del servidor web, y aqu\u00ed es donde entran las dudas. Cuando buscamos montar una web podemos elegir una gran cantidad de servidores web diferentes, desde Apache y Nginx, los m\u00e1s conocidos y utilizados con m\u00e1s de un 85% de uso entre ambos, hasta otros servidores menos conocidos como Microsoft IIS (si usamos un servidor Windows), LiteSpeed, Node.js, etc. Los dos servidores m\u00e1s utilizados para montar p\u00e1ginas web hoy en d\u00eda son Apache y Nginx, sin embargo, es imposible decir que uno es mejor que otro ya que cada uno de ellos tiene sus propias fortalezas y debilidades y puede mejorar mejor bajo ciertas circunstancias o simplemente ser m\u00e1s sencillo de utilizar. Nginx est\u00e1 orientado a mejorar el rendimiento, soportando mayores cargas de tr\u00e1fico y usuarios que Apache (Problema C10K), adem\u00e1s de ofrecer otras funcionalidades como hacer de proxy. En sus or\u00edgenes era especialmente eficiente ofreciendo contenido est\u00e1tico . Despu\u00e9s de ser lanzado, Nginx fue usado principalmente para servir archivos est\u00e1ticos y como un balanceador de carga o proxy inverso en frente de instalaciones Apache. Ejemplos de servicios de despliegue de p\u00e1ginas est\u00e1ticas: Netlify Surge GitHub Pages GitLab Pages Firebase Vercel Neocities Mientras evolucionaba la red, y la necesidad de exprimir hasta la \u00faltima gota de la velocidad y eficiencia de uso de hardware con este, m\u00e1s sitios empezaron a reemplazar Apache con Nginx por completo, gracias a un software mucho m\u00e1s maduro.","title":"Servidores web: Apache vs Nginx"},{"location":"ArqWeb/#razones-para-usar-nginx","text":"Es ligero Nginx reduce el consumo de RAM. Es multiplataforma y f\u00e1cil de instalar La mayor\u00eda de las grandes distribuciones de GNU/Linux, tienen Nginx en sus repositorios. \u00a1Se puede usar junto a Apache! S\u00ed, como lo lees, algunas empresas solo usan Nginx para servir contenido est\u00e1tico y Apache para el contenido din\u00e1mico. Cach\u00e9 Puedes usar Nginx como cach\u00e9, con algo de configuraci\u00f3n, permitiendo mejorar la eficiencia de tu aplicaci\u00f3n sin tocar la programaci\u00f3n de la misma. Balanceador de carga Este servidor web puede funcionar como balanceador de carga, distribuyendo el tr\u00e1fico entre varios servidores, permitiendo mayor escalabilidad. Soporte comunitario y profesional Nginx, Inc est\u00e1 detr\u00e1s del desarrollo de Nginx, adem\u00e1s de la comunidad en general, permitiendo tener un soporte tanto profesional como comunitario. Compatibilidad con las aplicaciones web m\u00e1s populares Nginx es compatible con una gran cantidad de CMS existentes en el mercado, y hay un muchos tutoriales y documentaci\u00f3n para instalar estos bajo Nginx, como por ejemplo: Wordpress, Joomla, Drupal, phpBB \u00a1y m\u00e1s!","title":"Razones para usar Nginx"},{"location":"P1.1/","text":"Pr\u00e0ctica 2.1 \u2013 Instalaci\u00f3n y configuraci\u00f3n de servidor web Nginx Instalaci\u00f3n servidor web Nginx Para instalar el servidor nginx en nuestra Debian, primero actualizamos los repositorios y despu\u00e9s instalamos el paquete correspondiente: sudo apt update sudo apt install nginx Comprobamos que nginx se ha instalado y que est\u00e1 funcionando correctamente: systemctl status nginx Info Esta pr\u00e1ctica se ha hecho con Nginx 1.18.0 Creaci\u00f3n de las carpeta del sitio web Igual que ocurre en Apache, todos los archivos que formar\u00e1n parte de un sitio web que servir\u00e1 nginx se organizar\u00e1n en carpetas. Estas carpetas, t\u00edpicamente est\u00e1n dentro de /var/www . As\u00ed pues, vamos a crear la carpeta de nuestro sitio web o dominio: sudo mkdir -p /var/www/nombre_web/html Donde el nombre de dominio puede ser la palabra que quer\u00e1is, sin espacios. Ah\u00ed, dentro de esa carpeta html, deb\u00e9is clonar el siguiente repositorio: https://github.com/cloudacademy/static-website-example Adem\u00e1s, haremos que el propietario de esta carpeta y todo lo que haya dentro sea el usuario www-data , t\u00edpicamente el usuario del servicio web. sudo chown -R www-data:www-data /var/www/nombre_web/html Y le daremos los permisos adecuados para que no nos de un error de acceso no autorizado al entrar en el sitio web: sudo chmod -R 755 /var/www/nombre_web Para comprobar que el servidor est\u00e1 funcionando y sirviendo p\u00e1ginas correctamente, pod\u00e9is acceder desde vuestro cliente a: http://IP-maq-virtual Y os deber\u00e1 aparecer algo as\u00ed: Lo que demuestra que todo es correcto hasta ahora. Configuraci\u00f3n de servidor web NGINX En Nginx hay dos rutas importantes. La primera de ellas es sites-available , que contiene los archivos de configuraci\u00f3n de los hosts virtuales o bloques disponibles en el servidor. Es decir, cada uno de los sitios webs que alberga el servido. La otra es sites-enabled , que contiene los archivos de configuraci\u00f3n de los sitios habilitados, es decir, los que funcionan en ese momento. Dentro de sites-available hay un archivo de configuraci\u00f3n por defecto (default), que es la p\u00e1gina que se muestra si accedemos al servidor sin indicar ning\u00fan sitio web o cuando el sitio web no es encontrado en el servidor (debido a una mala configuraci\u00f3n por ejemplo). Esta es la p\u00e1gina que nos ha aparecido en el apartado anterior. Para que Nginx presente el contenido de nuestra web, es necesario crear un bloque de servidor con las directivas correctas. En vez de modificar el archivo de configuraci\u00f3n predeterminado directamente, crearemos uno nuevo en /etc/nginx/sites-available/nombre_web : sudo nano /etc/nginx/sites-available/vuestro_dominio Y el contenido de ese archivo de configuraci\u00f3n: server { listen 80 ; listen [::]:80; root /ruta/absoluta/archivo/index ; index index.html index.htm index.nginx-debian.html; server_name nombre_web; location / { try_files $uri $uri/ =404; } } Aqu\u00ed la directiva root debe ir seguida de la ruta absoluta absoluta d\u00f3nde se encuentre el archivo index.html de nuestra p\u00e1gina web, que se encuentra entre todos los que hab\u00e9is descomprimido. Aqu\u00ed ten\u00e9is un ejemplo de un sitio webs con su ruta (directorios que hay) antes del archivo index.html: Info Ruta \u2192 /var/www/ejemplo2/html/2016_soft_landing Y crearemos un archivo simb\u00f3lico entre este archivo y el de sitios que est\u00e1n habilitados, para que se d\u00e9 de alta autom\u00e1ticamente. sudo ln -s /etc/nginx/sites-available/nombre_web /etc/nginx/sites-enabled/ Y reiniciamos el servidor para aplicar la configuraci\u00f3n: sudo systemctl restart nginx Comprobaciones Comprobaci\u00f3n del correcto funcionamiento Como a\u00fan no poseemos un servidor DNS que traduzca los nombres a IPs, debemos hacerlo de forma manual. Vamos a editar el archivo /etc/hosts de nuestra m\u00e1quina anfitriona para que asocie la IP de la m\u00e1quina virtual, a nuestro server_name . Este archivo, en Linux, est\u00e1 en: /etc/hosts Y en Windows: C:\\Windows\\System32\\drivers\\etc\\hosts Y deberemos a\u00f1adirle la l\u00ednea: 192.168.X.X nombre_web donde deb\u00e9is sustituir la IP por la que tenga vuestra m\u00e1quina virtual. Comprobar registros del servidor Comprobad que las peticiones se est\u00e1n registrando correctamente en los archivos de logs, tanto las correctas como las err\u00f3neas: /var/log/nginx/access.log : cada solicitud a su servidor web se registra en este archivo de registro, a menos que Nginx est\u00e9 configurado para hacer algo diferente. /var/log/nginx/error.log : cualquier error de Nginx se asentar\u00e1 en este registro. Info Si no os aparece nada en los logs, podr\u00eda pasar que el navegador ha cacheado la p\u00e1gina web y que, por tanto, ya no est\u00e1 obteniendo la p\u00e1gina del navegador sino de la propia memoria. Para solucionar esto, pod\u00e9is acceder con el modo privado del navegador y ya os deber\u00eda registrar esa actividad en los logs. FTP Si queremos tener varios dominios o sitios web en el mismo servidor nginx (es decir, que tendr\u00e1n la misma IP) debemos repetir todo el proceso anterior con el nuevo nombre de dominio que queramos configurar. \u00bfC\u00f3mo transferir archivos desde nuestra m\u00e1quina local/anfitri\u00f3n a nuestra m\u00e1quina virtual Debian/servidor remoto? A d\u00eda de hoy el proceso m\u00e1s sencillo y seguro es a trav\u00e9s de Github como hemos visto antes. No obstante, el curr\u00edculum de la Conseller\u00eda d'Educaci\u00f3 me obliga a ense\u00f1aros un m\u00e9todo un tanto obsoleto a d\u00eda de hoy, as\u00ed que vamos a ello, os presento al FTP. El FTP es un protocolo de transferencia de archivos entre sistemas conectados a una red TCP. Como su nombre indica, se trata de un protocolo que permite transferir archivos directamente de un dispositivo a otro. Actualmente, es un protocolo que poco a poco va abandon\u00e1ndose, pero ha estado vigente m\u00e1s de 50 a\u00f1os. El protocolo FTP tal cual es un protocolo inseguro, ya que su informaci\u00f3n no viaja cifrada. Sin embargo, en 2001 esto se solucion\u00f3 con el protocolo SFTP , que le a\u00f1ade una capa SSH para hacerlo m\u00e1s seguro y privado. SFTP no es m\u00e1s que el mismo protocolo FTP pero implementado por un canal seguro. Son las siglas de SSH File Transfer Protocol y consiste en una extensi\u00f3n de Secure Shell Protocol (SSH) creada para poder hacer transmisiones de archivos. La seguridad que nos aporta SFTP es importante para la transferencia de archivos porque, si no disponemos de ella, los archivos viajar\u00e1n tal cual por la red, sin ning\u00fan tipo de encriptaci\u00f3n. As\u00ed pues, usando FTP tradicional, si alg\u00fan agente consigue escuchar las transferencias, podr\u00eda ocurrir que la informaci\u00f3n quedase al descubierto. Esto ser\u00eda especialmente importante si los archivos que subimos contienen informaci\u00f3n confidencial o datos personales. Dado que usar SFTP aporta mayor seguridad a las transmisiones, es recomendable utilizarlo, m\u00e1s a\u00fan sabiendo que realmente no hay mucha dificultad en establecer las conexiones por el protocolo seguro. Configurar servidor SFTP en Debian En primer lugar, lo instalaremos desde los repositorios: sudo apt-get update sudo apt-get install vsftpd Ahora vamos a crear una carpeta en nuestro home en Debian: mkdir /home/nombre_usuario/ftp En la configuraci\u00f3n de vsftpd indicaremos que este ser\u00e1 el directorio al cual vsftpd se cambia despu\u00e9s de conectarse el usuario. Ahora vamos a crear los certificados de seguridad necesarios para aportar la capa de cifrado a nuestra conexi\u00f3n (algo parecido a HTTPS) sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/vsftpd.pem -out /etc/ssl/private/vsftpd.pem Y una vez realizados estos pasos, procedemos a realizar la configuraci\u00f3n de vsftpd propiamente dicha. Se trata, con el editor de texto que m\u00e1s os guste, de editar el archivo de configuraci\u00f3n de este servicio, por ejemplo con nano : sudo nano /etc/vsftpd.conf En primer lugar, buscaremos las siguientes l\u00edneas del archivo y las eliminaremos por completo: rsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem rsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.key ssl_enable=NO Tras ello, a\u00f1adiremos estas l\u00edneas en su lugar rsa_cert_file=/etc/ssl/private/vsftpd.pem rsa_private_key_file=/etc/ssl/private/vsftpd.pem ssl_enable=YES allow_anon_ssl=NO force_local_data_ssl=YES force_local_logins_ssl=YES ssl_tlsv1=YES ssl_sslv2=NO ssl_sslv3=NO require_ssl_reuse=NO ssl_ciphers=HIGH local_root=/home/nombre_usuario/ftp Y, tras guardar los cambios, reiniciamos el servicio para que coja la nueva configuraci\u00f3n: sudo systemctl restart --now vsftpd Tarea Configura un nuevo dominio (nombre web) para el .zip con el nuevo sitio web que os proporcionado. En este caso deb\u00e9is transferir los archivos a vuestra Debian mediante SFTP. Tras acabar esta configuraci\u00f3n, ya podremos acceder a nuestro servidor mediante un cliente FTP adecuado, como por ejemplo Filezilla de dos formas, a saber: Mediante el puerto por defecto del protocolo inseguro FTP, el 21, pero utilizando certificados que cifran el intercambio de datos convirti\u00e9ndolo as\u00ed en seguro Haciendo uso del protocolo SFTP , dedicado al intercambio de datos mediante una conexi\u00f3n similar a SSH, utilizando de hecho el puerto 22. Tras descargar el cliente FTP en nuestro ordenador, introducimos los datos necesarios para conectarnos a nuestro servidor FTP en Debian: La IP de Debian (recuadro rojo) El nombre de usuario de Debian (recuadro verde) La contrase\u00f1a de ese usuario (recuadro fucsia) El puerto de conexi\u00f3n, que ser\u00e1 el 21 para conectarnos utilizando los certificados generados previamente (recuadro marr\u00f3n) Tras darle al bot\u00f3n de Conexi\u00f3n r\u00e1pida , nos saltar\u00e1 un aviso a prop\u00f3sito del certificado, le damos a aceptar puesto que no entra\u00f1a peligro ya que lo hemos genrado nosotros mismos: Nos conectaremos directamente a la carpeta que le hab\u00edamos indicado en el archivo de configuraci\u00f3n /home/raul/ftp Una vez conectados, buscamos la carpeta de nuestro ordenador donde hemos descargado el .zip (en la parte izquierda de la pantalla) y en la parte derecha de la pantalla, buscamos la carpeta donde queremos subirla. Con un doble click o utilizando bot\u00f3n derecho > subir , la subimos al servidor. Si lo que quisi\u00e9ramos es conectarnos por SFTP , exactamente igual de v\u00e1lido, har\u00edamos: Fij\u00e1os que al utilizar las claves de SSH que ya estamos utilizando desde la Pr\u00e1ctica 1, no se debe introducir la contrase\u00f1a, \u00fanicamente el nombre de usuario. Puesto que nos estamos conectando usando las claves FTP, nos sale el mismo aviso que nos sal\u00eda al conectarnos por primera vez por SSH a nuestra Debian, que aceptamos porque sabemos que no entra\u00f1a ning\u00fan peligro en este caso: Y vemos que al ser una especie de conexi\u00f3n SSH, nos conecta al home del usuario, en lugar de a la carpeta ftp . A partir de aqu\u00ed ya proceder\u00edamos igual que en el otro caso. Recordemos que debemos tener nuestro sitio web en la carpeta /var/www y darle los permisos adecuados, de forma similiar a c\u00f3mo hemos hecho con el otro sitio web. El comando que nos permite descomprimir un .zip en un directorio concreto es: unzip archivo.zip -d /nombre/directorio Si no tuvier\u00e1is unzip instalado, lo instal\u00e1is: sudo apt-get update && sudo apt-get install unzip Cuestiones finales Cuesti\u00f3n 1 \u00bfQu\u00e9 pasa si no hago el link simb\u00f3lico entre sites-available y sites-enabled de mi sitio web? Cuesti\u00f3n 2 \u00bfQu\u00e9 pasa si no le doy los permisos adecuados a /var/www/nombre_web ? Evaluaci\u00f3n Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta del servidor web 1 puntos Comprobaci\u00f3n del correcto funcionamento del primer sitio web 3 puntos Configuraci\u00f3n correcta y comprobaci\u00f3n del funcionamento de una segunda web 2 puntos Cuestiones finales 2 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Pr\u00e0ctica 2.1 \u2013 Instalaci\u00f3n y configuraci\u00f3n de servidor web Nginx"},{"location":"P1.1/#practica-21-instalacion-y-configuracion-de-servidor-web-nginx","text":"","title":"Pr\u00e0ctica 2.1 \u2013 Instalaci\u00f3n y configuraci\u00f3n de servidor web Nginx"},{"location":"P1.1/#instalacion-servidor-web-nginx","text":"Para instalar el servidor nginx en nuestra Debian, primero actualizamos los repositorios y despu\u00e9s instalamos el paquete correspondiente: sudo apt update sudo apt install nginx Comprobamos que nginx se ha instalado y que est\u00e1 funcionando correctamente: systemctl status nginx Info Esta pr\u00e1ctica se ha hecho con Nginx 1.18.0","title":"Instalaci\u00f3n servidor web Nginx"},{"location":"P1.1/#creacion-de-las-carpeta-del-sitio-web","text":"Igual que ocurre en Apache, todos los archivos que formar\u00e1n parte de un sitio web que servir\u00e1 nginx se organizar\u00e1n en carpetas. Estas carpetas, t\u00edpicamente est\u00e1n dentro de /var/www . As\u00ed pues, vamos a crear la carpeta de nuestro sitio web o dominio: sudo mkdir -p /var/www/nombre_web/html Donde el nombre de dominio puede ser la palabra que quer\u00e1is, sin espacios. Ah\u00ed, dentro de esa carpeta html, deb\u00e9is clonar el siguiente repositorio: https://github.com/cloudacademy/static-website-example Adem\u00e1s, haremos que el propietario de esta carpeta y todo lo que haya dentro sea el usuario www-data , t\u00edpicamente el usuario del servicio web. sudo chown -R www-data:www-data /var/www/nombre_web/html Y le daremos los permisos adecuados para que no nos de un error de acceso no autorizado al entrar en el sitio web: sudo chmod -R 755 /var/www/nombre_web Para comprobar que el servidor est\u00e1 funcionando y sirviendo p\u00e1ginas correctamente, pod\u00e9is acceder desde vuestro cliente a: http://IP-maq-virtual Y os deber\u00e1 aparecer algo as\u00ed: Lo que demuestra que todo es correcto hasta ahora.","title":"Creaci\u00f3n de las carpeta del sitio web"},{"location":"P1.1/#configuracion-de-servidor-web-nginx","text":"En Nginx hay dos rutas importantes. La primera de ellas es sites-available , que contiene los archivos de configuraci\u00f3n de los hosts virtuales o bloques disponibles en el servidor. Es decir, cada uno de los sitios webs que alberga el servido. La otra es sites-enabled , que contiene los archivos de configuraci\u00f3n de los sitios habilitados, es decir, los que funcionan en ese momento. Dentro de sites-available hay un archivo de configuraci\u00f3n por defecto (default), que es la p\u00e1gina que se muestra si accedemos al servidor sin indicar ning\u00fan sitio web o cuando el sitio web no es encontrado en el servidor (debido a una mala configuraci\u00f3n por ejemplo). Esta es la p\u00e1gina que nos ha aparecido en el apartado anterior. Para que Nginx presente el contenido de nuestra web, es necesario crear un bloque de servidor con las directivas correctas. En vez de modificar el archivo de configuraci\u00f3n predeterminado directamente, crearemos uno nuevo en /etc/nginx/sites-available/nombre_web : sudo nano /etc/nginx/sites-available/vuestro_dominio Y el contenido de ese archivo de configuraci\u00f3n: server { listen 80 ; listen [::]:80; root /ruta/absoluta/archivo/index ; index index.html index.htm index.nginx-debian.html; server_name nombre_web; location / { try_files $uri $uri/ =404; } } Aqu\u00ed la directiva root debe ir seguida de la ruta absoluta absoluta d\u00f3nde se encuentre el archivo index.html de nuestra p\u00e1gina web, que se encuentra entre todos los que hab\u00e9is descomprimido. Aqu\u00ed ten\u00e9is un ejemplo de un sitio webs con su ruta (directorios que hay) antes del archivo index.html: Info Ruta \u2192 /var/www/ejemplo2/html/2016_soft_landing Y crearemos un archivo simb\u00f3lico entre este archivo y el de sitios que est\u00e1n habilitados, para que se d\u00e9 de alta autom\u00e1ticamente. sudo ln -s /etc/nginx/sites-available/nombre_web /etc/nginx/sites-enabled/ Y reiniciamos el servidor para aplicar la configuraci\u00f3n: sudo systemctl restart nginx","title":"Configuraci\u00f3n de servidor web NGINX"},{"location":"P1.1/#comprobaciones","text":"","title":"Comprobaciones"},{"location":"P1.1/#comprobacion-del-correcto-funcionamiento","text":"Como a\u00fan no poseemos un servidor DNS que traduzca los nombres a IPs, debemos hacerlo de forma manual. Vamos a editar el archivo /etc/hosts de nuestra m\u00e1quina anfitriona para que asocie la IP de la m\u00e1quina virtual, a nuestro server_name . Este archivo, en Linux, est\u00e1 en: /etc/hosts Y en Windows: C:\\Windows\\System32\\drivers\\etc\\hosts Y deberemos a\u00f1adirle la l\u00ednea: 192.168.X.X nombre_web donde deb\u00e9is sustituir la IP por la que tenga vuestra m\u00e1quina virtual.","title":"Comprobaci\u00f3n del correcto funcionamiento"},{"location":"P1.1/#comprobar-registros-del-servidor","text":"Comprobad que las peticiones se est\u00e1n registrando correctamente en los archivos de logs, tanto las correctas como las err\u00f3neas: /var/log/nginx/access.log : cada solicitud a su servidor web se registra en este archivo de registro, a menos que Nginx est\u00e9 configurado para hacer algo diferente. /var/log/nginx/error.log : cualquier error de Nginx se asentar\u00e1 en este registro. Info Si no os aparece nada en los logs, podr\u00eda pasar que el navegador ha cacheado la p\u00e1gina web y que, por tanto, ya no est\u00e1 obteniendo la p\u00e1gina del navegador sino de la propia memoria. Para solucionar esto, pod\u00e9is acceder con el modo privado del navegador y ya os deber\u00eda registrar esa actividad en los logs.","title":"Comprobar registros del servidor"},{"location":"P1.1/#ftp","text":"Si queremos tener varios dominios o sitios web en el mismo servidor nginx (es decir, que tendr\u00e1n la misma IP) debemos repetir todo el proceso anterior con el nuevo nombre de dominio que queramos configurar.","title":"FTP"},{"location":"P1.1/#como-transferir-archivos-desde-nuestra-maquina-localanfitrion-a-nuestra-maquina-virtual-debianservidor-remoto","text":"A d\u00eda de hoy el proceso m\u00e1s sencillo y seguro es a trav\u00e9s de Github como hemos visto antes. No obstante, el curr\u00edculum de la Conseller\u00eda d'Educaci\u00f3 me obliga a ense\u00f1aros un m\u00e9todo un tanto obsoleto a d\u00eda de hoy, as\u00ed que vamos a ello, os presento al FTP. El FTP es un protocolo de transferencia de archivos entre sistemas conectados a una red TCP. Como su nombre indica, se trata de un protocolo que permite transferir archivos directamente de un dispositivo a otro. Actualmente, es un protocolo que poco a poco va abandon\u00e1ndose, pero ha estado vigente m\u00e1s de 50 a\u00f1os. El protocolo FTP tal cual es un protocolo inseguro, ya que su informaci\u00f3n no viaja cifrada. Sin embargo, en 2001 esto se solucion\u00f3 con el protocolo SFTP , que le a\u00f1ade una capa SSH para hacerlo m\u00e1s seguro y privado. SFTP no es m\u00e1s que el mismo protocolo FTP pero implementado por un canal seguro. Son las siglas de SSH File Transfer Protocol y consiste en una extensi\u00f3n de Secure Shell Protocol (SSH) creada para poder hacer transmisiones de archivos. La seguridad que nos aporta SFTP es importante para la transferencia de archivos porque, si no disponemos de ella, los archivos viajar\u00e1n tal cual por la red, sin ning\u00fan tipo de encriptaci\u00f3n. As\u00ed pues, usando FTP tradicional, si alg\u00fan agente consigue escuchar las transferencias, podr\u00eda ocurrir que la informaci\u00f3n quedase al descubierto. Esto ser\u00eda especialmente importante si los archivos que subimos contienen informaci\u00f3n confidencial o datos personales. Dado que usar SFTP aporta mayor seguridad a las transmisiones, es recomendable utilizarlo, m\u00e1s a\u00fan sabiendo que realmente no hay mucha dificultad en establecer las conexiones por el protocolo seguro.","title":"\u00bfC\u00f3mo transferir archivos desde nuestra m\u00e1quina local/anfitri\u00f3n a nuestra m\u00e1quina virtual Debian/servidor remoto?"},{"location":"P1.1/#configurar-servidor-sftp-en-debian","text":"En primer lugar, lo instalaremos desde los repositorios: sudo apt-get update sudo apt-get install vsftpd Ahora vamos a crear una carpeta en nuestro home en Debian: mkdir /home/nombre_usuario/ftp En la configuraci\u00f3n de vsftpd indicaremos que este ser\u00e1 el directorio al cual vsftpd se cambia despu\u00e9s de conectarse el usuario. Ahora vamos a crear los certificados de seguridad necesarios para aportar la capa de cifrado a nuestra conexi\u00f3n (algo parecido a HTTPS) sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/vsftpd.pem -out /etc/ssl/private/vsftpd.pem Y una vez realizados estos pasos, procedemos a realizar la configuraci\u00f3n de vsftpd propiamente dicha. Se trata, con el editor de texto que m\u00e1s os guste, de editar el archivo de configuraci\u00f3n de este servicio, por ejemplo con nano : sudo nano /etc/vsftpd.conf En primer lugar, buscaremos las siguientes l\u00edneas del archivo y las eliminaremos por completo: rsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem rsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.key ssl_enable=NO Tras ello, a\u00f1adiremos estas l\u00edneas en su lugar rsa_cert_file=/etc/ssl/private/vsftpd.pem rsa_private_key_file=/etc/ssl/private/vsftpd.pem ssl_enable=YES allow_anon_ssl=NO force_local_data_ssl=YES force_local_logins_ssl=YES ssl_tlsv1=YES ssl_sslv2=NO ssl_sslv3=NO require_ssl_reuse=NO ssl_ciphers=HIGH local_root=/home/nombre_usuario/ftp Y, tras guardar los cambios, reiniciamos el servicio para que coja la nueva configuraci\u00f3n: sudo systemctl restart --now vsftpd Tarea Configura un nuevo dominio (nombre web) para el .zip con el nuevo sitio web que os proporcionado. En este caso deb\u00e9is transferir los archivos a vuestra Debian mediante SFTP. Tras acabar esta configuraci\u00f3n, ya podremos acceder a nuestro servidor mediante un cliente FTP adecuado, como por ejemplo Filezilla de dos formas, a saber: Mediante el puerto por defecto del protocolo inseguro FTP, el 21, pero utilizando certificados que cifran el intercambio de datos convirti\u00e9ndolo as\u00ed en seguro Haciendo uso del protocolo SFTP , dedicado al intercambio de datos mediante una conexi\u00f3n similar a SSH, utilizando de hecho el puerto 22. Tras descargar el cliente FTP en nuestro ordenador, introducimos los datos necesarios para conectarnos a nuestro servidor FTP en Debian: La IP de Debian (recuadro rojo) El nombre de usuario de Debian (recuadro verde) La contrase\u00f1a de ese usuario (recuadro fucsia) El puerto de conexi\u00f3n, que ser\u00e1 el 21 para conectarnos utilizando los certificados generados previamente (recuadro marr\u00f3n) Tras darle al bot\u00f3n de Conexi\u00f3n r\u00e1pida , nos saltar\u00e1 un aviso a prop\u00f3sito del certificado, le damos a aceptar puesto que no entra\u00f1a peligro ya que lo hemos genrado nosotros mismos: Nos conectaremos directamente a la carpeta que le hab\u00edamos indicado en el archivo de configuraci\u00f3n /home/raul/ftp Una vez conectados, buscamos la carpeta de nuestro ordenador donde hemos descargado el .zip (en la parte izquierda de la pantalla) y en la parte derecha de la pantalla, buscamos la carpeta donde queremos subirla. Con un doble click o utilizando bot\u00f3n derecho > subir , la subimos al servidor. Si lo que quisi\u00e9ramos es conectarnos por SFTP , exactamente igual de v\u00e1lido, har\u00edamos: Fij\u00e1os que al utilizar las claves de SSH que ya estamos utilizando desde la Pr\u00e1ctica 1, no se debe introducir la contrase\u00f1a, \u00fanicamente el nombre de usuario. Puesto que nos estamos conectando usando las claves FTP, nos sale el mismo aviso que nos sal\u00eda al conectarnos por primera vez por SSH a nuestra Debian, que aceptamos porque sabemos que no entra\u00f1a ning\u00fan peligro en este caso: Y vemos que al ser una especie de conexi\u00f3n SSH, nos conecta al home del usuario, en lugar de a la carpeta ftp . A partir de aqu\u00ed ya proceder\u00edamos igual que en el otro caso. Recordemos que debemos tener nuestro sitio web en la carpeta /var/www y darle los permisos adecuados, de forma similiar a c\u00f3mo hemos hecho con el otro sitio web. El comando que nos permite descomprimir un .zip en un directorio concreto es: unzip archivo.zip -d /nombre/directorio Si no tuvier\u00e1is unzip instalado, lo instal\u00e1is: sudo apt-get update && sudo apt-get install unzip","title":"Configurar servidor SFTP en Debian"},{"location":"P1.1/#cuestiones-finales","text":"Cuesti\u00f3n 1 \u00bfQu\u00e9 pasa si no hago el link simb\u00f3lico entre sites-available y sites-enabled de mi sitio web? Cuesti\u00f3n 2 \u00bfQu\u00e9 pasa si no le doy los permisos adecuados a /var/www/nombre_web ?","title":"Cuestiones finales"},{"location":"P1.1/#evaluacion","text":"Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta del servidor web 1 puntos Comprobaci\u00f3n del correcto funcionamento del primer sitio web 3 puntos Configuraci\u00f3n correcta y comprobaci\u00f3n del funcionamento de una segunda web 2 puntos Cuestiones finales 2 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Evaluaci\u00f3n"},{"location":"P1.2/","text":"Pr\u00e1ctica 2.2 \u2013 Autenticaci\u00f3n en Nginx Requisitos antes de comenzar la pr\u00e1ctica Atenci\u00f3n, muy importante antes de empezar La pr\u00e1ctica 2.1 ha de estar funcionando correctamente No empezar la pr\u00e1ctica antes de tener la 2.1 funcionando y comprobada Introducci\u00f3n En el contexto de una transacci\u00f3n HTTP, la autenticaci\u00f3n de acceso b\u00e1sica es un m\u00e9todo dise\u00f1ado para permitir a un navegador web, u otro programa cliente, proveer credenciales en la forma de usuario y contrase\u00f1a cuando se le solicita una p\u00e1gina al servidor. La autenticaci\u00f3n b\u00e1sica, como su nombre lo indica, es la forma m\u00e1s b\u00e1sica de autenticaci\u00f3n disponible para las aplicaciones Web. Fue definida por primera vez en la especificaci\u00f3n HTTP en s\u00ed y no es de ninguna manera elegante, pero cumple su funci\u00f3n. Este tipo de autenticaci\u00f3n es el tipo m\u00e1s simple disponible pero adolece de importantes problemas de seguridad que no la hacen recomendable en muchas situaciones. No requiere el uso ni de cookies, ni de identificadores de sesi\u00f3n, ni de p\u00e1gina de ingreso. Paquetes necesarios Para esta pr\u00e1ctica podemos utilizar la herramienta openssl para crear las contrase\u00f1as. En primer lugar debemos comprobar si el paquete est\u00e1 instalado: dpkg -l | grep openssl Y si no lo estuviera, instalarlo. Creaci\u00f3n de usuarios y contrase\u00f1as para el acceso web Crearemos un archivo oculto llamado \u201c.htpasswd\u201d en el directorio de configuraci\u00f3n /etc/nginx donde guardar nuestros usuarios y contrase\u00f1as (la -c es para crear el archivo): sudo sh -c \"echo -n 'vuestro_nombre:' >> /etc/nginx/.htpasswd\" Ahora crearemos un pasword cifrado para el usuario: sudo sh -c \"openssl passwd -apr1 >> /etc/nginx/.htpasswd\" Este proceso se podr\u00e1 repetir para tantos usuarios como haga falta. Crea dos usuarios, uno con tu nombre y otro con tu primer apellido Comprueba que el usuario y la contrase\u00f1a aparecen cifrados en el fichero: cat /etc/nginx/.htpasswd Configurando el servidor Nginx para usar autenticaci\u00f3n b\u00e1sica Editaremos la configuraci\u00f3n del server block sobre el cual queremos aplicar la restricci\u00f3n de acceso. Utilizaremos para esta autenticaci\u00f3n el sitio web de Perfect Learn : Info Recuerda que un server block es cada uno de los dominios ( server {...} dentro del archivo de configuraci\u00f3n) de alguno de los sitios web que hay en el seridor. sudo nano /etc/nginx/sites-available/nombre_web Debemos decidir qu\u00e9 recursos estar\u00e1n protegidos. Nginx permite a\u00f1adir restricciones a nivel de servidor o en un location (directorio o archivo) espec\u00edfico. Para nuestro ejemplo, vamos a proteger el document root (la ra\u00edz, la p\u00e1gina principal) de nuestro sitio. Utilizaremos la directiva auth_basic dentro del location y le pondremos el nombre a nuestro dominio que ser\u00e1 mostrado al usuario al solicitar las credenciales. Por \u00faltimo, configuramos Nginx para que utilice el fichero que previamente hemos creado con la directiva auth_basic_user_file : server { listen 80 ; listen [::]:80; root /var/www/webraul/html/simple-static-website ; index index.html index.htm index.nginx-debian.html; server_name nombre_web; location / { auth_basic \"\u00c1rea restringida\" ; auth_basic_user_file /etc/nginx/.htpasswd ; try_files $uri $uri/ =404; } } Una vez terminada la configuraci\u00f3n, reiniciamos el servicio para que aplique nuestra pol\u00edtica de acceso. sudo systemctl restart nginx Probando la nueva configuraci\u00f3n Comprobaci\u00f3n 1 Comprueba desde tu m\u00e1quina f\u00edsica/anfitri\u00f3n que puedes acceder a http://nombre-sitio-web y que se te solicita autenticaci\u00f3n Comprobaci\u00f3n 2 Comprueba que si decides cancelar la autenticaci\u00f3n, se te negar\u00e1 el acceso al sitio con un error. \u00bfQu\u00e9 error es? Cuidado Una vez os autentic\u00e1is con \u00e9xito, el navegador guardar\u00e1 esta autencaci\u00f3n exitosa y no volver\u00e1 a pediros usuario/contrase\u00f1a . Llegados a ese punto, si quer\u00e9is volver a probar a autenticaros, tendr\u00e9is que abriros una Nueva ventana privada del navegador. Tareas Tarea 1 Intenta entrar primero con un usuario err\u00f3neo y luego con otro correcto. Puedes ver todos los sucesos y registros en los logs access.log y error.log Adjunta una captura de pantalla de los logs donde se vea que intentas entrar primero con un usuario inv\u00e1lido y con otro v\u00e1lido. Indica d\u00f3nde podemos ver los errores de usuario inv\u00e1lido o no encontrado, as\u00ed como donde podemos ver el n\u00famero de error que os aparec\u00eda antes Cuando hemos configurado el siguiente bloque: location / { auth_basic \"\u00c0rea restringida\" ; auth_basic_user_file /etc/nginx/.htpasswd ; try_files $uri $uri/ =404; } La autenticaci\u00f3n se aplica al directorio/archivo que le indicamos en la declaraci\u00f3n del location y que en este caso el ra\u00edz / . As\u00ed pues, esta restricci\u00f3n se aplica al directorio ra\u00edz o base donde residen los archivos del sitio web y que es: /var/www/webraul/html/simple-static-website Y a todos los archivos que hay dentro, ya que no hemos especificado ninguno en concreto. Ahora bien, vamos a probar a aplicar autenticaci\u00f3n s\u00f3lo a una parte de la web. Vamos a intentar que s\u00f3lo se necesite autenticac\u00ed\u00f3n para entrar a la parte de portfolio: Esta secci\u00f3n se corresponde con el archivo contact.html dentro del directorio ra\u00edz. Tarea 2 Borra las dos l\u00edneas que hacen referencia a la autenticaci\u00f3n b\u00e1sica en el location del directorio ra\u00edz. Tras ello, a\u00f1ade un nuevo location debajo con la autenticaci\u00f3n b\u00e1sica para el archivo/secci\u00f3n contact.html \u00fanicamente. Warning Fij\u00e1os que deb\u00e9is tener cuidado porque la \u00faltima l\u00ednea del archivo ha de ser } que cierra la primera l\u00ednea server { del archivo. Combinaci\u00f3n de la autenticaci\u00f3n b\u00e1sica con la restricci\u00f3n de acceso por IP La autenticaci\u00f3n b\u00e1sica HTTP puede ser combinada de forma efectiva con la restricci\u00f3n de acceso por direcci\u00f3n IP. Se pueden implementar dos escenario: Un usuario debe estar ambas cosas, autenticado y tener una IP v\u00e1lida Un usuario debe o bien estar autenticado, o bien tener una IP v\u00e1lida Veamos c\u00f3mo lo har\u00edamos: Como permitir o denegar acceso sobre una IP concreta (directivas allow y deny , respectivamente). Dentro del block server o archivo de configuraci\u00f3n del dominio web, que recordad est\u00e1 en el directorio sites-available: El acceso se garantizar\u00e1 ala IP 192.168.1.1/24 , excluyendo a la direcci\u00f3n 192.168.1.2 . Hay que tener en cuenta que las directivas allow y deny se ir\u00e1n aplicando en el orden en el que aparecen el archivo. Aqu\u00ed aplican sobre la location /api (esto es s\u00f3lo un ejemplo de un hipot\u00e9tico directorio o archivo), pero podr\u00edan aplicar sobre cualquiera, incluida todo el sitio web, la location ra\u00edz / . La \u00faltima directiva deny all quiere decir que por defecto denegaremos el acceso a todo el mundo. Por eso hay que poner los allow y deny m\u00e1s espec\u00edficos justo antes de esta, porque al evaluarse en orden de aparici\u00f3n, si los pusi\u00e9ramos debajo se denegar\u00eda el acceso a todo el mundo, puesto que deny all ser\u00eda lo primero que se evaluar\u00eda. Combinar la restricci\u00f3n IP y la autenticaci\u00f3n HTTP con la directiva satisfy . Si establecemos el valor de la directiva a \u201call\u201d, el acceso se permite si el cliente satisface ambas condiciones (IP y usario v\u00e1lido). Si lo establecemos a \u201cany\u201d, el acceso se permite si se satisface al menos una de las dos condiciones. Tareas Tarea 1 Configura Nginx para que no deje acceder con la IP de la m\u00e1quina anfitriona al directorio ra\u00edz de una de tus dos webs. Modifica su server block o archivo de configuraci\u00f3n. Comprueba como se deniega el acceso: Muestra la p\u00e1gina de error en el navegador Muestra el mensaje de error de error.log Tarea 2 Configura Nginx para que desde tu m\u00e1quina anfitriona se tenga que tener tanto una IP v\u00e1lida como un usuario v\u00e1lido, ambas cosas a la vez, y comprueba que s\u00ed puede acceder sin problemas Cuestiones finales Cuesti\u00f3n 1 Supongamos que yo soy el cliente con la IP 172.1.10.15 e intento acceder al directorio web_muy_guay de mi sitio web, equivoc\u00e1ndome al poner el usuario y contrase\u00f1a. \u00bfPodr\u00e9 acceder?\u00bfPor qu\u00e9? location /web_muy_guay { #... satisfy all ; deny 172.1.10.6 ; allow 172.1.10.15 ; allow 172.1.3.14 ; deny all ; auth_basic \"Cuesti\u00f3n final 1\" ; auth_basic_user_file conf/htpasswd; } Cuesti\u00f3n 2 ask \"Cuesti\u00f3n 1\" Supongamos que yo soy el cliente con la IP 172.1.10.15 e intento acceder al directorio web_muy_guay de mi sitio web, introduciendo correctamente usuari y contrase\u00f1a. \u00bfPodr\u00e9 acceder?\u00bfPor qu\u00e9? location /web_muy_guay { #... satisfy all ; deny all ; deny 172.1.10.6 ; allow 172.1.10.15 ; allow 172.1.3.14 ; auth_basic \"Cuesti\u00f3n final 2: The revenge\" ; auth_basic_user_file conf/htpasswd; } Cuesti\u00f3n 3 Supongamos que yo soy el cliente con la IP 172.1.10.15 e intento acceder al directorio web_muy_guay de mi sitio web, introduciendo correctamente usuario y contrase\u00f1a. \u00bfPodr\u00e9 acceder?\u00bfPor qu\u00e9? location /web_muy_guay { #... satisfy any ; deny 172.1.10.6 ; deny 172.1.10.15 ; allow 172.1.3.14 ; auth_basic \"Cuesti\u00f3n final 3: The final combat\" ; auth_basic_user_file conf/htpasswd; } Cuesti\u00f3n 4 A lo mejor no sab\u00e9is que tengo una web para documentar todas mis excursiones espaciales con Jeff, es esta: Jeff Bezos y yo Supongamos que quiero restringir el acceso al directorio de proyectos porque es muy secreto, eso quiere decir a\u00f1adir autenticaci\u00f3n b\u00e1sica a la URL: Proyectos Completa la configuraci\u00f3n para conseguirlo: server { listen 80 ; listen [::]:80; root /var/www/freewebsitetemplates.com/preview/space-science ; index index.html index.htm index.nginx-debian.html; server_name freewebsitetemplates.com www.freewebsitetemplates.com; location { try_files $uri $uri/ =404; } } Evaluaci\u00f3n Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta de la autorizaci\u00f3n b\u00e1sica de Nginx, comprobaci\u00f3n e identificaci\u00f3n del error 2 puntos Capturas correctas del log 1 puntos Configuraci\u00f3n correcta de la autorizaci\u00f3n b\u00e1sica en contact 1.5 puntos Correcta configuraci\u00f3n y comprobaci\u00f3n de las tareas de autenticaci\u00f3n b\u00e1sica y restricci\u00f3n por IP 1.5 puntos Cuestiones finales 2 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Pr\u00e1ctica 2.2 \u2013 Autenticaci\u00f3n en Nginx"},{"location":"P1.2/#practica-22-autenticacion-en-nginx","text":"","title":"Pr\u00e1ctica 2.2 \u2013 Autenticaci\u00f3n en Nginx"},{"location":"P1.2/#requisitos-antes-de-comenzar-la-practica","text":"Atenci\u00f3n, muy importante antes de empezar La pr\u00e1ctica 2.1 ha de estar funcionando correctamente No empezar la pr\u00e1ctica antes de tener la 2.1 funcionando y comprobada","title":"Requisitos antes de comenzar la pr\u00e1ctica"},{"location":"P1.2/#introduccion","text":"En el contexto de una transacci\u00f3n HTTP, la autenticaci\u00f3n de acceso b\u00e1sica es un m\u00e9todo dise\u00f1ado para permitir a un navegador web, u otro programa cliente, proveer credenciales en la forma de usuario y contrase\u00f1a cuando se le solicita una p\u00e1gina al servidor. La autenticaci\u00f3n b\u00e1sica, como su nombre lo indica, es la forma m\u00e1s b\u00e1sica de autenticaci\u00f3n disponible para las aplicaciones Web. Fue definida por primera vez en la especificaci\u00f3n HTTP en s\u00ed y no es de ninguna manera elegante, pero cumple su funci\u00f3n. Este tipo de autenticaci\u00f3n es el tipo m\u00e1s simple disponible pero adolece de importantes problemas de seguridad que no la hacen recomendable en muchas situaciones. No requiere el uso ni de cookies, ni de identificadores de sesi\u00f3n, ni de p\u00e1gina de ingreso.","title":"Introducci\u00f3n"},{"location":"P1.2/#paquetes-necesarios","text":"Para esta pr\u00e1ctica podemos utilizar la herramienta openssl para crear las contrase\u00f1as. En primer lugar debemos comprobar si el paquete est\u00e1 instalado: dpkg -l | grep openssl Y si no lo estuviera, instalarlo.","title":"Paquetes necesarios"},{"location":"P1.2/#creacion-de-usuarios-y-contrasenas-para-el-acceso-web","text":"Crearemos un archivo oculto llamado \u201c.htpasswd\u201d en el directorio de configuraci\u00f3n /etc/nginx donde guardar nuestros usuarios y contrase\u00f1as (la -c es para crear el archivo): sudo sh -c \"echo -n 'vuestro_nombre:' >> /etc/nginx/.htpasswd\" Ahora crearemos un pasword cifrado para el usuario: sudo sh -c \"openssl passwd -apr1 >> /etc/nginx/.htpasswd\" Este proceso se podr\u00e1 repetir para tantos usuarios como haga falta. Crea dos usuarios, uno con tu nombre y otro con tu primer apellido Comprueba que el usuario y la contrase\u00f1a aparecen cifrados en el fichero: cat /etc/nginx/.htpasswd","title":"Creaci\u00f3n de usuarios y contrase\u00f1as para el acceso web"},{"location":"P1.2/#configurando-el-servidor-nginx-para-usar-autenticacion-basica","text":"Editaremos la configuraci\u00f3n del server block sobre el cual queremos aplicar la restricci\u00f3n de acceso. Utilizaremos para esta autenticaci\u00f3n el sitio web de Perfect Learn : Info Recuerda que un server block es cada uno de los dominios ( server {...} dentro del archivo de configuraci\u00f3n) de alguno de los sitios web que hay en el seridor. sudo nano /etc/nginx/sites-available/nombre_web Debemos decidir qu\u00e9 recursos estar\u00e1n protegidos. Nginx permite a\u00f1adir restricciones a nivel de servidor o en un location (directorio o archivo) espec\u00edfico. Para nuestro ejemplo, vamos a proteger el document root (la ra\u00edz, la p\u00e1gina principal) de nuestro sitio. Utilizaremos la directiva auth_basic dentro del location y le pondremos el nombre a nuestro dominio que ser\u00e1 mostrado al usuario al solicitar las credenciales. Por \u00faltimo, configuramos Nginx para que utilice el fichero que previamente hemos creado con la directiva auth_basic_user_file : server { listen 80 ; listen [::]:80; root /var/www/webraul/html/simple-static-website ; index index.html index.htm index.nginx-debian.html; server_name nombre_web; location / { auth_basic \"\u00c1rea restringida\" ; auth_basic_user_file /etc/nginx/.htpasswd ; try_files $uri $uri/ =404; } } Una vez terminada la configuraci\u00f3n, reiniciamos el servicio para que aplique nuestra pol\u00edtica de acceso. sudo systemctl restart nginx","title":"Configurando el servidor Nginx para usar autenticaci\u00f3n b\u00e1sica"},{"location":"P1.2/#probando-la-nueva-configuracion","text":"Comprobaci\u00f3n 1 Comprueba desde tu m\u00e1quina f\u00edsica/anfitri\u00f3n que puedes acceder a http://nombre-sitio-web y que se te solicita autenticaci\u00f3n Comprobaci\u00f3n 2 Comprueba que si decides cancelar la autenticaci\u00f3n, se te negar\u00e1 el acceso al sitio con un error. \u00bfQu\u00e9 error es? Cuidado Una vez os autentic\u00e1is con \u00e9xito, el navegador guardar\u00e1 esta autencaci\u00f3n exitosa y no volver\u00e1 a pediros usuario/contrase\u00f1a . Llegados a ese punto, si quer\u00e9is volver a probar a autenticaros, tendr\u00e9is que abriros una Nueva ventana privada del navegador.","title":"Probando la nueva configuraci\u00f3n"},{"location":"P1.2/#tareas","text":"Tarea 1 Intenta entrar primero con un usuario err\u00f3neo y luego con otro correcto. Puedes ver todos los sucesos y registros en los logs access.log y error.log Adjunta una captura de pantalla de los logs donde se vea que intentas entrar primero con un usuario inv\u00e1lido y con otro v\u00e1lido. Indica d\u00f3nde podemos ver los errores de usuario inv\u00e1lido o no encontrado, as\u00ed como donde podemos ver el n\u00famero de error que os aparec\u00eda antes Cuando hemos configurado el siguiente bloque: location / { auth_basic \"\u00c0rea restringida\" ; auth_basic_user_file /etc/nginx/.htpasswd ; try_files $uri $uri/ =404; } La autenticaci\u00f3n se aplica al directorio/archivo que le indicamos en la declaraci\u00f3n del location y que en este caso el ra\u00edz / . As\u00ed pues, esta restricci\u00f3n se aplica al directorio ra\u00edz o base donde residen los archivos del sitio web y que es: /var/www/webraul/html/simple-static-website Y a todos los archivos que hay dentro, ya que no hemos especificado ninguno en concreto. Ahora bien, vamos a probar a aplicar autenticaci\u00f3n s\u00f3lo a una parte de la web. Vamos a intentar que s\u00f3lo se necesite autenticac\u00ed\u00f3n para entrar a la parte de portfolio: Esta secci\u00f3n se corresponde con el archivo contact.html dentro del directorio ra\u00edz. Tarea 2 Borra las dos l\u00edneas que hacen referencia a la autenticaci\u00f3n b\u00e1sica en el location del directorio ra\u00edz. Tras ello, a\u00f1ade un nuevo location debajo con la autenticaci\u00f3n b\u00e1sica para el archivo/secci\u00f3n contact.html \u00fanicamente. Warning Fij\u00e1os que deb\u00e9is tener cuidado porque la \u00faltima l\u00ednea del archivo ha de ser } que cierra la primera l\u00ednea server { del archivo.","title":"Tareas"},{"location":"P1.2/#combinacion-de-la-autenticacion-basica-con-la-restriccion-de-acceso-por-ip","text":"La autenticaci\u00f3n b\u00e1sica HTTP puede ser combinada de forma efectiva con la restricci\u00f3n de acceso por direcci\u00f3n IP. Se pueden implementar dos escenario: Un usuario debe estar ambas cosas, autenticado y tener una IP v\u00e1lida Un usuario debe o bien estar autenticado, o bien tener una IP v\u00e1lida Veamos c\u00f3mo lo har\u00edamos: Como permitir o denegar acceso sobre una IP concreta (directivas allow y deny , respectivamente). Dentro del block server o archivo de configuraci\u00f3n del dominio web, que recordad est\u00e1 en el directorio sites-available: El acceso se garantizar\u00e1 ala IP 192.168.1.1/24 , excluyendo a la direcci\u00f3n 192.168.1.2 . Hay que tener en cuenta que las directivas allow y deny se ir\u00e1n aplicando en el orden en el que aparecen el archivo. Aqu\u00ed aplican sobre la location /api (esto es s\u00f3lo un ejemplo de un hipot\u00e9tico directorio o archivo), pero podr\u00edan aplicar sobre cualquiera, incluida todo el sitio web, la location ra\u00edz / . La \u00faltima directiva deny all quiere decir que por defecto denegaremos el acceso a todo el mundo. Por eso hay que poner los allow y deny m\u00e1s espec\u00edficos justo antes de esta, porque al evaluarse en orden de aparici\u00f3n, si los pusi\u00e9ramos debajo se denegar\u00eda el acceso a todo el mundo, puesto que deny all ser\u00eda lo primero que se evaluar\u00eda. Combinar la restricci\u00f3n IP y la autenticaci\u00f3n HTTP con la directiva satisfy . Si establecemos el valor de la directiva a \u201call\u201d, el acceso se permite si el cliente satisface ambas condiciones (IP y usario v\u00e1lido). Si lo establecemos a \u201cany\u201d, el acceso se permite si se satisface al menos una de las dos condiciones.","title":"Combinaci\u00f3n de la autenticaci\u00f3n b\u00e1sica con la restricci\u00f3n de acceso por IP"},{"location":"P1.2/#tareas_1","text":"Tarea 1 Configura Nginx para que no deje acceder con la IP de la m\u00e1quina anfitriona al directorio ra\u00edz de una de tus dos webs. Modifica su server block o archivo de configuraci\u00f3n. Comprueba como se deniega el acceso: Muestra la p\u00e1gina de error en el navegador Muestra el mensaje de error de error.log Tarea 2 Configura Nginx para que desde tu m\u00e1quina anfitriona se tenga que tener tanto una IP v\u00e1lida como un usuario v\u00e1lido, ambas cosas a la vez, y comprueba que s\u00ed puede acceder sin problemas","title":"Tareas"},{"location":"P1.2/#cuestiones-finales","text":"Cuesti\u00f3n 1 Supongamos que yo soy el cliente con la IP 172.1.10.15 e intento acceder al directorio web_muy_guay de mi sitio web, equivoc\u00e1ndome al poner el usuario y contrase\u00f1a. \u00bfPodr\u00e9 acceder?\u00bfPor qu\u00e9? location /web_muy_guay { #... satisfy all ; deny 172.1.10.6 ; allow 172.1.10.15 ; allow 172.1.3.14 ; deny all ; auth_basic \"Cuesti\u00f3n final 1\" ; auth_basic_user_file conf/htpasswd; } Cuesti\u00f3n 2 ask \"Cuesti\u00f3n 1\" Supongamos que yo soy el cliente con la IP 172.1.10.15 e intento acceder al directorio web_muy_guay de mi sitio web, introduciendo correctamente usuari y contrase\u00f1a. \u00bfPodr\u00e9 acceder?\u00bfPor qu\u00e9? location /web_muy_guay { #... satisfy all ; deny all ; deny 172.1.10.6 ; allow 172.1.10.15 ; allow 172.1.3.14 ; auth_basic \"Cuesti\u00f3n final 2: The revenge\" ; auth_basic_user_file conf/htpasswd; } Cuesti\u00f3n 3 Supongamos que yo soy el cliente con la IP 172.1.10.15 e intento acceder al directorio web_muy_guay de mi sitio web, introduciendo correctamente usuario y contrase\u00f1a. \u00bfPodr\u00e9 acceder?\u00bfPor qu\u00e9? location /web_muy_guay { #... satisfy any ; deny 172.1.10.6 ; deny 172.1.10.15 ; allow 172.1.3.14 ; auth_basic \"Cuesti\u00f3n final 3: The final combat\" ; auth_basic_user_file conf/htpasswd; } Cuesti\u00f3n 4 A lo mejor no sab\u00e9is que tengo una web para documentar todas mis excursiones espaciales con Jeff, es esta: Jeff Bezos y yo Supongamos que quiero restringir el acceso al directorio de proyectos porque es muy secreto, eso quiere decir a\u00f1adir autenticaci\u00f3n b\u00e1sica a la URL: Proyectos Completa la configuraci\u00f3n para conseguirlo: server { listen 80 ; listen [::]:80; root /var/www/freewebsitetemplates.com/preview/space-science ; index index.html index.htm index.nginx-debian.html; server_name freewebsitetemplates.com www.freewebsitetemplates.com; location { try_files $uri $uri/ =404; } }","title":"Cuestiones finales"},{"location":"P1.2/#evaluacion","text":"Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta de la autorizaci\u00f3n b\u00e1sica de Nginx, comprobaci\u00f3n e identificaci\u00f3n del error 2 puntos Capturas correctas del log 1 puntos Configuraci\u00f3n correcta de la autorizaci\u00f3n b\u00e1sica en contact 1.5 puntos Correcta configuraci\u00f3n y comprobaci\u00f3n de las tareas de autenticaci\u00f3n b\u00e1sica y restricci\u00f3n por IP 1.5 puntos Cuestiones finales 2 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Evaluaci\u00f3n"},{"location":"P1.3/","text":"Pr\u00e1ctica 2.3 \u2013 Proxy inverso con Nginx Requisitos antes de comenzar la pr\u00e1ctica Atenci\u00f3n, importante antes de comenzar La pr\u00e1ctica 2.1 ha de estar funcionando correctamente No comenzar la pr\u00e1ctica antes de tener la 2.1 funcionando y comprobada Introducci\u00f3n \u00bfQu\u00e9 es un servidor proxy? Un proxy de reenv\u00edo, a menudo llamado proxy, servidor proxy o proxy web, es un servidor que se encuentra frente a un grupo de m\u00e1quinas cliente. Cuando esas m\u00e1quinas realizan solicitudes a sitios y servicios en Internet, el servidor proxy intercepta esas solicitudes y luego se comunica con los servidores web en nombre de esos clientes, como un intermediario. Por ejemplo, tomemos como ejemplo 3 m\u00e1quinas involucradas en una comunicaci\u00f3n t\u00edpica de proxy de reenv\u00edo: A: Esta es la m\u00e1quina del hogar de un usuario. B: este es un servidor proxy de reenv\u00edo C: este es el servidor de origen de un sitio web (donde se almacenan los datos del sitio web) En una comunicaci\u00f3n est\u00e1ndar por Internet, la m\u00e1quina A se comunicar\u00eda directamente con la m\u00e1quina C, con el cliente enviando solicitudes al servidor de origen y el servidor de origen respondiendo al cliente. Cuando hay un proxy de reenv\u00edo, A enviar\u00e1 solicitudes a B, que luego reenviar\u00e1 la solicitud a C. C enviar\u00e1 una respuesta a B, que reenviar\u00e1 la respuesta a A. \u00bfPor qu\u00e9 agregar este intermediario adicional a nuestra actividad en Internet? Hay algunas razones por las que uno podr\u00eda querer usar un proxy de reenv\u00edo: Para evitar restricciones de navegaci\u00f3n estatales o institucionales : algunos gobiernos, escuelas y otras organizaciones usan firewalls para dar a sus usuarios acceso a una versi\u00f3n limitada de Internet. Se puede usar un proxy de reenv\u00edo para sortear estas restricciones, ya que permiten que el usuario se conecte al proxy en lugar de directamente a los sitios que est\u00e1 visitando. Para bloquear el acceso a cierto contenido : a la inversa, los proxies tambi\u00e9n se pueden configurar para bloquear el acceso de un grupo de usuarios a ciertos sitios. Por ejemplo, una red escolar puede estar configurada para conectarse a la web a trav\u00e9s de un proxy que habilita reglas de filtrado de contenido, neg\u00e1ndose a reenviar respuestas de Facebook y otros sitios de redes sociales. Para proteger su identidad en l\u00ednea : en algunos casos, los usuarios habituales de Internet simplemente desean un mayor anonimato en l\u00ednea, pero en otros casos, los usuarios de Internet viven en lugares donde el gobierno puede imponer graves consecuencias a los disidentes pol\u00edticos. Criticar al gobierno en un foro web o en las redes sociales puede dar lugar a multas o encarcelamiento para estos usuarios. Si uno de estos disidentes usa un proxy de reenv\u00edo para conectarse a un sitio web donde publica comentarios pol\u00edticamente sensibles, la direcci\u00f3n IP utilizada para publicar los comentarios ser\u00e1 m\u00e1s dif\u00edcil de rastrear hasta el disidente. Solo estar\u00e1 visible la direcci\u00f3n IP del servidor proxy. \u00bfEn qu\u00e9 se diferencia un proxy inverso? Estar\u00edamos hablando del caso opuesto al anterior. Un proxy inverso es un servidor que se encuentra frente a uno o m\u00e1s servidores web, interceptando las solicitudes de los clientes. Esto es diferente de un proxy de reenv\u00edo, donde el proxy se encuentra frente a los clientes. Con un proxy inverso, cuando los clientes env\u00edan solicitudes al servidor de un sitio web, esas solicitudes son interceptadas en la frontera de la red por el servidor proxy inverso. El servidor proxy inverso enviar\u00e1 solicitudes y recibir\u00e1 respuestas del servidor del sitio web. La diferencia entre un proxy directo y inverso es sutil pero importante. Una forma simplificada de resumir ser\u00eda decir que un proxy de reenv\u00edo se encuentra frente a un cliente y garantiza que ning\u00fan servidor de origen se comunique nunca directamente con ese cliente espec\u00edfico . Por otro lado, un proxy inverso se encuentra frente a un servidor de origen y garantiza que ning\u00fan cliente se comunique nunca directamente con ese servidor de origen . Una vez m\u00e1s, ilustremos nombrando las m\u00e1quinas involucradas: D: cualquier n\u00famero de ordenadores dom\u00e9sticos de los usuarios E: este es un servidor proxy inverso F: uno o m\u00e1s servidores de origen Normalmente, todas las solicitudes de D ir\u00edan directamente a F, y F enviar\u00eda respuestas directamente a D. Con un proxy inverso, todas las solicitudes de D ir\u00e1n directamente a E, y E enviar\u00e1 sus solicitudes ay recibir\u00e1 respuestas de F. E luego transmita las respuestas apropiadas a D. A continuaci\u00f3n se describen algunos de los beneficios de un proxy inverso: Balanceo de carga : es posible que un sitio web popular que recibe millones de usuarios todos los d\u00edas no pueda manejar todo el tr\u00e1fico entrante del sitio con un solo servidor de origen. En cambio, el sitio se puede distribuir entre un grupo de servidores diferentes, todos manejando solicitudes para el mismo sitio. En este caso, un proxy inverso puede proporcionar una soluci\u00f3n de balanceo de carga que distribuir\u00e1 el tr\u00e1fico entrante de manera uniforme entre los diferentes servidores para evitar que un solo servidor se sobrecargue. En el caso de que un servidor falle por completo, otros servidores pueden intensificar para manejar el tr\u00e1fico. Protecci\u00f3n contra ataques : con un proxy inverso en su lugar, un sitio web o servicio nunca necesita revelar la direcci\u00f3n IP de su (s) servidor (es) de origen. Esto hace que sea mucho m\u00e1s dif\u00edcil para los atacantes aprovechar un ataque dirigido contra ellos, como un ataque DdoS. Almacenamiento en cach \u00e9: un proxy inverso tambi\u00e9n puede almacenar contenido en cach\u00e9 , lo que resulta en un rendimiento m\u00e1s r\u00e1pido. Por ejemplo, si un usuario en Par\u00eds visita un sitio web con proxy inverso con servidores web en Los \u00c1ngeles, el usuario podr\u00eda conectarse a un servidor proxy inverso local en Par\u00eds, que luego tendr\u00e1 que comunicarse con un servidor de origen en Los \u00c1ngeles. El servidor proxy luego puede almacenar en cach\u00e9 (o guardar temporalmente) los datos de respuesta. Los usuarios parisinos posteriores que naveguen por el sitio obtendr\u00e1n la versi\u00f3n en cach\u00e9 local del servidor proxy inverso parisino, lo que dar\u00e1 como resultado un rendimiento mucho m\u00e1s r\u00e1pido. Cifrado SSL - Cifrado y descifrado SSL (o TLS comunicaciones) para cada cliente pueden ser computacionalmente caro para un servidor de origen. Se puede configurar un proxy inverso para descifrar todas las solicitudes entrantes y cifrar todas las respuestas salientes, liberando valiosos recursos en el servidor de origen. Tarea Configuraciones Nginx servidor web Vamos a configurar dos Debian con sendos servidores Nginx. Ten\u00e9is la m\u00e1quina virtual inicial y deb\u00e9is clonarla para tener una segunda: Uno servir\u00e1 las p\u00e1ginas web que ya hemos configurado, as\u00ed pues utilizaremos el servidor que ya tenemos configurado de la Pr\u00e1ctica 2.1. El nuevo servidor clon Debian con Nginx configurado como proxy inverso Realizaremos las peticiones HTTP desde el navegador web de nuestra m\u00e1quina f\u00edsica/anfitri\u00f3n hacia el proxy clonado, que nos redirigir\u00e1 al servidor web original Cuidado Ojo al clonar las m\u00e1quinas virtuales porque hay que darle a crear una nueva MAC, de lo contrario no tendr\u00e9is IP en esa m\u00e1quina. El diagrama de red quedar\u00eda as\u00ed: Para que todo quede m\u00e1s diferenciado y os quede m\u00e1s claro que la petici\u00f3n est\u00e1 pasando por el proxy inverso y llega al servidor web destino, vamos a hacer que cada uno de los servidores escuche las peticiones en un puerto distinto. En primer lugar, deb\u00e9is cambiar el nombre que tuviera vuestra web por el de webserver , ello implica: Cambiar el nombre del archivo de configuraci\u00f3n de sitios disponibles par Nginx Cambiar el nombre del sitio web dentro de este archivo de configuraci\u00f3n donde haga falta No os olvid\u00e9is de eliminar el link simb\u00f3lico antiguo con el comando unlink nombre_del_link dentro de la carpeta sites-enabled y crear el nuevo para el nuevo nombre de archivo. En el archivo de configuraci\u00f3n del sitio web, en lugar de hacer que el servidor escuche en el puerto 80, cambiadlo al 8080. Reiniciar Nginx Nginx proxy inverso Ahora, cuando intentamos acceder a http://ejemplo-proxy (o el nombre que tuvier\u00e1is de vuestra web de las pr\u00e1cticas anteriores), en realidad estaremos accediendo al proxy, que nos redirigir\u00e1 a http://webserver:8080 , el servidor web que acabamos de configurar para que escuche con ese nombre en el puerto 8080. Para ello: Crear un archivo de configuraci\u00f3n en sites-available con el nombre ejemplo-proxy (o el que tuvier\u00e1is vosotros) Este archivo de configuraci\u00f3n ser\u00e1 m\u00e1s simple, tendr\u00e1 la siguiente forma server { listen __; server_name ____________; location / { proxy_pass http://_________:____; } } Donde, mirando el diagrama de red y teniendo en cuenta la configuraci\u00f3n hecha hasta ahora , deb\u00e9is completar: El puerto donde est\u00e1 escuchando el proxy inverso El nombre de vuestro dominio o sitio web original al que accedemos en el proxy La directiva proxy_pass indica a d\u00f3nde se van a redirigir las peticiones, esto es, al servidor web. Por tanto, deb\u00e9is poner la IP y n\u00famero de puerto adecuados de vuestro sitio web configurado en el apartado anterior. Crear el link simb\u00f3lico pertinente Esto es para simular la situaci\u00f3n en la que nosotros, como clientes, cuando accedamos a nuestro sitio web, no necesitemos saber c\u00f3mo est\u00e1 todo configurado, s\u00f3lo necesitamos saber el nombre de la web. \u00a1Atenci\u00f3n, muy importante! Deb\u00e9is modificar el archivo host que configurast\u00e9is en la pr\u00e1ctica 2.1. Si mir\u00e1is el diagrama de red, ahora el nombre de vuestro sitio web se corresponder\u00e1 con la IP de la nueva m\u00e1quina clon que hace de proxy . Ser\u00e1 \u00e9sta la encargada de redirigirnos autom\u00e1ticamente al verdadero sitio web. Comprobaciones Si acced\u00e9is a vuestro sitio web, deb\u00e9is poder seguir accediendo sin problemas. Comprobad en los access.log de los dos servidores que llega la petici\u00f3n Comprobad adem\u00e1s la petici\u00f3n y respuesta con las herramientas de desarrollador de Firefox en Xubuntu. Pulsando F12 en el navegador os aparecer\u00e1n estas herramientas En la primera petici\u00f3n (marcada en rojo), utilizando el apartado \u201cRed\u201d (tambi\u00e9n marcado en rojo) y tambi\u00e9n en rojo est\u00e1 se\u00f1alado d\u00f3nde se puede ver la respuesta de la petici\u00f3n GET HTTP (200 OK). Tambi\u00e9n vemos las cabeceras que se incluyen en la petici\u00f3n (m\u00e9todo GET) y en la respuesta a esta petici\u00f3n. A\u00f1adiendo cabeceras Adem\u00e1s de haber mirado los logs, vamos a demostrar a\u00fan de forma m\u00e1s clara que la petici\u00f3n est\u00e1 pasando por el proxy inverso y que est\u00e1 llegando al servidor web y que vuelve por el mismo camino. Si record\u00e1is de teor\u00eda, el servidor web es capaz de a\u00f1adir cabeceras en las respuestas a las peticiones. As\u00ed pues, vamos a configurar tanto el proxy inverso como el servidor web para que a\u00f1adan cada uno la cabecera \u201cHost\u201d que tambi\u00e9n vimos en teor\u00eda. Para a\u00f1adir cabeceras, en el archivo de configuraci\u00f3n del sitio web debemos a\u00f1adir dentro del bloque location / { \u2026 } debemos a\u00f1adir la directiva: add_header Host nombre_del_host; A\u00f1adiremos primero esta cabecera \u00fanicamente en el archivo de configuraci\u00f3n del sitio web del proxy inverso. El Nombre_del_host ser\u00e1 Proxy_inverso_vuestronombre. Reiniciamos Nginx Comprobamos que podemos acceder al sitio web sin problemas Con las herramientas de desarrollador comprobamos que la petici\u00f3n ha pasado por el proxy inverso que ha a\u00f1adido la cabecera en la respuesta: Hacemos lo propio con el servidor web. Esta vez el Nombre_del_host ser\u00e1 servidor_web_vuestronombre . Si todo est\u00e1 configurado correctamente, al examinar las peticiones y respuestas, os aparecer\u00e1n las dos cabeceras que han incluido en la respuesta tanto el proxy inverso como el servidor web. . Es muy importante que para realizar estas comprobaciones teng\u00e1is marcado el checkbox Desactivar cach\u00e9 o en una ventana privada del navegador. Si no marc\u00e1is esto, la p\u00e1gina se guardar\u00e1 en la memoria cach\u00e9 del navegador y no estar\u00e9is recibiendo la respuesta del servidor sino de la cach\u00e9 del navegador, lo que puede dar lugar a resultados err\u00f3neos. Evaluaci\u00f3n Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta y completa del servidor web 3 puntos Configuraci\u00f3n correcta y completa del proxy inverso 4 puntos Comprobaci\u00f3n del correcto funcionamento, incluyendo las cabeceras 1 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Pr\u00e1ctica 2.3 \u2013 Proxy inverso con Nginx"},{"location":"P1.3/#practica-23-proxy-inverso-con-nginx","text":"","title":"Pr\u00e1ctica 2.3 \u2013 Proxy inverso con Nginx"},{"location":"P1.3/#requisitos-antes-de-comenzar-la-practica","text":"Atenci\u00f3n, importante antes de comenzar La pr\u00e1ctica 2.1 ha de estar funcionando correctamente No comenzar la pr\u00e1ctica antes de tener la 2.1 funcionando y comprobada","title":"Requisitos antes de comenzar  la pr\u00e1ctica"},{"location":"P1.3/#introduccion","text":"","title":"Introducci\u00f3n"},{"location":"P1.3/#que-es-un-servidor-proxy","text":"Un proxy de reenv\u00edo, a menudo llamado proxy, servidor proxy o proxy web, es un servidor que se encuentra frente a un grupo de m\u00e1quinas cliente. Cuando esas m\u00e1quinas realizan solicitudes a sitios y servicios en Internet, el servidor proxy intercepta esas solicitudes y luego se comunica con los servidores web en nombre de esos clientes, como un intermediario. Por ejemplo, tomemos como ejemplo 3 m\u00e1quinas involucradas en una comunicaci\u00f3n t\u00edpica de proxy de reenv\u00edo: A: Esta es la m\u00e1quina del hogar de un usuario. B: este es un servidor proxy de reenv\u00edo C: este es el servidor de origen de un sitio web (donde se almacenan los datos del sitio web) En una comunicaci\u00f3n est\u00e1ndar por Internet, la m\u00e1quina A se comunicar\u00eda directamente con la m\u00e1quina C, con el cliente enviando solicitudes al servidor de origen y el servidor de origen respondiendo al cliente. Cuando hay un proxy de reenv\u00edo, A enviar\u00e1 solicitudes a B, que luego reenviar\u00e1 la solicitud a C. C enviar\u00e1 una respuesta a B, que reenviar\u00e1 la respuesta a A. \u00bfPor qu\u00e9 agregar este intermediario adicional a nuestra actividad en Internet? Hay algunas razones por las que uno podr\u00eda querer usar un proxy de reenv\u00edo: Para evitar restricciones de navegaci\u00f3n estatales o institucionales : algunos gobiernos, escuelas y otras organizaciones usan firewalls para dar a sus usuarios acceso a una versi\u00f3n limitada de Internet. Se puede usar un proxy de reenv\u00edo para sortear estas restricciones, ya que permiten que el usuario se conecte al proxy en lugar de directamente a los sitios que est\u00e1 visitando. Para bloquear el acceso a cierto contenido : a la inversa, los proxies tambi\u00e9n se pueden configurar para bloquear el acceso de un grupo de usuarios a ciertos sitios. Por ejemplo, una red escolar puede estar configurada para conectarse a la web a trav\u00e9s de un proxy que habilita reglas de filtrado de contenido, neg\u00e1ndose a reenviar respuestas de Facebook y otros sitios de redes sociales. Para proteger su identidad en l\u00ednea : en algunos casos, los usuarios habituales de Internet simplemente desean un mayor anonimato en l\u00ednea, pero en otros casos, los usuarios de Internet viven en lugares donde el gobierno puede imponer graves consecuencias a los disidentes pol\u00edticos. Criticar al gobierno en un foro web o en las redes sociales puede dar lugar a multas o encarcelamiento para estos usuarios. Si uno de estos disidentes usa un proxy de reenv\u00edo para conectarse a un sitio web donde publica comentarios pol\u00edticamente sensibles, la direcci\u00f3n IP utilizada para publicar los comentarios ser\u00e1 m\u00e1s dif\u00edcil de rastrear hasta el disidente. Solo estar\u00e1 visible la direcci\u00f3n IP del servidor proxy.","title":"\u00bfQu\u00e9 es un servidor proxy?"},{"location":"P1.3/#en-que-se-diferencia-un-proxy-inverso","text":"Estar\u00edamos hablando del caso opuesto al anterior. Un proxy inverso es un servidor que se encuentra frente a uno o m\u00e1s servidores web, interceptando las solicitudes de los clientes. Esto es diferente de un proxy de reenv\u00edo, donde el proxy se encuentra frente a los clientes. Con un proxy inverso, cuando los clientes env\u00edan solicitudes al servidor de un sitio web, esas solicitudes son interceptadas en la frontera de la red por el servidor proxy inverso. El servidor proxy inverso enviar\u00e1 solicitudes y recibir\u00e1 respuestas del servidor del sitio web. La diferencia entre un proxy directo y inverso es sutil pero importante. Una forma simplificada de resumir ser\u00eda decir que un proxy de reenv\u00edo se encuentra frente a un cliente y garantiza que ning\u00fan servidor de origen se comunique nunca directamente con ese cliente espec\u00edfico . Por otro lado, un proxy inverso se encuentra frente a un servidor de origen y garantiza que ning\u00fan cliente se comunique nunca directamente con ese servidor de origen . Una vez m\u00e1s, ilustremos nombrando las m\u00e1quinas involucradas: D: cualquier n\u00famero de ordenadores dom\u00e9sticos de los usuarios E: este es un servidor proxy inverso F: uno o m\u00e1s servidores de origen Normalmente, todas las solicitudes de D ir\u00edan directamente a F, y F enviar\u00eda respuestas directamente a D. Con un proxy inverso, todas las solicitudes de D ir\u00e1n directamente a E, y E enviar\u00e1 sus solicitudes ay recibir\u00e1 respuestas de F. E luego transmita las respuestas apropiadas a D. A continuaci\u00f3n se describen algunos de los beneficios de un proxy inverso: Balanceo de carga : es posible que un sitio web popular que recibe millones de usuarios todos los d\u00edas no pueda manejar todo el tr\u00e1fico entrante del sitio con un solo servidor de origen. En cambio, el sitio se puede distribuir entre un grupo de servidores diferentes, todos manejando solicitudes para el mismo sitio. En este caso, un proxy inverso puede proporcionar una soluci\u00f3n de balanceo de carga que distribuir\u00e1 el tr\u00e1fico entrante de manera uniforme entre los diferentes servidores para evitar que un solo servidor se sobrecargue. En el caso de que un servidor falle por completo, otros servidores pueden intensificar para manejar el tr\u00e1fico. Protecci\u00f3n contra ataques : con un proxy inverso en su lugar, un sitio web o servicio nunca necesita revelar la direcci\u00f3n IP de su (s) servidor (es) de origen. Esto hace que sea mucho m\u00e1s dif\u00edcil para los atacantes aprovechar un ataque dirigido contra ellos, como un ataque DdoS. Almacenamiento en cach \u00e9: un proxy inverso tambi\u00e9n puede almacenar contenido en cach\u00e9 , lo que resulta en un rendimiento m\u00e1s r\u00e1pido. Por ejemplo, si un usuario en Par\u00eds visita un sitio web con proxy inverso con servidores web en Los \u00c1ngeles, el usuario podr\u00eda conectarse a un servidor proxy inverso local en Par\u00eds, que luego tendr\u00e1 que comunicarse con un servidor de origen en Los \u00c1ngeles. El servidor proxy luego puede almacenar en cach\u00e9 (o guardar temporalmente) los datos de respuesta. Los usuarios parisinos posteriores que naveguen por el sitio obtendr\u00e1n la versi\u00f3n en cach\u00e9 local del servidor proxy inverso parisino, lo que dar\u00e1 como resultado un rendimiento mucho m\u00e1s r\u00e1pido. Cifrado SSL - Cifrado y descifrado SSL (o TLS comunicaciones) para cada cliente pueden ser computacionalmente caro para un servidor de origen. Se puede configurar un proxy inverso para descifrar todas las solicitudes entrantes y cifrar todas las respuestas salientes, liberando valiosos recursos en el servidor de origen.","title":"\u00bfEn qu\u00e9 se diferencia un proxy inverso?"},{"location":"P1.3/#tarea","text":"","title":"Tarea"},{"location":"P1.3/#configuraciones","text":"","title":"Configuraciones"},{"location":"P1.3/#nginx-servidor-web","text":"Vamos a configurar dos Debian con sendos servidores Nginx. Ten\u00e9is la m\u00e1quina virtual inicial y deb\u00e9is clonarla para tener una segunda: Uno servir\u00e1 las p\u00e1ginas web que ya hemos configurado, as\u00ed pues utilizaremos el servidor que ya tenemos configurado de la Pr\u00e1ctica 2.1. El nuevo servidor clon Debian con Nginx configurado como proxy inverso Realizaremos las peticiones HTTP desde el navegador web de nuestra m\u00e1quina f\u00edsica/anfitri\u00f3n hacia el proxy clonado, que nos redirigir\u00e1 al servidor web original Cuidado Ojo al clonar las m\u00e1quinas virtuales porque hay que darle a crear una nueva MAC, de lo contrario no tendr\u00e9is IP en esa m\u00e1quina. El diagrama de red quedar\u00eda as\u00ed: Para que todo quede m\u00e1s diferenciado y os quede m\u00e1s claro que la petici\u00f3n est\u00e1 pasando por el proxy inverso y llega al servidor web destino, vamos a hacer que cada uno de los servidores escuche las peticiones en un puerto distinto. En primer lugar, deb\u00e9is cambiar el nombre que tuviera vuestra web por el de webserver , ello implica: Cambiar el nombre del archivo de configuraci\u00f3n de sitios disponibles par Nginx Cambiar el nombre del sitio web dentro de este archivo de configuraci\u00f3n donde haga falta No os olvid\u00e9is de eliminar el link simb\u00f3lico antiguo con el comando unlink nombre_del_link dentro de la carpeta sites-enabled y crear el nuevo para el nuevo nombre de archivo. En el archivo de configuraci\u00f3n del sitio web, en lugar de hacer que el servidor escuche en el puerto 80, cambiadlo al 8080. Reiniciar Nginx","title":"Nginx servidor web"},{"location":"P1.3/#nginx-proxy-inverso","text":"Ahora, cuando intentamos acceder a http://ejemplo-proxy (o el nombre que tuvier\u00e1is de vuestra web de las pr\u00e1cticas anteriores), en realidad estaremos accediendo al proxy, que nos redirigir\u00e1 a http://webserver:8080 , el servidor web que acabamos de configurar para que escuche con ese nombre en el puerto 8080. Para ello: Crear un archivo de configuraci\u00f3n en sites-available con el nombre ejemplo-proxy (o el que tuvier\u00e1is vosotros) Este archivo de configuraci\u00f3n ser\u00e1 m\u00e1s simple, tendr\u00e1 la siguiente forma server { listen __; server_name ____________; location / { proxy_pass http://_________:____; } } Donde, mirando el diagrama de red y teniendo en cuenta la configuraci\u00f3n hecha hasta ahora , deb\u00e9is completar: El puerto donde est\u00e1 escuchando el proxy inverso El nombre de vuestro dominio o sitio web original al que accedemos en el proxy La directiva proxy_pass indica a d\u00f3nde se van a redirigir las peticiones, esto es, al servidor web. Por tanto, deb\u00e9is poner la IP y n\u00famero de puerto adecuados de vuestro sitio web configurado en el apartado anterior. Crear el link simb\u00f3lico pertinente Esto es para simular la situaci\u00f3n en la que nosotros, como clientes, cuando accedamos a nuestro sitio web, no necesitemos saber c\u00f3mo est\u00e1 todo configurado, s\u00f3lo necesitamos saber el nombre de la web. \u00a1Atenci\u00f3n, muy importante! Deb\u00e9is modificar el archivo host que configurast\u00e9is en la pr\u00e1ctica 2.1. Si mir\u00e1is el diagrama de red, ahora el nombre de vuestro sitio web se corresponder\u00e1 con la IP de la nueva m\u00e1quina clon que hace de proxy . Ser\u00e1 \u00e9sta la encargada de redirigirnos autom\u00e1ticamente al verdadero sitio web.","title":"Nginx proxy inverso"},{"location":"P1.3/#comprobaciones","text":"Si acced\u00e9is a vuestro sitio web, deb\u00e9is poder seguir accediendo sin problemas. Comprobad en los access.log de los dos servidores que llega la petici\u00f3n Comprobad adem\u00e1s la petici\u00f3n y respuesta con las herramientas de desarrollador de Firefox en Xubuntu. Pulsando F12 en el navegador os aparecer\u00e1n estas herramientas En la primera petici\u00f3n (marcada en rojo), utilizando el apartado \u201cRed\u201d (tambi\u00e9n marcado en rojo) y tambi\u00e9n en rojo est\u00e1 se\u00f1alado d\u00f3nde se puede ver la respuesta de la petici\u00f3n GET HTTP (200 OK). Tambi\u00e9n vemos las cabeceras que se incluyen en la petici\u00f3n (m\u00e9todo GET) y en la respuesta a esta petici\u00f3n.","title":"Comprobaciones"},{"location":"P1.3/#anadiendo-cabeceras","text":"Adem\u00e1s de haber mirado los logs, vamos a demostrar a\u00fan de forma m\u00e1s clara que la petici\u00f3n est\u00e1 pasando por el proxy inverso y que est\u00e1 llegando al servidor web y que vuelve por el mismo camino. Si record\u00e1is de teor\u00eda, el servidor web es capaz de a\u00f1adir cabeceras en las respuestas a las peticiones. As\u00ed pues, vamos a configurar tanto el proxy inverso como el servidor web para que a\u00f1adan cada uno la cabecera \u201cHost\u201d que tambi\u00e9n vimos en teor\u00eda. Para a\u00f1adir cabeceras, en el archivo de configuraci\u00f3n del sitio web debemos a\u00f1adir dentro del bloque location / { \u2026 } debemos a\u00f1adir la directiva: add_header Host nombre_del_host; A\u00f1adiremos primero esta cabecera \u00fanicamente en el archivo de configuraci\u00f3n del sitio web del proxy inverso. El Nombre_del_host ser\u00e1 Proxy_inverso_vuestronombre. Reiniciamos Nginx Comprobamos que podemos acceder al sitio web sin problemas Con las herramientas de desarrollador comprobamos que la petici\u00f3n ha pasado por el proxy inverso que ha a\u00f1adido la cabecera en la respuesta: Hacemos lo propio con el servidor web. Esta vez el Nombre_del_host ser\u00e1 servidor_web_vuestronombre . Si todo est\u00e1 configurado correctamente, al examinar las peticiones y respuestas, os aparecer\u00e1n las dos cabeceras que han incluido en la respuesta tanto el proxy inverso como el servidor web. . Es muy importante que para realizar estas comprobaciones teng\u00e1is marcado el checkbox Desactivar cach\u00e9 o en una ventana privada del navegador. Si no marc\u00e1is esto, la p\u00e1gina se guardar\u00e1 en la memoria cach\u00e9 del navegador y no estar\u00e9is recibiendo la respuesta del servidor sino de la cach\u00e9 del navegador, lo que puede dar lugar a resultados err\u00f3neos.","title":"A\u00f1adiendo cabeceras"},{"location":"P1.3/#evaluacion","text":"Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta y completa del servidor web 3 puntos Configuraci\u00f3n correcta y completa del proxy inverso 4 puntos Comprobaci\u00f3n del correcto funcionamento, incluyendo las cabeceras 1 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Evaluaci\u00f3n"},{"location":"P1.4/","text":"Pr\u00e1ctica 2.4 \u2013 Balanceo de carga con proxy inverso en Nginx Requisitos aantes de comenzar la pr\u00e1ctica Atenci\u00f3n, muy importante antes de empezar La pr\u00e1ctica 2.3 debe estar funcionando correctamente No empezar la pr\u00e1ctica antes de tener la 2.3 funcionando y comprobada Introducci\u00f3n Los servidores proxy inversos y los balanceadores de carga son componentes de una arquitectura inform\u00e1tica cliente-servidor. Ambos act\u00faan como intermediarios en la comunicaci\u00f3n entre los clientes y los servidores, realizando funciones que mejoran la eficiencia. Las definiciones b\u00e1sicas son simples: Un proxy inverso acepta una solicitud de un cliente, la reenv\u00eda a un servidor que puede cumplirla y devuelve la respuesta del servidor al cliente. Un balanceador de carga distribuye las solicitudes entrantes del cliente entre un grupo de servidores, en cada caso devolviendo la respuesta del servidor seleccionado al cliente apropiado. Suenan bastante similares, \u00bfverdad? Ambos tipos de aplicaciones se ubican entre clientes y servidores, aceptando solicitudes del primero y entregando respuestas del segundo. No es de extra\u00f1ar que haya confusi\u00f3n sobre qu\u00e9 es un proxy inverso y un balanceador de carga. Para ayudar a diferenciarlos, exploremos cu\u00e1ndo y por qu\u00e9 normalmente se implementan en un sitio web. . Proxy inverso Ya conocemos este concepto de la pr\u00e1ctica anterior. Mientras que implementar un balanceador de carga solo tiene sentido cuando se tienen varios servidores, a menudo tiene sentido implementar un proxy inverso incluso con un solo servidor web o servidor de aplicaciones. Se puede pensar en el proxy inverso como la \"cara p\u00fablica\" de un sitio web. Su direcci\u00f3n es la que se anuncia para el sitio web y se encuentra en la frontera de la red del sitio para aceptar solicitudes de navegadores web y aplicaciones m\u00f3viles para el contenido alojado en el sitio web. Balanceadores de carga Los balanceadores de carga se implementan con mayor frecuencia cuando un sitio necesita varios servidores porque el volumen de solicitudes es demasiado para que un solo servidor lo maneje de manera eficiente. La implementaci\u00f3n de varios servidores tambi\u00e9n elimina un solo punto de fallo, lo que hace que el sitio web sea m\u00e1s confiable. Por lo general, todos los servidores alojan el mismo contenido, y el trabajo del balanceador de carga es distribuir la carga de trabajo de manera que se haga el mejor uso de la capacidad de cada servidor, evite la sobrecarga en cualquiera de ellos y d\u00e9 como resultado la respuesta m\u00e1s r\u00e1pida posible al cliente. . Un balanceador de carga tambi\u00e9n puede mejorar la experiencia del usuario al reducir la cantidad de respuestas de error que ve el cliente. Lo hace detectando cu\u00e1ndo los servidores caen y desviando las solicitudes de ellos a los otros servidores del grupo . En la implementaci\u00f3n m\u00e1s simple, el balanceador de carga detecta el estado del servidor al interceptar las respuestas de error a las solicitudes regulares. En esta pr\u00e1ctica tendremos el escenario donde Nginx har\u00e1 tanto de proxy inverso como de balanceador de carga al mismo tiempo. Info En esta pr\u00e1ctica tendremos un escenario donde Nginx har\u00e1 tanto de proxy inverso como de balanceador de carga al mismo tiempo Tarea Vamos a configurar dos servidores web Nginx con dos m\u00e1quinas Debian, adem\u00e1s de reutilizar el proxy inverso Nginx configurado en la pr\u00e1ctica anterior. Partiremos por tanto de la configuraci\u00f3n de la pr\u00e1ctica anterior, a\u00f1adiendo lo necesario: Cada servidor web presentar\u00e1 un sitio web espec\u00edfico para esta pr\u00e1ctica El webserver2 debe tener la IP asignada de forma fija mediante la configuraci\u00f3n DHCP. El proxy inverso que ya ten\u00edamos configurado, habr\u00e1 ahora que configurarlo para que realice el balanceo de carga que deseamos Realizaremos las peticiones HTTP desde el navegador web de nuestra m\u00e1quina anfitriona. El diagrama de red quedar\u00eda as\u00ed: Haremos las peticiones web desde el navegador al proxy inverso, que las repartir\u00e1 entre los dos servidores web que tenemos. Accederemos a http://balanceo y debemos observar que las peticiones, efectivamente, se van repartiendo entre el servidor 1 y el 2. Configuraciones Atenci\u00f3n Ya no vamos a utilizar los sitios web que hemos configurado en las pr\u00e1cticas anteriores. Por ello, para evitarnos una serie de problemas que pueden surgir, vamos a desactivarlos. Dentro de la carpeta /etc/nginx/sites-enabled debemos ejecutar unlink nombre_archivo para cada uno de los archivos de los sitios web que tenemos. Si no hac\u00e9is esto obtendr\u00e9is errores en todas las pr\u00e1cticas que quedan de este tema. Nginx Servidor Web 1 El primer servidor web ser\u00e1 el servidor principal que hemos venido utilizando hasta ahora durante el curso, el original, donde tenemos instalado ya el servicio Web. Debemos configurar este servidor web para que sirva el siguiente index.html que deb\u00e9is crear dentro de la carpeta /var/www/webserver1/html : El nombre del sitio web que deb\u00e9is utilizar en los archivos correspondientes ( sites-available \u2026) que deb\u00e9is crear para Nginx es webserver1 , as\u00ed como en sus configuraciones. Fij\u00e1os en las configuraciones que hicisteis en pr\u00e1cticas anteriores a modo de referencia. El sitio web debe escuchar en el puerto 8080. Deb\u00e9is a\u00f1adir una cabecera que se llame Serv_Web1_vuestronombre . Nginx Servidor Web 2 Debe ser una m\u00e1quina Debian, clon del servidor web 1. En este servidor web debemos realizar una configuraci\u00f3n id\u00e9ntica al servidor web 1 pero cambiando webserver1 por webserver2 (tambi\u00e9n en el index.html), as\u00ed como el nombre de la cabecera a\u00f1adida, que ser\u00e1 Serv_Web2_vuestronombre Warning Es importante que no quede ninguna referencia a webserver1 por ning\u00fan archivo, de otra forma os dar\u00e1 resultados err\u00f3neos y os dificultar\u00e1 mucho encontrar el error. Nginx Proxy Inverso Ya disponemos de los dos servidores web entre los que se van a repartir las peticiones que realice el cliente desde el navegador. Vamos, por tanto, a configurar el proxy inverso para que realice este reparto de peticiones: En sites-available deb\u00e9is crear el archivo de configuraci\u00f3n con el nombre balanceo Este archivo tendr\u00e1 el siguiente formato: upstream backend_hosts { random ; server ________:____; server ________:____; } server { listen 80 ; server_name ________; location / { proxy_pass http://backend_hosts; } } Donde: El bloque upstream \u2192 son los servidores entre los que se va a repartir la carga, que son los dos que hemos configurado anteriormente. Si mir\u00e1is el diagrama y ten\u00e9is en cuenta la configuraci\u00f3n que hab\u00e9is hecho hasta ahora, aqu\u00ed deber\u00e9is colocar la IP de cada servidor, as\u00ed como el puerto donde est\u00e1 escuchando las peticiones web. A este grupo de servidores le ponemos un nombre, que es backend_hosts Aclaraci\u00f3n En un sitio web, el backend se encarga de todos los procesos necesarios para que la web funcione de forma correcta. Estos procesos o funciones no son visibles pero tienen mucha importancia en el buen funcionamiento de un sitio web. El par\u00e1metro random lo que hace es repartir las peticiones HTTP que llegan al proxy inverso de forma completamente aleatoria entre el grupo de servidores que se haya definido en el bloque upstream (en nuestro caso s\u00f3lo hay dos). Pondremos random porque es lo m\u00e1s f\u00e1cil para comprobar que todo funciona bien en la pr\u00e1ctica, pero hay diferentes formas de repartir la carga (las peticiones HTTP). Comprobaciones Si acced\u00e9is a vuestro sitio web, deb\u00e9is poder seguir accediendo sin problemas. Comprobad d\u00e1ndole repetidamente a F5, que acced\u00e9is cada vez a uno de los servidores. Se os mostrar\u00e1 el contenido del index.html del servidor correspondiente cada vez. Para una doble comprobaci\u00f3n, utilizando las herramientas de desarrollador, mostrad que la web que se os muestra coincide con la cabecera que ha a\u00f1adido el servidor web en la respuesta HTTP. Recordatorio Recordad que es muy importante que para realizar estas comprobaciones teng\u00e1is marcado el checkbox Desactivar cach\u00e9 . Si no marc\u00e1is esto, la p\u00e1gina se guardar\u00e1 en la memoria cach\u00e9 del navegador y no estar\u00e9is recibiendo la respuesta del servidor sino de la cach\u00e9 del navegador, lo que puede dar lugar a resultados err\u00f3neos. Otra opci\u00f3n, si esto no funcionara, es hacer las pruebas con una nueva ventana privada del navegador. Comprobaci\u00f3n del balanceo de carga cuando cae un servidor Nuestro balanceador de carga est\u00e1 constantemente monitorizando \u201cla salud\u201d de los servidores web. De esta forma, si uno deja de funcionar por cualquier raz\u00f3n, siempre enviar\u00e1 las solicitudes a los que queden \u201cvivos\u201d. Vamos a comprobarlo: Para el servicio Nginx en el servidor web 1 y comprueba, de la misma forma que en el apartado anterior, que todas las solicitudes se env\u00edan ahora al servidor web 2 Tras iniciar de nuevo Nginx en el servidor web 1, repite el proceso con el servidor web 2. Cuestiones finales Cuesti\u00f3n 1 Busca informaci\u00f3n de qu\u00e9 otros m\u00e9todos de balanceo se pueden aplicar con Nginx y describe al menos 3 de ellos. Cuesti\u00f3n 2 Si quiero a\u00f1adir 2 servidores web m\u00e1s al balanceo de carga, describe detalladamente qu\u00e9 configuraci\u00f3n habr\u00eda que a\u00f1adir y d\u00f3nde. Cuesti\u00f3n 3 Describe todos los pasos que deber\u00edamos seguir y configurar para realizar el balanceo de carga con una de las webs de pr\u00e1cticas anteriores. Indicad la configuraci\u00f3n de todas las m\u00e1quinas (webservers, proxy...) y de sus servicios Evaluaci\u00f3n Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta y completa del servidor web 1 2 puntos Configuraci\u00f3n correcta y completa del servidor web 2 2 puntos Configuraci\u00f3n correcta y completa del proxy inverso 3 puntos Cuestiones finales 1 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Pr\u00e1ctica 2.4 \u2013 Balanceo de carga con proxy inverso en Nginx"},{"location":"P1.4/#practica-24-balanceo-de-carga-con-proxy-inverso-en-nginx","text":"","title":"Pr\u00e1ctica 2.4 \u2013 Balanceo de carga con proxy inverso en Nginx"},{"location":"P1.4/#requisitos-aantes-de-comenzar-la-practica","text":"Atenci\u00f3n, muy importante antes de empezar La pr\u00e1ctica 2.3 debe estar funcionando correctamente No empezar la pr\u00e1ctica antes de tener la 2.3 funcionando y comprobada","title":"Requisitos aantes de comenzar la pr\u00e1ctica"},{"location":"P1.4/#introduccion","text":"Los servidores proxy inversos y los balanceadores de carga son componentes de una arquitectura inform\u00e1tica cliente-servidor. Ambos act\u00faan como intermediarios en la comunicaci\u00f3n entre los clientes y los servidores, realizando funciones que mejoran la eficiencia. Las definiciones b\u00e1sicas son simples: Un proxy inverso acepta una solicitud de un cliente, la reenv\u00eda a un servidor que puede cumplirla y devuelve la respuesta del servidor al cliente. Un balanceador de carga distribuye las solicitudes entrantes del cliente entre un grupo de servidores, en cada caso devolviendo la respuesta del servidor seleccionado al cliente apropiado. Suenan bastante similares, \u00bfverdad? Ambos tipos de aplicaciones se ubican entre clientes y servidores, aceptando solicitudes del primero y entregando respuestas del segundo. No es de extra\u00f1ar que haya confusi\u00f3n sobre qu\u00e9 es un proxy inverso y un balanceador de carga. Para ayudar a diferenciarlos, exploremos cu\u00e1ndo y por qu\u00e9 normalmente se implementan en un sitio web. .","title":"Introducci\u00f3n"},{"location":"P1.4/#proxy-inverso","text":"Ya conocemos este concepto de la pr\u00e1ctica anterior. Mientras que implementar un balanceador de carga solo tiene sentido cuando se tienen varios servidores, a menudo tiene sentido implementar un proxy inverso incluso con un solo servidor web o servidor de aplicaciones. Se puede pensar en el proxy inverso como la \"cara p\u00fablica\" de un sitio web. Su direcci\u00f3n es la que se anuncia para el sitio web y se encuentra en la frontera de la red del sitio para aceptar solicitudes de navegadores web y aplicaciones m\u00f3viles para el contenido alojado en el sitio web.","title":"Proxy inverso"},{"location":"P1.4/#balanceadores-de-carga","text":"Los balanceadores de carga se implementan con mayor frecuencia cuando un sitio necesita varios servidores porque el volumen de solicitudes es demasiado para que un solo servidor lo maneje de manera eficiente. La implementaci\u00f3n de varios servidores tambi\u00e9n elimina un solo punto de fallo, lo que hace que el sitio web sea m\u00e1s confiable. Por lo general, todos los servidores alojan el mismo contenido, y el trabajo del balanceador de carga es distribuir la carga de trabajo de manera que se haga el mejor uso de la capacidad de cada servidor, evite la sobrecarga en cualquiera de ellos y d\u00e9 como resultado la respuesta m\u00e1s r\u00e1pida posible al cliente. . Un balanceador de carga tambi\u00e9n puede mejorar la experiencia del usuario al reducir la cantidad de respuestas de error que ve el cliente. Lo hace detectando cu\u00e1ndo los servidores caen y desviando las solicitudes de ellos a los otros servidores del grupo . En la implementaci\u00f3n m\u00e1s simple, el balanceador de carga detecta el estado del servidor al interceptar las respuestas de error a las solicitudes regulares. En esta pr\u00e1ctica tendremos el escenario donde Nginx har\u00e1 tanto de proxy inverso como de balanceador de carga al mismo tiempo. Info En esta pr\u00e1ctica tendremos un escenario donde Nginx har\u00e1 tanto de proxy inverso como de balanceador de carga al mismo tiempo","title":"Balanceadores de carga"},{"location":"P1.4/#tarea","text":"Vamos a configurar dos servidores web Nginx con dos m\u00e1quinas Debian, adem\u00e1s de reutilizar el proxy inverso Nginx configurado en la pr\u00e1ctica anterior. Partiremos por tanto de la configuraci\u00f3n de la pr\u00e1ctica anterior, a\u00f1adiendo lo necesario: Cada servidor web presentar\u00e1 un sitio web espec\u00edfico para esta pr\u00e1ctica El webserver2 debe tener la IP asignada de forma fija mediante la configuraci\u00f3n DHCP. El proxy inverso que ya ten\u00edamos configurado, habr\u00e1 ahora que configurarlo para que realice el balanceo de carga que deseamos Realizaremos las peticiones HTTP desde el navegador web de nuestra m\u00e1quina anfitriona. El diagrama de red quedar\u00eda as\u00ed: Haremos las peticiones web desde el navegador al proxy inverso, que las repartir\u00e1 entre los dos servidores web que tenemos. Accederemos a http://balanceo y debemos observar que las peticiones, efectivamente, se van repartiendo entre el servidor 1 y el 2.","title":"Tarea"},{"location":"P1.4/#configuraciones","text":"Atenci\u00f3n Ya no vamos a utilizar los sitios web que hemos configurado en las pr\u00e1cticas anteriores. Por ello, para evitarnos una serie de problemas que pueden surgir, vamos a desactivarlos. Dentro de la carpeta /etc/nginx/sites-enabled debemos ejecutar unlink nombre_archivo para cada uno de los archivos de los sitios web que tenemos. Si no hac\u00e9is esto obtendr\u00e9is errores en todas las pr\u00e1cticas que quedan de este tema.","title":"Configuraciones"},{"location":"P1.4/#nginx-servidor-web-1","text":"El primer servidor web ser\u00e1 el servidor principal que hemos venido utilizando hasta ahora durante el curso, el original, donde tenemos instalado ya el servicio Web. Debemos configurar este servidor web para que sirva el siguiente index.html que deb\u00e9is crear dentro de la carpeta /var/www/webserver1/html : El nombre del sitio web que deb\u00e9is utilizar en los archivos correspondientes ( sites-available \u2026) que deb\u00e9is crear para Nginx es webserver1 , as\u00ed como en sus configuraciones. Fij\u00e1os en las configuraciones que hicisteis en pr\u00e1cticas anteriores a modo de referencia. El sitio web debe escuchar en el puerto 8080. Deb\u00e9is a\u00f1adir una cabecera que se llame Serv_Web1_vuestronombre .","title":"Nginx Servidor Web 1"},{"location":"P1.4/#nginx-servidor-web-2","text":"Debe ser una m\u00e1quina Debian, clon del servidor web 1. En este servidor web debemos realizar una configuraci\u00f3n id\u00e9ntica al servidor web 1 pero cambiando webserver1 por webserver2 (tambi\u00e9n en el index.html), as\u00ed como el nombre de la cabecera a\u00f1adida, que ser\u00e1 Serv_Web2_vuestronombre Warning Es importante que no quede ninguna referencia a webserver1 por ning\u00fan archivo, de otra forma os dar\u00e1 resultados err\u00f3neos y os dificultar\u00e1 mucho encontrar el error.","title":"Nginx Servidor Web 2"},{"location":"P1.4/#nginx-proxy-inverso","text":"Ya disponemos de los dos servidores web entre los que se van a repartir las peticiones que realice el cliente desde el navegador. Vamos, por tanto, a configurar el proxy inverso para que realice este reparto de peticiones: En sites-available deb\u00e9is crear el archivo de configuraci\u00f3n con el nombre balanceo Este archivo tendr\u00e1 el siguiente formato: upstream backend_hosts { random ; server ________:____; server ________:____; } server { listen 80 ; server_name ________; location / { proxy_pass http://backend_hosts; } } Donde: El bloque upstream \u2192 son los servidores entre los que se va a repartir la carga, que son los dos que hemos configurado anteriormente. Si mir\u00e1is el diagrama y ten\u00e9is en cuenta la configuraci\u00f3n que hab\u00e9is hecho hasta ahora, aqu\u00ed deber\u00e9is colocar la IP de cada servidor, as\u00ed como el puerto donde est\u00e1 escuchando las peticiones web. A este grupo de servidores le ponemos un nombre, que es backend_hosts Aclaraci\u00f3n En un sitio web, el backend se encarga de todos los procesos necesarios para que la web funcione de forma correcta. Estos procesos o funciones no son visibles pero tienen mucha importancia en el buen funcionamiento de un sitio web. El par\u00e1metro random lo que hace es repartir las peticiones HTTP que llegan al proxy inverso de forma completamente aleatoria entre el grupo de servidores que se haya definido en el bloque upstream (en nuestro caso s\u00f3lo hay dos). Pondremos random porque es lo m\u00e1s f\u00e1cil para comprobar que todo funciona bien en la pr\u00e1ctica, pero hay diferentes formas de repartir la carga (las peticiones HTTP).","title":"Nginx Proxy Inverso"},{"location":"P1.4/#comprobaciones","text":"Si acced\u00e9is a vuestro sitio web, deb\u00e9is poder seguir accediendo sin problemas. Comprobad d\u00e1ndole repetidamente a F5, que acced\u00e9is cada vez a uno de los servidores. Se os mostrar\u00e1 el contenido del index.html del servidor correspondiente cada vez. Para una doble comprobaci\u00f3n, utilizando las herramientas de desarrollador, mostrad que la web que se os muestra coincide con la cabecera que ha a\u00f1adido el servidor web en la respuesta HTTP. Recordatorio Recordad que es muy importante que para realizar estas comprobaciones teng\u00e1is marcado el checkbox Desactivar cach\u00e9 . Si no marc\u00e1is esto, la p\u00e1gina se guardar\u00e1 en la memoria cach\u00e9 del navegador y no estar\u00e9is recibiendo la respuesta del servidor sino de la cach\u00e9 del navegador, lo que puede dar lugar a resultados err\u00f3neos. Otra opci\u00f3n, si esto no funcionara, es hacer las pruebas con una nueva ventana privada del navegador.","title":"Comprobaciones"},{"location":"P1.4/#comprobacion-del-balanceo-de-carga-cuando-cae-un-servidor","text":"Nuestro balanceador de carga est\u00e1 constantemente monitorizando \u201cla salud\u201d de los servidores web. De esta forma, si uno deja de funcionar por cualquier raz\u00f3n, siempre enviar\u00e1 las solicitudes a los que queden \u201cvivos\u201d. Vamos a comprobarlo: Para el servicio Nginx en el servidor web 1 y comprueba, de la misma forma que en el apartado anterior, que todas las solicitudes se env\u00edan ahora al servidor web 2 Tras iniciar de nuevo Nginx en el servidor web 1, repite el proceso con el servidor web 2.","title":"Comprobaci\u00f3n del balanceo de carga cuando cae un servidor"},{"location":"P1.4/#cuestiones-finales","text":"Cuesti\u00f3n 1 Busca informaci\u00f3n de qu\u00e9 otros m\u00e9todos de balanceo se pueden aplicar con Nginx y describe al menos 3 de ellos. Cuesti\u00f3n 2 Si quiero a\u00f1adir 2 servidores web m\u00e1s al balanceo de carga, describe detalladamente qu\u00e9 configuraci\u00f3n habr\u00eda que a\u00f1adir y d\u00f3nde. Cuesti\u00f3n 3 Describe todos los pasos que deber\u00edamos seguir y configurar para realizar el balanceo de carga con una de las webs de pr\u00e1cticas anteriores. Indicad la configuraci\u00f3n de todas las m\u00e1quinas (webservers, proxy...) y de sus servicios","title":"Cuestiones finales"},{"location":"P1.4/#evaluacion","text":"Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta y completa del servidor web 1 2 puntos Configuraci\u00f3n correcta y completa del servidor web 2 2 puntos Configuraci\u00f3n correcta y completa del proxy inverso 3 puntos Cuestiones finales 1 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Evaluaci\u00f3n"},{"location":"P1.5/","text":"Practica 2.5 - Proxy inverso y balanceo de carga con SSL en NGINX Requisitos antes de comenzar la pr\u00e1ctica Atenci\u00f3n, muy importante antes de comenzar La pr\u00e1ctica 4.4 ha d'estar funcionant correctament No comenzar la pr\u00e1ctica antes de tener la 1.3 funcionant i comprovada Introducci\u00f3n A partir de las pr\u00e1cticas anteriores hemos llegado a un escenario donde un proxy inverso act\u00faa de intermediario entre dos servidores web Nginx, balanceando la carga entre ellos. Ya dijimos que una importante funci\u00f3n que pod\u00eda tener un proxy inverso era realizar el cifrado y descifrado de SSL para utilizar HTTPS en los servidores web. De esta forma se aliviaba la carga de trabajo de los servidores web, ya que es una tarea que consume recursos. En definitiva, tendr\u00edamos un esquema como este: Podr\u00eda llegarse a pensar que en t\u00e9rminos de seguridad no es adecuado que el tr\u00e1fico de red entre el balanceador de carga y los servidores web vaya sin cifrar (HTTP). Sin embargo, pensando en un caso real, la red privada y el proxy inverso/balanceador de carga, adem\u00e1s de estar en la misma red privada, suelen estar administrados por las mismas personas de la misma empresa, por lo que no supone un peligro real que ese tr\u00e1fico vaya sin cifrar. Podr\u00eda cifrarse si fuera necesario, pero entonces pierde sentido que el proxy inverso se encargue del cifrado SSL para HTTPS, ya que har\u00edamos el mismo trabajo dos veces. As\u00ed las cosas, nos quedaremos con el esquema de la imagen de m\u00e1s arriba para la pr\u00e1ctica. Certificados HTTPS se basa en el uso de certificados digitales. Grosso modo, cuando entramos en una web v\u00eda HTTPS, \u00e9sta nos presenta un certificado digital para asegurar que es qui\u00e9n dice ser. \u00bfC\u00f3mo sabemos que ese certificado es v\u00e1lido? Debemos consultar a la Autoridad de Certificaci\u00f3n (CA) que emiti\u00f3 ese certificado si es v\u00e1lido. Las CA son entidades que emiten certificados y su funcionamiento se basa en la confianza. Confiamos en que los certificados emitidos y firmados por esas entidades son reales y funcionales. Los navegadores web tienen precargadas las Autoridades de Certificaci\u00f3n en las que conf\u00edan por defecto a la hora de navegar por webs HTTPS: Si accedemos a una web cuyo certificado no haya sido emitido y firmado por una de estas entidades, nos saltar\u00e1 el famoso aviso: Ya que si el certificado no ha sido emitido y firmado por una CA de confianza, puede que se trate de una web maliciosa que nos suponga un riesgo de seguridad, como bien dice el aviso. Tarea Partimos de la configuraci\u00f3n exacta de la pr\u00e1ctica anterior, que recordemos era esta: Por lo que en esta pr\u00e1ctica simplemente debemos a\u00f1adir la configuraci\u00f3n SSL para el cifrado en el Proxy Inverso: Tal y como quedar\u00e1 la configuraci\u00f3n, desde el cliente a\u00fan podr\u00edamos acceder a los dos servidores web con HTTP (pod\u00e9is probarlo) pero es algo que solucionaremos en siguientes temas, configurando un firewall para que s\u00f3lo la IP del proxy inverso pueda acceder por HTTP a los servidores web y nadie m\u00e1s. Creaci\u00f3n de certificado autofirmado Nosotros no utilizaremos certificados de ninguna CA de confianza, b\u00e1sicamente porque: Nuestros servicios no est\u00e1n publicados en Internet Estos certificados son de pago As\u00ed pues, nosotros crearemos nuestros propios certificados y los firmaremos nosotros mismos como si fu\u00e9ramos una CA aut\u00e9ntica para poder simular este escenario. Warning Esto provocar\u00e1 que cuando accedamos por HTTPS a nuestro sitio web por primera vez, nos salt\u00e9 el aviso de seguridad que se comentaba en la introducci\u00f3n. En este caso no habr\u00e1 peligro puesto que estamos 100% seguros que ese certificado lo hemos emitido nosotros para esta pr\u00e1ctica, no hay dudas. Veamos pues el proceso para generar los certificados y las claves asociadas a ellos (privada/p\u00fablica). En primer lugar debemos crear el siguiente directorio: /etc/nginx/ssl Podemos crear el certificado y las claves de forma simult\u00e1nea con un \u00fanico comando, donde: openssl : esta es la herramienta por l\u00ednea de comandos b\u00e1sica para crear y administrar certificados, claves y otros archivos OpenSSL. req : este subcomando se utiliza para generar una solicitud de certificados y tambi\u00e9n solicitudes de firma de certificados (CSR). -x509 : Esto modifica a\u00fan m\u00e1s el subcomando anterior al decirle a la herramienta que queremos crear un certificado autofirmado en lugar de generar una solicitud de firma de certificado, como suceder\u00eda normalmente. -nodes : Esto le dice a OpenSSL que omita la opci\u00f3n de asegurar nuestro certificado con contrase\u00f1a. Necesitamos que Nginx pueda leer el archivo sin la intervenci\u00f3n del usuario cuando se inicia el servidor. Una contrase\u00f1a evitar\u00eda que esto sucediera ya que tendr\u00edamos que introducirla a mano despu\u00e9s de cada reinicio. -days 365 : esta opci\u00f3n establece el tiempo durante el cual el certificado se considerar\u00e1 v\u00e1lido. Lo configuramos para un a\u00f1o. -newkey rsa: 2048 : Esto especifica que queremos generar un nuevo certificado y una nueva clave al mismo tiempo. No creamos la clave necesaria para firmar el certificado en un paso anterior, por lo que debemos crearla junto con el certificado. La rsa:2048parte le dice que cree una clave RSA de 2048 bits de longitud. -keyout : este par\u00e1metro le dice a OpenSSL d\u00f3nde colocar el archivo de clave privada generado que estamos creando. -out : Esto le dice a OpenSSL d\u00f3nde colocar el certificado que estamos creando. El comando completo ser\u00eda as\u00ed: Os solicitar\u00e1 que introduzc\u00e1is una serie de par\u00e1metros, como v\u00e9is en el recuadro rojo de abajo de la imagen. Deb\u00e9is introducir los mismos par\u00e1metros que en la imagen excepto en el \u201cOrganizational Unit Name\u201d que v\u00e9is recuadrado en amarillo. Ah\u00ed deber\u00e9is poner 2DAW \u2013 DEAW - Vuestronombre Configuraci\u00f3n SSL en el proxy inverso De la pr\u00e1ctica anterior, dentro del directorio /etc/nginx/sites-available ya deb\u00e9is tener el archivo de configuraci\u00f3n llamado \u201cbalanceo\u201d. Es precisamente aqu\u00ed donde realizaremos la configuraci\u00f3n para que el acceso al sitio web se realice mediante SSL (HTTPS). Dentro del bloque server {\u2026} deb\u00e9is cambiar el puerto de escucha ( listen 80 ) por lo que v\u00e9is en la imagen de abajo, a\u00f1adiendo las siguientes l\u00edneas de configuraci\u00f3n tambi\u00e9n, de tal forma que quede: Donde le est\u00e1is diciendo que: Escuche en el puerto 443 \u2192 Puerto por defecto de HTTPS El directorio donde est\u00e1 el certificado que hab\u00e9is generado anteriormente El directorio donde est\u00e1 la clave que hab\u00e9is generado anteriormente Los protocolos y tipos de cifrados que se pueden utilizar \u2192 Estas son las versiones de protocolos y los tipos de cifrados considerados seguros a d\u00eda de hoy (hay muchos m\u00e1s pero no se consideran seguros actualmente) server_name ya lo ten\u00edais de la pr\u00e1ctica anterior, no hace falta tocarlo El archivo donde se guardan los logs cambia de nombre, ahora ser\u00e1 https_access.log Recordad que tras modificar cualquier configuraci\u00f3n de un servicio, hay que reiniciar el servicio, en este caso Nginx. Comprobaciones Si acced\u00e9is ahora a https://balanceo os deber\u00eda saltar un aviso de seguridad debido a que nuestro certificado es autofirmado, como coment\u00e1bamos anteriormente. Si a\u00f1ad\u00eds una una excepci\u00f3n podr\u00e9is acceder al sitio web y recargando repetidamente la p\u00e1gina con F5, ver\u00e9is que el balanceo de carga se hace correctamente accediendo mediante HTTPS. Para comprobar que los datos del certificado son, efectivamente, los vuestros pod\u00e9is comprobarlo as\u00ed. Pulsando en el candado de la barra de b\u00fasqueda: Con m\u00e1s informaci\u00f3n: Info Aqu\u00ed tambi\u00e9n podr\u00e9is eliminar la excepci\u00f3n que hab\u00e9is a\u00f1adido en la p\u00e1gina de la advertencia de seguridad, por si necesit\u00e1is reiniciar las pruebas. Y por \u00faltimo, ver certificado: Y podremos ver los detalles: Si ahora intent\u00e1is acceder a http://balanceo , \u00bfdeber\u00edais poder acceder? Comprobadlo y describid qu\u00e9 pasa y por qu\u00e9. Redirecci\u00f3n forzosa a HTTPS Para que, indistintamente de la forma por la que accedamos al sitio web balanceo, siempre se fuerce a utilizar HTTPS, necesitaremos una configuraci\u00f3n adicional. Necesitamos a\u00f1adir un bloque \u201cserver\u201d adicional y separado del otro, al archivo de configuraci\u00f3n de \u201cbalanceo\u201d. Algo as\u00ed: Con esta configuraci\u00f3n le estamos diciendo que: Escuche en el puerto 80 (HTTP) Que el nombre al que responder\u00e1 el servidor/sitio web es balanceo Que guarde los logs de este bloque en ese directorio y con ese nombre Cuando se recibe una petici\u00f3n con las dos condiciones anteriores, se devuelve un c\u00f3digo HTTP 301: HTTP 301 Moved Permanently (Movido permanentemente en espa\u00f1ol) es un c\u00f3digo de estado de HTTP que indica que el host ha sido capaz de comunicarse con el servidor pero que el recurso solicitado ha sido movido a otra direcci\u00f3n permanentementeEs muy importante configurar las redirecciones 301 en los sitios web y para ello hay diferentes m\u00e9todos y sintaxis para realizar la redirecci\u00f3n 301. La redirecci\u00f3n 301 es un c\u00f3digo o comando insertado por un Webmaster que permite redirigir a los usuarios y buscadores de un sitio web de un sitio a otro. Aclaraci\u00f3n Es decir, lo que estamos haciendo es que cuando se reciba una petici\u00f3n HTTP (puerto 80) en http://balanceo , se redirija a https://balanceo (HTTPS) Tarea Eliminad del otro bloque server{\u2026} la l\u00edneas que hagan referencia a escuchar en el puerto 80 (listen 80\u2026). Reiniciad el servicio Comprobad ahora que cuando entr\u00e1is en http://balanceo , autom\u00e1ticamente os redirige a la versi\u00f3n segura de la web. Comprobad que cuando realiz\u00e1is una petici\u00f3n en el archivo de log http_access.log aparece la redirecci\u00f3n 301 y que, de la misma manera, aparece una petici\u00f3n GET en https_access.log . Cuestiones finals Cuesti\u00f3n 1 Hemos configurado nuestro proxy inverso con todo lo que nos hace falta pero no nos funciona y da un error del tipo This site can't provide a secure connection, ERR_SSL_PROTOCOL_ERROR. Dentro de nuestro server block tenemos esto: server { listen 443 ; ssl_certificate /etc/nginx/ssl/enrico-berlinguer/server.crt ; ssl_certificate_key /etc/nginx/ssl/enrico-berlinguer/server.key ; ssl_protocols TLSv1.3; ssl_ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS; server_name enrico-berlinguer; access_log /var/log/nginx/https_access.log ; location / { proxy_pass http://red-party; } } Cuesti\u00f3n 2 Imaginad que intentamos acceder a nuestro sitio web HTTPS y nos encontramos con el siguiente error: Investigad qu\u00e9 est\u00e1 pasando y como se ha de solucionar. Evaluaci\u00f3n Criterio Puntuaci\u00f3n Creaci\u00f3n correcta del certificado 1 puntos Configuraci\u00f3n SSL correcta del proxy 3 puntos Comprobaciones 2 puntos Configuraci\u00f3n correcta de la redirecci\u00f3n forzosa a HTTPS y comprobaciones 1 puntos Cuestiones finales 2 puntos SSe ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 1 punto","title":"Practica 2.5 - Proxy inverso y balanceo de carga con SSL en NGINX"},{"location":"P1.5/#practica-25-proxy-inverso-y-balanceo-de-carga-con-ssl-en-nginx","text":"","title":"Practica 2.5 - Proxy inverso y balanceo de carga con SSL en NGINX"},{"location":"P1.5/#requisitos-antes-de-comenzar-la-practica","text":"Atenci\u00f3n, muy importante antes de comenzar La pr\u00e1ctica 4.4 ha d'estar funcionant correctament No comenzar la pr\u00e1ctica antes de tener la 1.3 funcionant i comprovada","title":"Requisitos antes de comenzar la pr\u00e1ctica"},{"location":"P1.5/#introduccion","text":"A partir de las pr\u00e1cticas anteriores hemos llegado a un escenario donde un proxy inverso act\u00faa de intermediario entre dos servidores web Nginx, balanceando la carga entre ellos. Ya dijimos que una importante funci\u00f3n que pod\u00eda tener un proxy inverso era realizar el cifrado y descifrado de SSL para utilizar HTTPS en los servidores web. De esta forma se aliviaba la carga de trabajo de los servidores web, ya que es una tarea que consume recursos. En definitiva, tendr\u00edamos un esquema como este: Podr\u00eda llegarse a pensar que en t\u00e9rminos de seguridad no es adecuado que el tr\u00e1fico de red entre el balanceador de carga y los servidores web vaya sin cifrar (HTTP). Sin embargo, pensando en un caso real, la red privada y el proxy inverso/balanceador de carga, adem\u00e1s de estar en la misma red privada, suelen estar administrados por las mismas personas de la misma empresa, por lo que no supone un peligro real que ese tr\u00e1fico vaya sin cifrar. Podr\u00eda cifrarse si fuera necesario, pero entonces pierde sentido que el proxy inverso se encargue del cifrado SSL para HTTPS, ya que har\u00edamos el mismo trabajo dos veces. As\u00ed las cosas, nos quedaremos con el esquema de la imagen de m\u00e1s arriba para la pr\u00e1ctica.","title":"Introducci\u00f3n"},{"location":"P1.5/#certificados","text":"HTTPS se basa en el uso de certificados digitales. Grosso modo, cuando entramos en una web v\u00eda HTTPS, \u00e9sta nos presenta un certificado digital para asegurar que es qui\u00e9n dice ser. \u00bfC\u00f3mo sabemos que ese certificado es v\u00e1lido? Debemos consultar a la Autoridad de Certificaci\u00f3n (CA) que emiti\u00f3 ese certificado si es v\u00e1lido. Las CA son entidades que emiten certificados y su funcionamiento se basa en la confianza. Confiamos en que los certificados emitidos y firmados por esas entidades son reales y funcionales. Los navegadores web tienen precargadas las Autoridades de Certificaci\u00f3n en las que conf\u00edan por defecto a la hora de navegar por webs HTTPS: Si accedemos a una web cuyo certificado no haya sido emitido y firmado por una de estas entidades, nos saltar\u00e1 el famoso aviso: Ya que si el certificado no ha sido emitido y firmado por una CA de confianza, puede que se trate de una web maliciosa que nos suponga un riesgo de seguridad, como bien dice el aviso.","title":"Certificados"},{"location":"P1.5/#tarea","text":"Partimos de la configuraci\u00f3n exacta de la pr\u00e1ctica anterior, que recordemos era esta: Por lo que en esta pr\u00e1ctica simplemente debemos a\u00f1adir la configuraci\u00f3n SSL para el cifrado en el Proxy Inverso: Tal y como quedar\u00e1 la configuraci\u00f3n, desde el cliente a\u00fan podr\u00edamos acceder a los dos servidores web con HTTP (pod\u00e9is probarlo) pero es algo que solucionaremos en siguientes temas, configurando un firewall para que s\u00f3lo la IP del proxy inverso pueda acceder por HTTP a los servidores web y nadie m\u00e1s.","title":"Tarea"},{"location":"P1.5/#creacion-de-certificado-autofirmado","text":"Nosotros no utilizaremos certificados de ninguna CA de confianza, b\u00e1sicamente porque: Nuestros servicios no est\u00e1n publicados en Internet Estos certificados son de pago As\u00ed pues, nosotros crearemos nuestros propios certificados y los firmaremos nosotros mismos como si fu\u00e9ramos una CA aut\u00e9ntica para poder simular este escenario. Warning Esto provocar\u00e1 que cuando accedamos por HTTPS a nuestro sitio web por primera vez, nos salt\u00e9 el aviso de seguridad que se comentaba en la introducci\u00f3n. En este caso no habr\u00e1 peligro puesto que estamos 100% seguros que ese certificado lo hemos emitido nosotros para esta pr\u00e1ctica, no hay dudas. Veamos pues el proceso para generar los certificados y las claves asociadas a ellos (privada/p\u00fablica). En primer lugar debemos crear el siguiente directorio: /etc/nginx/ssl Podemos crear el certificado y las claves de forma simult\u00e1nea con un \u00fanico comando, donde: openssl : esta es la herramienta por l\u00ednea de comandos b\u00e1sica para crear y administrar certificados, claves y otros archivos OpenSSL. req : este subcomando se utiliza para generar una solicitud de certificados y tambi\u00e9n solicitudes de firma de certificados (CSR). -x509 : Esto modifica a\u00fan m\u00e1s el subcomando anterior al decirle a la herramienta que queremos crear un certificado autofirmado en lugar de generar una solicitud de firma de certificado, como suceder\u00eda normalmente. -nodes : Esto le dice a OpenSSL que omita la opci\u00f3n de asegurar nuestro certificado con contrase\u00f1a. Necesitamos que Nginx pueda leer el archivo sin la intervenci\u00f3n del usuario cuando se inicia el servidor. Una contrase\u00f1a evitar\u00eda que esto sucediera ya que tendr\u00edamos que introducirla a mano despu\u00e9s de cada reinicio. -days 365 : esta opci\u00f3n establece el tiempo durante el cual el certificado se considerar\u00e1 v\u00e1lido. Lo configuramos para un a\u00f1o. -newkey rsa: 2048 : Esto especifica que queremos generar un nuevo certificado y una nueva clave al mismo tiempo. No creamos la clave necesaria para firmar el certificado en un paso anterior, por lo que debemos crearla junto con el certificado. La rsa:2048parte le dice que cree una clave RSA de 2048 bits de longitud. -keyout : este par\u00e1metro le dice a OpenSSL d\u00f3nde colocar el archivo de clave privada generado que estamos creando. -out : Esto le dice a OpenSSL d\u00f3nde colocar el certificado que estamos creando. El comando completo ser\u00eda as\u00ed: Os solicitar\u00e1 que introduzc\u00e1is una serie de par\u00e1metros, como v\u00e9is en el recuadro rojo de abajo de la imagen. Deb\u00e9is introducir los mismos par\u00e1metros que en la imagen excepto en el \u201cOrganizational Unit Name\u201d que v\u00e9is recuadrado en amarillo. Ah\u00ed deber\u00e9is poner 2DAW \u2013 DEAW - Vuestronombre","title":"Creaci\u00f3n de certificado autofirmado"},{"location":"P1.5/#configuracion-ssl-en-el-proxy-inverso","text":"De la pr\u00e1ctica anterior, dentro del directorio /etc/nginx/sites-available ya deb\u00e9is tener el archivo de configuraci\u00f3n llamado \u201cbalanceo\u201d. Es precisamente aqu\u00ed donde realizaremos la configuraci\u00f3n para que el acceso al sitio web se realice mediante SSL (HTTPS). Dentro del bloque server {\u2026} deb\u00e9is cambiar el puerto de escucha ( listen 80 ) por lo que v\u00e9is en la imagen de abajo, a\u00f1adiendo las siguientes l\u00edneas de configuraci\u00f3n tambi\u00e9n, de tal forma que quede: Donde le est\u00e1is diciendo que: Escuche en el puerto 443 \u2192 Puerto por defecto de HTTPS El directorio donde est\u00e1 el certificado que hab\u00e9is generado anteriormente El directorio donde est\u00e1 la clave que hab\u00e9is generado anteriormente Los protocolos y tipos de cifrados que se pueden utilizar \u2192 Estas son las versiones de protocolos y los tipos de cifrados considerados seguros a d\u00eda de hoy (hay muchos m\u00e1s pero no se consideran seguros actualmente) server_name ya lo ten\u00edais de la pr\u00e1ctica anterior, no hace falta tocarlo El archivo donde se guardan los logs cambia de nombre, ahora ser\u00e1 https_access.log Recordad que tras modificar cualquier configuraci\u00f3n de un servicio, hay que reiniciar el servicio, en este caso Nginx.","title":"Configuraci\u00f3n SSL en el proxy inverso"},{"location":"P1.5/#comprobaciones","text":"Si acced\u00e9is ahora a https://balanceo os deber\u00eda saltar un aviso de seguridad debido a que nuestro certificado es autofirmado, como coment\u00e1bamos anteriormente. Si a\u00f1ad\u00eds una una excepci\u00f3n podr\u00e9is acceder al sitio web y recargando repetidamente la p\u00e1gina con F5, ver\u00e9is que el balanceo de carga se hace correctamente accediendo mediante HTTPS. Para comprobar que los datos del certificado son, efectivamente, los vuestros pod\u00e9is comprobarlo as\u00ed. Pulsando en el candado de la barra de b\u00fasqueda: Con m\u00e1s informaci\u00f3n: Info Aqu\u00ed tambi\u00e9n podr\u00e9is eliminar la excepci\u00f3n que hab\u00e9is a\u00f1adido en la p\u00e1gina de la advertencia de seguridad, por si necesit\u00e1is reiniciar las pruebas. Y por \u00faltimo, ver certificado: Y podremos ver los detalles: Si ahora intent\u00e1is acceder a http://balanceo , \u00bfdeber\u00edais poder acceder? Comprobadlo y describid qu\u00e9 pasa y por qu\u00e9.","title":"Comprobaciones"},{"location":"P1.5/#redireccion-forzosa-a-https","text":"Para que, indistintamente de la forma por la que accedamos al sitio web balanceo, siempre se fuerce a utilizar HTTPS, necesitaremos una configuraci\u00f3n adicional. Necesitamos a\u00f1adir un bloque \u201cserver\u201d adicional y separado del otro, al archivo de configuraci\u00f3n de \u201cbalanceo\u201d. Algo as\u00ed: Con esta configuraci\u00f3n le estamos diciendo que: Escuche en el puerto 80 (HTTP) Que el nombre al que responder\u00e1 el servidor/sitio web es balanceo Que guarde los logs de este bloque en ese directorio y con ese nombre Cuando se recibe una petici\u00f3n con las dos condiciones anteriores, se devuelve un c\u00f3digo HTTP 301: HTTP 301 Moved Permanently (Movido permanentemente en espa\u00f1ol) es un c\u00f3digo de estado de HTTP que indica que el host ha sido capaz de comunicarse con el servidor pero que el recurso solicitado ha sido movido a otra direcci\u00f3n permanentementeEs muy importante configurar las redirecciones 301 en los sitios web y para ello hay diferentes m\u00e9todos y sintaxis para realizar la redirecci\u00f3n 301. La redirecci\u00f3n 301 es un c\u00f3digo o comando insertado por un Webmaster que permite redirigir a los usuarios y buscadores de un sitio web de un sitio a otro. Aclaraci\u00f3n Es decir, lo que estamos haciendo es que cuando se reciba una petici\u00f3n HTTP (puerto 80) en http://balanceo , se redirija a https://balanceo (HTTPS) Tarea Eliminad del otro bloque server{\u2026} la l\u00edneas que hagan referencia a escuchar en el puerto 80 (listen 80\u2026). Reiniciad el servicio Comprobad ahora que cuando entr\u00e1is en http://balanceo , autom\u00e1ticamente os redirige a la versi\u00f3n segura de la web. Comprobad que cuando realiz\u00e1is una petici\u00f3n en el archivo de log http_access.log aparece la redirecci\u00f3n 301 y que, de la misma manera, aparece una petici\u00f3n GET en https_access.log .","title":"Redirecci\u00f3n forzosa a HTTPS"},{"location":"P1.5/#cuestiones-finals","text":"Cuesti\u00f3n 1 Hemos configurado nuestro proxy inverso con todo lo que nos hace falta pero no nos funciona y da un error del tipo This site can't provide a secure connection, ERR_SSL_PROTOCOL_ERROR. Dentro de nuestro server block tenemos esto: server { listen 443 ; ssl_certificate /etc/nginx/ssl/enrico-berlinguer/server.crt ; ssl_certificate_key /etc/nginx/ssl/enrico-berlinguer/server.key ; ssl_protocols TLSv1.3; ssl_ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS; server_name enrico-berlinguer; access_log /var/log/nginx/https_access.log ; location / { proxy_pass http://red-party; } } Cuesti\u00f3n 2 Imaginad que intentamos acceder a nuestro sitio web HTTPS y nos encontramos con el siguiente error: Investigad qu\u00e9 est\u00e1 pasando y como se ha de solucionar.","title":"Cuestiones finals"},{"location":"P1.5/#evaluacion","text":"Criterio Puntuaci\u00f3n Creaci\u00f3n correcta del certificado 1 puntos Configuraci\u00f3n SSL correcta del proxy 3 puntos Comprobaciones 2 puntos Configuraci\u00f3n correcta de la redirecci\u00f3n forzosa a HTTPS y comprobaciones 1 puntos Cuestiones finales 2 puntos SSe ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 1 punto","title":"Evaluaci\u00f3n"},{"location":"P3.1-Tomcat/","text":"Pr\u00e1ctica 3.1: Instalaci\u00f3n de Tomcat Introducci\u00f3n Si consultamos el apartado de versiones de Tomcat en su p\u00e1gina oficial, nos daremos cuenta de que no vamos a usar la \u00faltima versi\u00f3n, la 10, para esta pr\u00e1ctica, sino la anterior, la 9. La pregunta es casi inevitable: \u00bfPor qu\u00e9? En el enlace anterior vemos como desde su versi\u00f3n 9, Tomcat da soporta a Java 8 y superiores. Sin embargo, a partir de la versi\u00f3n 10.1.x, da soporte a Java 11 y superiores. \u00bfQu\u00e9 significa esto? En Java 9 se introdujeron novedades como un nuevo sistema de m\u00f3dulos (Jigsaw), entre otras . En Java 11 se dio un paso m\u00e1s al haber renombrado completamente las rutas de paquetes javax. a jakarta. . Oracle, a pesar de haber hecho p\u00fablico el desarrollo de Java, no hizo lo mismo con su nombre. As\u00ed las cosas, resulta que Java 8 puede que a d\u00eda de hoy a\u00fan sea la m\u00e1s usada en proyectos reales. Dicho esto, podr\u00eda realizarse un proceso de migraci\u00f3n de un proyecto de Java 8 a Java 11 y utilizarlo en Tomcat 10. No obstante, para Java 8 su soporte para uso comercial (pagando) acab\u00f3 en Marzo de 2022 , pero para uso no comercial sigue hasta 2030. En conclusi\u00f3n, no es raro encontrarse en el mundo real un proyecto a desplegar realizado en Java 8. Podr\u00eda realizarse una migraci\u00f3n y los conceptos de despliegue que veremos seguir\u00edan aplicando. As\u00ed las cosas, por facilidad en la realizaci\u00f3n de las pr\u00e1cticas utilizaremos Tomcat 9 y el plugin oficial de Maven para Tomcat 7 para el despliegue (luego veremos el motivo). Instalaci\u00f3n de Tomcat Esta pr\u00e1ctica es muy sencilla y va a consistir en realizar la instalaci\u00f3n del servidor de aplicaciones Tomcat 9, en una m\u00e1quina virtual corriendo Debian 11 Bullseye. Se puede hacer tanto con el administrador de paquetes apt como de forma manual. La forma m\u00e1s recomendable por su sencillez es la primera. Para ello, y como sugerencia, pod\u00e9is apoyaros en este tutorial online , aunque s\u00f3is libres de consultar tantas fuentes como dese\u00e9is. Obviamente, deb\u00e9is utilizar vuestro propios usuarios y contrase\u00f1a. Despliegue manual mediante la GUI de administraci\u00f3n Realizaremos el despliegue manual de una aplicaci\u00f3n ya previamente empaquetada en formato WAR. Para ello: Nos logueamos con el usuario previamente creado. Buscamos la secci\u00f3n que nos permite desplegar un WAR manualmente, seleccionamos nuestro archivo y lo desplegamos. Tras estos pasos, se nos listar\u00e1 la aplicaci\u00f3n ya desplegada como un directorio m\u00e1s y podremos acceder a ella. Task Documenta el despliegue manual de la aplicaci\u00f3n que os pod\u00e9is descargar para tal efecto en Aules (archivo .war). Despliegue con Maven Instalaci\u00f3n de Maven Para instalar Maven en nuestro Debian tenemos, de nuevo, dos opciones: Instalaci\u00f3n mediante gestor de paquetes APT Instalaci\u00f3n manual La primera, recomendada , es mucho m\u00e1s sencilla y automatizada (establece todos los paths y variables de entorno), aunque con la segunda se podr\u00eda conseguir un paquete m\u00e1s actualizado. Ambos m\u00e9todos vienen explicados aqu\u00ed Si decidimos seguir el primer m\u00e9todo, el m\u00e1s sencillo, vemos que es tan simple como actualizar los repositorios: sudo apt update E instalar Maven sudo apt install maven Para comprobar que todo ha ido correctamente, podemos ver la versi\u00f3n instalada de Maven: mvn --v Configuraci\u00f3n de Maven Para poder realizar despliegues en nuestro Tomcat previamente instalado, necesitamos realizar la configuraci\u00f3n adecuada para Maven. Ya sabemos que esto en Linux significa editar los archivos de configuraci\u00f3n adecuados. Vamos a ello. En primer lugar necesitamos asegurarnos de que en el apartado anterior de la pr\u00e1ctica hemos a\u00f1adido todos los usuarios necesarios, as\u00ed como sus respectivos roles. Debemos a\u00f1adir el rol de manager-script para permitir que Maven se autentique contra Tomcat y pueda realizar el despliegue. Los roles utilizados por Tomcat vienen detallados en su documentaci\u00f3n , que merece ser consultada: En dicha documentaci\u00f3n se nos indica que, por temas de seguridad, es recomendable no otorgar los roles de manager-script o manager-jmx al mismo usuario que tenga el rol de manager-gui . Info Tendremos dos usuarios, uno para la GUI y otro exclusivamente para hacer los deploys de Maven. As\u00ed las cosas, modificamos el archivo /etc/tomcat9/tomcat-users.xml acorde a nuestras necesidades (los nombres de usuario y contrase\u00f1a deber\u00e1n ser los que elij\u00e1is para vosotros): Editar el archivo /etc/maven/settings.xml para indicarle a Maven, un identificador para el servidor sobre el que vamos a desplegar (no es m\u00e1s que un nombre, ponedle el nombre que consider\u00e9is), as\u00ed como las credenciales. Todo esto se har\u00e1 dentro del bloque servers del XML: Ahora debemos modificar el POM del proyecto para que haga referencia a que el despliegue se realice con el plugin de Maven para Tomcat. Info No existen plugins oficiales para Tomcat m\u00e1s all\u00e1 de la versi\u00f3n 7 del servidor. No obstante, el plugin para Tomcat 7 sigue funcionando correctamente con Tomcat 9. Otra opci\u00f3n ser\u00eda utilizar el plugin Cargo Donde lo que a\u00f1adimos es el bloque <build> <finalName>war-deploy</finalName> #(1) <plugins> <plugin> <groupId>org.apache.tomcat.maven</groupId> <artifactId>tomcat7-maven-plugin</artifactId> <version>2.2</version> <configuration> <url>http://localhost:8080/manager/text</url> #(2) <server>Tomcat.P.3.1</server> #(3) <path>/myapp</path> #(4) </configuration> </plugin> </plugins> </build> Nombre final del ejecutable .jar que se va a generar URL del servidor Tomcat donde se har\u00e1 el despliegue. Como en nuestro caso Maven y Tomcat est\u00e1n en el mismo servidor, la URL corresponde a localhost. Esta URL debe ir seguida por /manager/text , tal y como leemos en la documentaci\u00f3n del plugin . Nombre del server donde se va a desplegar la aplicaci\u00f3n. El nombre debe ser consistente con lo que hayamos puesto en el settings.xml del paso anterior. Nombre que la aplicaci\u00f3n utilizar\u00e1 en el path de la URL Despliegue Teniendo ya todo listo para realizar despliegues, ahora crearemos una aplicaci\u00f3n Java de prueba para ver si podemos desplegarla sobre la arquitectura que hemos montado. Para ello utilizamos el comando: mvn archetype:generate -DgroupId = raul -DartifactId = war-deploy -DarchetypeArtifactId = maven-archetype-webapp -DinteractiveMode = false Pod\u00e9is sustituir los valores de groupID y artifactId (este ser\u00e1 el nombre de la aplicaci\u00f3n) por lo que quer\u00e1is. Tras generar esta aplicaci\u00f3n, los comandos finales que se utilizan en Maven para desplegar, volver a desplegar o desplegar una aplicaci\u00f3n, son: mvn tomcat7:deploy mvn tomcat7:redeploy mvn tomcat7:undeploy As\u00ed pues, tras el despliegue con Maven nos indicar\u00e1 que todo ha ido correctamente con un mensaje de BUILD SUCCESS , tal que as\u00ed: Y, accediendo a trav\u00e9s de la GUI, debemos ver que la aplicaci\u00f3n est\u00e1 desplegado y que podemos acceder a ella perfectamente: Tarea Realizar el despliegue con la aplicaci\u00f3n de prueba. Repetir el despliegue pero esta vez con otra aplicaci\u00f3n que no es la de prueba. M\u00e1s adelante ya hablaremos de git pero de momento, usaremos los comandos que veremos a continuaci\u00f3n. Nos clonamos el repositorio: git clone https://github.com/cameronmcnz/rock-paper-scissors.git Nos situamos dentro de \u00e9l: cd rock-paper-scissors Y cambiamos de rama: git checkout patch-1 Tras esto debemos proceder exactamente igual que en el caso anterior, con la ventaja de que ya tenemos configurados los usuarios de Tomcat y los par\u00e1metros de Maven. As\u00ed pues, s\u00f3lo habr\u00eda que a\u00f1adir el bloque <plugin>...</plugin> adecuado para poder hacer nuestro despliegue. Task Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final. Cuestiones Hab\u00e9is visto que los archivos de configuraci\u00f3n que hemos tocado contienen contrase\u00f1as en texto plano, por lo que cualquiera con acceso a ellos obtendr\u00eda las credenciales de nuestras herramientas. En principio esto representa un gran riesgo de seguridad, \u00bfsabr\u00edas razonar o averig\u00fcar por qu\u00e9 esto est\u00e1 dise\u00f1ado de esta forma? Referencias Tutorial Tomcat I Tutorial Tomcat II Tutorial Tomcat para Ubuntu Instalaci\u00f3n Maven JSF 3.0 en Tomcat 10 con Java 11 Migraci\u00f3n de Java 8 a Java 11 Install and configure jdk11 + Tomcat + Maven under Linux system Step-by-step Maven Tomcat WAR file deploy example How to deploy the java application to Tomcat 9 webserver using Maven How to Install Apache Maven on Debian 11 Bullseye How to Deploy a WAR File to Tomcat Migrate Maven Projects to Java 11 How to configure Tomcat 9.0 in Maven Github: cameronmcnz/rock-paper-scissors Why are plain text passwords in the config files? How to avoid storing passwords in the clear for tomcat's server.xml Resource definition of a DataSource? Evaluaci\u00f3n Criterio Puntuaci\u00f3n Despliegue manual de un .war en Tomcat correcto y bien documentado 1 punto Despliegue del .war de prueba utilizando maven correcto y bien documentado 0.25 puntos Despliegue de una aplicaci\u00f3n con maven desde un repositorio de Github correcto y bien documentado 4.75 puntos Cuestiones 2 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Pr\u00e1ctica 3.1: Instalaci\u00f3n de Tomcat y Maven para despliegue de aplicaci\u00f3n Java"},{"location":"P3.1-Tomcat/#practica-31-instalacion-de-tomcat","text":"","title":"Pr\u00e1ctica 3.1: Instalaci\u00f3n de Tomcat"},{"location":"P3.1-Tomcat/#introduccion","text":"Si consultamos el apartado de versiones de Tomcat en su p\u00e1gina oficial, nos daremos cuenta de que no vamos a usar la \u00faltima versi\u00f3n, la 10, para esta pr\u00e1ctica, sino la anterior, la 9. La pregunta es casi inevitable:","title":"Introducci\u00f3n"},{"location":"P3.1-Tomcat/#por-que","text":"En el enlace anterior vemos como desde su versi\u00f3n 9, Tomcat da soporta a Java 8 y superiores. Sin embargo, a partir de la versi\u00f3n 10.1.x, da soporte a Java 11 y superiores. \u00bfQu\u00e9 significa esto? En Java 9 se introdujeron novedades como un nuevo sistema de m\u00f3dulos (Jigsaw), entre otras . En Java 11 se dio un paso m\u00e1s al haber renombrado completamente las rutas de paquetes javax. a jakarta. . Oracle, a pesar de haber hecho p\u00fablico el desarrollo de Java, no hizo lo mismo con su nombre. As\u00ed las cosas, resulta que Java 8 puede que a d\u00eda de hoy a\u00fan sea la m\u00e1s usada en proyectos reales. Dicho esto, podr\u00eda realizarse un proceso de migraci\u00f3n de un proyecto de Java 8 a Java 11 y utilizarlo en Tomcat 10. No obstante, para Java 8 su soporte para uso comercial (pagando) acab\u00f3 en Marzo de 2022 , pero para uso no comercial sigue hasta 2030. En conclusi\u00f3n, no es raro encontrarse en el mundo real un proyecto a desplegar realizado en Java 8. Podr\u00eda realizarse una migraci\u00f3n y los conceptos de despliegue que veremos seguir\u00edan aplicando. As\u00ed las cosas, por facilidad en la realizaci\u00f3n de las pr\u00e1cticas utilizaremos Tomcat 9 y el plugin oficial de Maven para Tomcat 7 para el despliegue (luego veremos el motivo).","title":"\u00bfPor qu\u00e9?"},{"location":"P3.1-Tomcat/#instalacion-de-tomcat","text":"Esta pr\u00e1ctica es muy sencilla y va a consistir en realizar la instalaci\u00f3n del servidor de aplicaciones Tomcat 9, en una m\u00e1quina virtual corriendo Debian 11 Bullseye. Se puede hacer tanto con el administrador de paquetes apt como de forma manual. La forma m\u00e1s recomendable por su sencillez es la primera. Para ello, y como sugerencia, pod\u00e9is apoyaros en este tutorial online , aunque s\u00f3is libres de consultar tantas fuentes como dese\u00e9is. Obviamente, deb\u00e9is utilizar vuestro propios usuarios y contrase\u00f1a.","title":"Instalaci\u00f3n de Tomcat"},{"location":"P3.1-Tomcat/#despliegue-manual-mediante-la-gui-de-administracion","text":"Realizaremos el despliegue manual de una aplicaci\u00f3n ya previamente empaquetada en formato WAR. Para ello: Nos logueamos con el usuario previamente creado. Buscamos la secci\u00f3n que nos permite desplegar un WAR manualmente, seleccionamos nuestro archivo y lo desplegamos. Tras estos pasos, se nos listar\u00e1 la aplicaci\u00f3n ya desplegada como un directorio m\u00e1s y podremos acceder a ella. Task Documenta el despliegue manual de la aplicaci\u00f3n que os pod\u00e9is descargar para tal efecto en Aules (archivo .war).","title":"Despliegue manual mediante la GUI de administraci\u00f3n"},{"location":"P3.1-Tomcat/#despliegue-con-maven","text":"","title":"Despliegue con Maven"},{"location":"P3.1-Tomcat/#instalacion-de-maven","text":"Para instalar Maven en nuestro Debian tenemos, de nuevo, dos opciones: Instalaci\u00f3n mediante gestor de paquetes APT Instalaci\u00f3n manual La primera, recomendada , es mucho m\u00e1s sencilla y automatizada (establece todos los paths y variables de entorno), aunque con la segunda se podr\u00eda conseguir un paquete m\u00e1s actualizado. Ambos m\u00e9todos vienen explicados aqu\u00ed Si decidimos seguir el primer m\u00e9todo, el m\u00e1s sencillo, vemos que es tan simple como actualizar los repositorios: sudo apt update E instalar Maven sudo apt install maven Para comprobar que todo ha ido correctamente, podemos ver la versi\u00f3n instalada de Maven: mvn --v","title":"Instalaci\u00f3n de Maven"},{"location":"P3.1-Tomcat/#configuracion-de-maven","text":"Para poder realizar despliegues en nuestro Tomcat previamente instalado, necesitamos realizar la configuraci\u00f3n adecuada para Maven. Ya sabemos que esto en Linux significa editar los archivos de configuraci\u00f3n adecuados. Vamos a ello. En primer lugar necesitamos asegurarnos de que en el apartado anterior de la pr\u00e1ctica hemos a\u00f1adido todos los usuarios necesarios, as\u00ed como sus respectivos roles. Debemos a\u00f1adir el rol de manager-script para permitir que Maven se autentique contra Tomcat y pueda realizar el despliegue. Los roles utilizados por Tomcat vienen detallados en su documentaci\u00f3n , que merece ser consultada: En dicha documentaci\u00f3n se nos indica que, por temas de seguridad, es recomendable no otorgar los roles de manager-script o manager-jmx al mismo usuario que tenga el rol de manager-gui . Info Tendremos dos usuarios, uno para la GUI y otro exclusivamente para hacer los deploys de Maven. As\u00ed las cosas, modificamos el archivo /etc/tomcat9/tomcat-users.xml acorde a nuestras necesidades (los nombres de usuario y contrase\u00f1a deber\u00e1n ser los que elij\u00e1is para vosotros): Editar el archivo /etc/maven/settings.xml para indicarle a Maven, un identificador para el servidor sobre el que vamos a desplegar (no es m\u00e1s que un nombre, ponedle el nombre que consider\u00e9is), as\u00ed como las credenciales. Todo esto se har\u00e1 dentro del bloque servers del XML: Ahora debemos modificar el POM del proyecto para que haga referencia a que el despliegue se realice con el plugin de Maven para Tomcat. Info No existen plugins oficiales para Tomcat m\u00e1s all\u00e1 de la versi\u00f3n 7 del servidor. No obstante, el plugin para Tomcat 7 sigue funcionando correctamente con Tomcat 9. Otra opci\u00f3n ser\u00eda utilizar el plugin Cargo Donde lo que a\u00f1adimos es el bloque <build> <finalName>war-deploy</finalName> #(1) <plugins> <plugin> <groupId>org.apache.tomcat.maven</groupId> <artifactId>tomcat7-maven-plugin</artifactId> <version>2.2</version> <configuration> <url>http://localhost:8080/manager/text</url> #(2) <server>Tomcat.P.3.1</server> #(3) <path>/myapp</path> #(4) </configuration> </plugin> </plugins> </build> Nombre final del ejecutable .jar que se va a generar URL del servidor Tomcat donde se har\u00e1 el despliegue. Como en nuestro caso Maven y Tomcat est\u00e1n en el mismo servidor, la URL corresponde a localhost. Esta URL debe ir seguida por /manager/text , tal y como leemos en la documentaci\u00f3n del plugin . Nombre del server donde se va a desplegar la aplicaci\u00f3n. El nombre debe ser consistente con lo que hayamos puesto en el settings.xml del paso anterior. Nombre que la aplicaci\u00f3n utilizar\u00e1 en el path de la URL","title":"Configuraci\u00f3n de Maven"},{"location":"P3.1-Tomcat/#despliegue","text":"Teniendo ya todo listo para realizar despliegues, ahora crearemos una aplicaci\u00f3n Java de prueba para ver si podemos desplegarla sobre la arquitectura que hemos montado. Para ello utilizamos el comando: mvn archetype:generate -DgroupId = raul -DartifactId = war-deploy -DarchetypeArtifactId = maven-archetype-webapp -DinteractiveMode = false Pod\u00e9is sustituir los valores de groupID y artifactId (este ser\u00e1 el nombre de la aplicaci\u00f3n) por lo que quer\u00e1is. Tras generar esta aplicaci\u00f3n, los comandos finales que se utilizan en Maven para desplegar, volver a desplegar o desplegar una aplicaci\u00f3n, son: mvn tomcat7:deploy mvn tomcat7:redeploy mvn tomcat7:undeploy As\u00ed pues, tras el despliegue con Maven nos indicar\u00e1 que todo ha ido correctamente con un mensaje de BUILD SUCCESS , tal que as\u00ed: Y, accediendo a trav\u00e9s de la GUI, debemos ver que la aplicaci\u00f3n est\u00e1 desplegado y que podemos acceder a ella perfectamente:","title":"Despliegue"},{"location":"P3.1-Tomcat/#tarea","text":"Realizar el despliegue con la aplicaci\u00f3n de prueba. Repetir el despliegue pero esta vez con otra aplicaci\u00f3n que no es la de prueba. M\u00e1s adelante ya hablaremos de git pero de momento, usaremos los comandos que veremos a continuaci\u00f3n. Nos clonamos el repositorio: git clone https://github.com/cameronmcnz/rock-paper-scissors.git Nos situamos dentro de \u00e9l: cd rock-paper-scissors Y cambiamos de rama: git checkout patch-1 Tras esto debemos proceder exactamente igual que en el caso anterior, con la ventaja de que ya tenemos configurados los usuarios de Tomcat y los par\u00e1metros de Maven. As\u00ed pues, s\u00f3lo habr\u00eda que a\u00f1adir el bloque <plugin>...</plugin> adecuado para poder hacer nuestro despliegue. Task Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final.","title":"Tarea"},{"location":"P3.1-Tomcat/#cuestiones","text":"Hab\u00e9is visto que los archivos de configuraci\u00f3n que hemos tocado contienen contrase\u00f1as en texto plano, por lo que cualquiera con acceso a ellos obtendr\u00eda las credenciales de nuestras herramientas. En principio esto representa un gran riesgo de seguridad, \u00bfsabr\u00edas razonar o averig\u00fcar por qu\u00e9 esto est\u00e1 dise\u00f1ado de esta forma?","title":"Cuestiones"},{"location":"P3.1-Tomcat/#referencias","text":"Tutorial Tomcat I Tutorial Tomcat II Tutorial Tomcat para Ubuntu Instalaci\u00f3n Maven JSF 3.0 en Tomcat 10 con Java 11 Migraci\u00f3n de Java 8 a Java 11 Install and configure jdk11 + Tomcat + Maven under Linux system Step-by-step Maven Tomcat WAR file deploy example How to deploy the java application to Tomcat 9 webserver using Maven How to Install Apache Maven on Debian 11 Bullseye How to Deploy a WAR File to Tomcat Migrate Maven Projects to Java 11 How to configure Tomcat 9.0 in Maven Github: cameronmcnz/rock-paper-scissors Why are plain text passwords in the config files? How to avoid storing passwords in the clear for tomcat's server.xml Resource definition of a DataSource?","title":"Referencias"},{"location":"P3.1-Tomcat/#evaluacion","text":"Criterio Puntuaci\u00f3n Despliegue manual de un .war en Tomcat correcto y bien documentado 1 punto Despliegue del .war de prueba utilizando maven correcto y bien documentado 0.25 puntos Despliegue de una aplicaci\u00f3n con maven desde un repositorio de Github correcto y bien documentado 4.75 puntos Cuestiones 2 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Evaluaci\u00f3n"},{"location":"P3.2-NodeJS-Express/","text":"Pr\u00e1ctica 3.2: Despliegue de aplicaciones con Node Express Introducci\u00f3n En esta pr\u00e1ctica vamos a realizar el despliegue de aplicaciones Node.js sobre un servidor Node Express. Lo curioso de este caso es que el despliegue aqu\u00ed cambia un poco puesto que no se hace sobre el servidor, sino que la aplicaci\u00f3n es el servidor. Warning Comprueba que el servidor Tomcat de pr\u00e1cticas anteriores no est\u00e1 corriendo o nos dar\u00e1 problemas: sudo systemctl status tomcat9 Y en caso de salir activo, pararlo: sudo systemctl stop tomcat9 Instalaci\u00f3n de Node.js, Express y test de la primera aplicaci\u00f3n La primera parte de la pr\u00e1ctica es muy sencilla. Consistir\u00e1 en instalar sobre nuestra Debian 11 tanto Node.js como Express y tras ello crear un archivo .js de prueba para comprobar que nuestro primer despliegue funciona correctamente. Para ello, os pod\u00e9is apoyar en este sencillo tutorial . En lugar de acceder a http://localhost:3000 , deb\u00e9is acceder desde vuestra m\u00e1quina local a http://IP-maq-virtual:3000 , utilizando la IP concreta de vuestra m\u00e1quina virtual. Recordad parar el servidor (CTRL+C) al acabar la pr\u00e1ctica. Task Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final. Despliegue de una nueva aplicaci\u00f3n Vamos ahora a realizar el despliegue de una aplicaci\u00f3n de terceros para ver c\u00f3mo es el proceso. Se trata de un \"prototipo\" de una especie de CMS que pod\u00e9is encontrar en este repositorio de Github . Tal y como indican las instrucciones del propio repositorio, los pasos a seguir son, en primer lugar, clonar el repositorio a nuesta m\u00e1quina: git clone https://github.com/contentful/the-example-app.nodejs.git Movernos al nuevo directorio: cd the-example-app.nodejs Instalar las librer\u00edas necesarias (paciencia, este proceso puede tardar un buen rato): npm install Y, por \u00faltimo, iniciar la aplicaci\u00f3n: npm run start:dev Tarea Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final. Cuestiones Cuando ejecut\u00e1is el comando npm run start:dev , lo que est\u00e1is haciendo es ejecutar un script: \u00bfDonde podemos ver que script se est\u00e1 ejecutando? \u00bfQu\u00e9 comando est\u00e1 ejecutando? Como ayuda, pod\u00e9is consultar esta informaci\u00f3n . Referencias How to install ExpressJS on Debian 11? Evaluaci\u00f3n Criterio Puntuaci\u00f3n Instalaci\u00f3n de Node.js, Express y despligue de primera aplicaci\u00f3n correcto y bien documentado 3 puntos Despliegue de una nueva aplicaci\u00f3n de forma correcta y bien documentada 3 puntos Cuestiones 2 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Pr\u00e1ctica 3.2 - Despliegue de aplicaciones con Node Express"},{"location":"P3.2-NodeJS-Express/#practica-32-despliegue-de-aplicaciones-con-node-express","text":"","title":"Pr\u00e1ctica 3.2: Despliegue de aplicaciones con Node Express"},{"location":"P3.2-NodeJS-Express/#introduccion","text":"En esta pr\u00e1ctica vamos a realizar el despliegue de aplicaciones Node.js sobre un servidor Node Express. Lo curioso de este caso es que el despliegue aqu\u00ed cambia un poco puesto que no se hace sobre el servidor, sino que la aplicaci\u00f3n es el servidor. Warning Comprueba que el servidor Tomcat de pr\u00e1cticas anteriores no est\u00e1 corriendo o nos dar\u00e1 problemas: sudo systemctl status tomcat9 Y en caso de salir activo, pararlo: sudo systemctl stop tomcat9","title":"Introducci\u00f3n"},{"location":"P3.2-NodeJS-Express/#instalacion-de-nodejs-express-y-test-de-la-primera-aplicacion","text":"La primera parte de la pr\u00e1ctica es muy sencilla. Consistir\u00e1 en instalar sobre nuestra Debian 11 tanto Node.js como Express y tras ello crear un archivo .js de prueba para comprobar que nuestro primer despliegue funciona correctamente. Para ello, os pod\u00e9is apoyar en este sencillo tutorial . En lugar de acceder a http://localhost:3000 , deb\u00e9is acceder desde vuestra m\u00e1quina local a http://IP-maq-virtual:3000 , utilizando la IP concreta de vuestra m\u00e1quina virtual. Recordad parar el servidor (CTRL+C) al acabar la pr\u00e1ctica. Task Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final.","title":"Instalaci\u00f3n de Node.js, Express y test de la primera aplicaci\u00f3n"},{"location":"P3.2-NodeJS-Express/#despliegue-de-una-nueva-aplicacion","text":"Vamos ahora a realizar el despliegue de una aplicaci\u00f3n de terceros para ver c\u00f3mo es el proceso. Se trata de un \"prototipo\" de una especie de CMS que pod\u00e9is encontrar en este repositorio de Github . Tal y como indican las instrucciones del propio repositorio, los pasos a seguir son, en primer lugar, clonar el repositorio a nuesta m\u00e1quina: git clone https://github.com/contentful/the-example-app.nodejs.git Movernos al nuevo directorio: cd the-example-app.nodejs Instalar las librer\u00edas necesarias (paciencia, este proceso puede tardar un buen rato): npm install Y, por \u00faltimo, iniciar la aplicaci\u00f3n: npm run start:dev Tarea Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final.","title":"Despliegue de una nueva aplicaci\u00f3n"},{"location":"P3.2-NodeJS-Express/#cuestiones","text":"Cuando ejecut\u00e1is el comando npm run start:dev , lo que est\u00e1is haciendo es ejecutar un script: \u00bfDonde podemos ver que script se est\u00e1 ejecutando? \u00bfQu\u00e9 comando est\u00e1 ejecutando? Como ayuda, pod\u00e9is consultar esta informaci\u00f3n .","title":"Cuestiones"},{"location":"P3.2-NodeJS-Express/#referencias","text":"How to install ExpressJS on Debian 11?","title":"Referencias"},{"location":"P3.2-NodeJS-Express/#evaluacion","text":"Criterio Puntuaci\u00f3n Instalaci\u00f3n de Node.js, Express y despligue de primera aplicaci\u00f3n correcto y bien documentado 3 puntos Despliegue de una nueva aplicaci\u00f3n de forma correcta y bien documentada 3 puntos Cuestiones 2 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Evaluaci\u00f3n"},{"location":"P3.3-Cluster/","text":"Pr\u00e1ctica 3.3: Despliegue de una aplicaci\u00f3n \"clusterizada\" con Node Express Introducci\u00f3n Cuando se construye una aplicaci\u00f3n de producci\u00f3n, normalmente se busca la forma de optimizar su rendimiento llegando a una soluci\u00f3n de compromiso. En esta pr\u00e1ctica echaremos un vistazo a un enfoque que puede ofrecer una victoria r\u00e1pida cuando se trata de mejorar la manera en que las aplicaciones Node.js manejan la carga de trabajo. Una instancia de Node.js se ejecuta en un solo hilo, lo que significa que en un sistema multin\u00facleo (como la mayor\u00eda de los ordenadores de hoy en d\u00eda), no todos los n\u00facleos ser\u00e1n utilizados por la aplicaci\u00f3n. Para aprovechar los otros n\u00facleos disponibles, podemos lanzar un cluster de procesos Node.js y distribuir la carga entre ellos. Tener varios hilos para manejar las peticiones mejora el rendimiento (peticiones/segundo) del servidor, ya que varios clientes pueden ser atendidos simult\u00e1neamente. Veremos c\u00f3mo crear procesos hijos con el m\u00f3dulo de cluster de Node.js para, m\u00e1s tarde, ver c\u00f3mo gestionar el cluster con el gestor de procesos PM2. Un vistazo r\u00e1pido a los clusters El m\u00f3dulo de cl\u00faster de Node.js permite la creaci\u00f3n de procesos secundarios ( workers ) que se ejecutan simult\u00e1neamente y comparten el mismo puerto de servidor. Cada hijo generado tiene su propio ciclo de eventos y memoria. Los procesos secundarios utilizan IPC (comunicaci\u00f3n entre procesos) para comunicarse con el proceso principal de Node.js. Tener m\u00faltiples procesos para manejar las solicitudes entrantes significa que se pueden procesar varias solicitudes simult\u00e1neamente y si hay una operaci\u00f3n de bloqueo/ejecuci\u00f3n prolongada en un worker , los otros workers pueden continuar administrando otras solicitudes entrantes; la aplicaci\u00f3n no se detendr\u00e1 hasta que finalice la operaci\u00f3n de bloqueo. La ejecuci\u00f3n de varios workers tambi\u00e9n permite actualizar la aplicaci\u00f3n en producci\u00f3n con poco o ning\u00fan tiempo de inactividad. Se pueden realizar cambios en la aplicaci\u00f3n y reiniciar los workers uno por uno, esperando que un proceso secundario se genere por completo antes de reiniciar otro. De esta manera, siempre habr\u00e1 workers ejecut\u00e1ndose mientras se produce la actualizaci\u00f3n. Las conexiones entrantes se distribuyen entre los procesos secundarios de dos maneras: El proceso maestro escucha las conexiones en un puerto y las distribuye entre los workers de forma rotatoria. Este es el enfoque por defecto en todas las plataformas, excepto Windows. El proceso maestro crea un socket de escucha y lo env\u00eda a los workers interesados \u200b\u200bque luego podr\u00e1n aceptar conexiones entrantes directamente. Usando los clusters Primero sin cl\u00faster Para ver las ventajas que ofrece la agrupaci\u00f3n en cl\u00fasteres, comenzaremos con una aplicaci\u00f3n de prueba en Node.js que no usa cl\u00fasteres y la compararemos con una que s\u00ed los usa, se trata de la siguiente: const express = require ( \"express\" ); const app = express (); const port = 3000 ; app . get ( \"/\" , ( req , res ) => { res . send ( \"Hello World!\" ); }); app . get ( \"/api/:n\" , function ( req , res ) { let n = parseInt ( req . params . n ); let count = 0 ; if ( n > 5000000000 ) n = 5000000000 ; for ( let i = 0 ; i <= n ; i ++ ) { count += i ; } res . send ( `Final count is ${ count } ` ); }); app . listen ( port , () => { console . log ( `App listening on port ${ port } ` ); }); Se trata de una aplicaci\u00f3n un tanto prefabricada en el sentido de que es algo que jam\u00e1s encontrar\u00edamos en el mundo real. No obstante, nos servir\u00e1 para ilustrar nuestro prop\u00f3sito. Esta aplicaci\u00f3n contiene dos rutas, una ruta ra\u00edz / que devuelve la cadena Hello World! y otra ruta /api/n donde se toma n como par\u00e1metro y va realizando una operaci\u00f3n de suma (el bucle for) cuyo resultado acumula en la variable count que se muestra al final. Si a este par\u00e1metro n , le damos un valor muy alto, nos permitir\u00e1 simular operaciones intensivas y de ejecuci\u00f3n prolongada en el servidor. Le damos como valor l\u00edmite 5000000000 para evitar una operaci\u00f3n demasiado costosa para nuestro ordenador. Task Deb\u00e9is conectaros al servidor Debian mediante SSH Deb\u00e9is crear un directorio para el proyecto de esta aplicaci\u00f3n DENTRO del directorio ejecutar\u00e9is 2 comandos: npm init para crear autom\u00e1ticamente la estructura de carpetas y el archivo package.json (Con ir d\u00e1ndole a <ENTER> a todas las preguntas, os basta) npm install express para instalar express para este proyecto Tras esto, DENTRO del directorio, ya pod\u00e9is iniciar la aplicaci\u00f3n con: node nombre_aplicacion.js Para comprobarlo, pod\u00e9is acceder a http://IP-maq-virtual:3000 o a http://IP-maq-virtual:3000/api/50 donde IP-maq-virtual es la IP del adaptador puente de vuestra Debian. Utilizada un valor de n relativamente peque\u00f1o, como el 50 del ejemplo anterior y comprobar\u00e9is que se ejecutar\u00e1 r\u00e1pidamente, devolviendo una respuesta casi inmediata. Hagamos otra simple comprobaci\u00f3n para valores de n m\u00e1s grandes. Desplegada e iniciada la aplicaci\u00f3n, acceded a la ruta http://IP-maq-virtual:3000/api/5000000000 . Mientras esta solicitud que tarda unos segundos se est\u00e1 procesando, acceded en otra pesta\u00f1a del navegador a http://IP-maq-virtual:3000 o a http://IP-maq-virtual:3000/api/n siendo n el valor que le quer\u00e1is dar. Utilizando las devoloper tools, podemos ver el tiempo que tardan en procesarse las solicitudes: La primera solicitud, al tener un valor de n grande, nos lleva unos cuantos segundos completarla. La segunda solicitud, pese a tener un valor de n que ya hab\u00edamos comprobado que ofrec\u00eda una respuesta casi inmediata, tambi\u00e9n se demora unos segundos. \u00bfPor qu\u00e9 ocurre esto? Porque el \u00fanico subproceso estar\u00e1 ocupado procesando la otra operaci\u00f3n de ejecuci\u00f3n prolongada. El \u00fanico n\u00facleo de la CPU tiene que completar la primera solicitud antes de que pueda encargarse de la otra. \u00a1Ahora con m\u00e1s cl\u00faster! Ahora usaremos el m\u00f3dulo de cl\u00faster en la aplicaci\u00f3n para generar algunos procesos secundarios y ver c\u00f3mo eso mejora las cosas. A continuaci\u00f3n se muestra la aplicaci\u00f3n modificada: const express = require ( \"express\" ); const port = 3000 ; const cluster = require ( \"cluster\" ); const totalCPUs = require ( \"os\" ). cpus (). length ; if ( cluster . isMaster ) { console . log ( `Number of CPUs is ${ totalCPUs } ` ); console . log ( `Master ${ process . pid } is running` ); // Fork workers. for ( let i = 0 ; i < totalCPUs ; i ++ ) { cluster . fork (); } cluster . on ( \"exit\" , ( worker , code , signal ) => { console . log ( `worker ${ worker . process . pid } died` ); console . log ( \"Let's fork another worker!\" ); cluster . fork (); }); } else { const app = express (); console . log ( `Worker ${ process . pid } started` ); app . get ( \"/\" , ( req , res ) => { res . send ( \"Hello World!\" ); }); app . get ( \"/api/:n\" , function ( req , res ) { let n = parseInt ( req . params . n ); let count = 0 ; if ( n > 5000000000 ) n = 5000000000 ; for ( let i = 0 ; i <= n ; i ++ ) { count += i ; } res . send ( `Final count is ${ count } ` ); }); app . listen ( port , () => { console . log ( `App listening on port ${ port } ` ); }); } Esta aplicaci\u00f3n hace lo mismo que antes pero esta vez estamos generando varios procesos secundarios que compartir\u00e1n el puerto 3000 y que podr\u00e1n manejar las solicitudes enviadas a este puerto. Los procesos de trabajo se generan utilizando el m\u00e9todo child_process.fork() . El m\u00e9todo devuelve un objeto ChildProcess que tiene un canal de comunicaci\u00f3n incorporado que permite que los mensajes se transmitan entre el hijo y su padre. Creamos tantos procesos secundarios como n\u00facleos de CPU hay en la m\u00e1quina en la que se ejecuta la aplicaci\u00f3n. Se recomienda no crear m\u00e1s workers que n\u00facleos l\u00f3gicos en la computadora, ya que esto puede causar una sobrecarga en t\u00e9rminos de costos de programaci\u00f3n. Esto sucede porque el sistema tendr\u00e1 que programar todos los procesos creados para que se vayan ejecutando por turnos en los n\u00facleos. Los workers son creados y administrados por el proceso maestro. Cuando la aplicaci\u00f3n se ejecuta por primera vez, verificamos si es un proceso maestro con isMaster . Esto est\u00e1 determinado por la variable process.env.NODE_UNIQUE_ID . Si process.env.NODE_UNIQUE_ID tiene valor undefined , entonces isMaster ser\u00e1 true . Si el proceso es un maestro, llamamos a cluster.fork() para generar varios procesos. Registramos los ID de proceso maestro y worker . Cuando un proceso secundario muere, generamos uno nuevo para seguir utilizando los n\u00facleos de CPU disponibles. Ahora repetiremos el mismo experimento de antes, primero realizamos una solicitud al servidor con un valor alto n : Y ejecutamos r\u00e1pidamente otra solicitud en otra pesta\u00f1a del navegador, midiendo los tiempos de procesamiento de ambas: Comprobaremos que \u00e9stos se reducen dr\u00e1sticamente. Note Con varios workers disponibles para aceptar solicitudes, se mejoran tanto la disponibilidad del servidor como el rendimiento. Ejecutar una solicitud en una pesta\u00f1a del navegador y ejecutar r\u00e1pidamente otra en una segunda pesta\u00f1a sirve para mostrarnos la mejora que ofrece la agrupaci\u00f3n en cl\u00fasteres para nuestro ejemplo de una forma m\u00e1s o menos r\u00e1pida, pero es un m\u00e9todo un tanto \"chapucero\" y no es una forma adecuada o confiable de determinar las mejoras de rendimiento. En el siguiente apartado echaremos un vistazo a algunos puntos de referencia que demostrar\u00e1n mejor cu\u00e1nto ha mejorado la agrupaci\u00f3n en cl\u00fasteres nuestra aplicaci\u00f3n. M\u00e9tricas de rendimiento Realizaremos una prueba de carga en nuestras dos aplicaciones para ver c\u00f3mo cada una maneja una gran cantidad de conexiones entrantes. Usaremos el paquete loadtest para esto. El paquete loadtest nos permite simular una gran cantidad de conexiones simult\u00e1neas a nuestra API para que podamos medir su rendimiento. Para usar loadtest , primero debemos instalarlo globalmente. Tras conectaros por SSH al servidor Debian: npm install -g loadtest Luego ejecutamos la aplicaci\u00f3n que queremos probar ( node nombre_aplicacion.js ). Comenzaremos probando la versi\u00f3n que no utiliza la agrupaci\u00f3n en cl\u00fasteres. Mientras ejecutamos la aplicaci\u00f3n, en otro terminal realizamos la siguiente prueba de carga: loadtest http://localhost:3000/api/500000 -n 1000 -c 100 El comando anterior enviar\u00e1 1000 solicitudes a la URL dada, de las cuales 100 son concurrentes. El siguiente es el resultado de ejecutar el comando anterior: Vemos que con la misma solicitud (con n= 500000) el servidor ha podido manejar 404 solicitudes por segundo con una latencia media de 232.4 milisegundos (el tiempo promedio que tarda en completar una sola solicitud). Intent\u00e9moslo de nuevo, pero esta vez con m\u00e1s solicitudes (y sin cl\u00fasteres): Vemos que las m\u00e9tricas arrojan resultados a\u00fan peores. Ahora detenemos nuestra aplicaci\u00f3n sin cl\u00fasters y ejecutamos la que s\u00ed los tiene ( node nombre_aplicacion_cluster.js ). Ejecutaremos exactamente las mismas pruebas con el objetivo de realizar una comparaci\u00f3n: Es obvio que los cl\u00fasters permiten manejar una mayor cantidad de peticiones por segundo con una menor latencia. Uso de PM2 para administrar un cl\u00faster de Node.js En nuestra aplicaci\u00f3n, hemos usado el m\u00f3dulo cluster de Node.js para crear y administrar manualmente los procesos. Primero hemos determinado la cantidad de workers (usando la cantidad de n\u00facleos de CPU como referencia), luego los hemos generado y, finalmente, escuchamos si hay workers muertos para poder generar nuevos. En nuestra aplicaci\u00f3n de ejemplo muy sencilla, tuvimos que escribir una cantidad considerable de c\u00f3digo solo para administraci\u00f3n la agrupaci\u00f3n en cl\u00fasteres. En una aplicaci\u00f3n de producci\u00f3n es bastante probable que se deba escribir a\u00fan m\u00e1s c\u00f3digo. Existe una herramienta que nos puede ayudar a administrar todo esto un poco mejor: el administrador de procesos PM2 . PM2 es un administrador de procesos de producci\u00f3n para aplicaciones Node.js con un balanceador de carga incorporado. Cuando est\u00e1 configurado correctamente, PM2 ejecuta autom\u00e1ticamente la aplicaci\u00f3n en modo de cl\u00faster, generando workers y se encarga de generar nuevos workers cuando uno de ellos muera. PM2 facilita la parada, eliminaci\u00f3n e inicio de procesos, adem\u00e1s de disponer de algunas herramientas de monitorizaci\u00f3n que pueden ayudarnos a monitorizar y ajustar el rendimiento de su aplicaci\u00f3n. Para usar PM2 , primero instalamos globalmente en nuestra Debian: npm install pm2 -g Vamos a utilizarlo con nuestra primera aplicaci\u00f3n, la que no estab \"clusterizada\" en el c\u00f3digo. Para ello ejecutaremos el siguiente comando: pm2 start nombre_aplicacion_sin_cluster.js -i 0 Donde: -i le indicar\u00e1 a PM2 que inicie la aplicaci\u00f3n en cluster_mode (a diferencia de fork_mode ). Si se establece a 0, PM2 generar\u00e1 autom\u00e1ticamente tantos workers como n\u00facleos de CPU haya. Y as\u00ed, nuestra aplicaci\u00f3n se ejecuta en modo de cl\u00faster, sin necesidad de cambios de c\u00f3digo. Task Ejecuta y documenta con capturas de pantallas, las mismas pruebas que antes pero utilizando PM2 y comprueba si se obtienen los mismos resultados. Por detr\u00e1s, PM2 tambi\u00e9n utiliza el m\u00f3dulo cluster de Node.js, as\u00ed como otras herramientas que facilitan la gesti\u00f3n de procesos. En el Terminal, obtendremos una tabla que muestra algunos detalles de los procesos generados: Podemos detener la aplicaci\u00f3n con el siguiente comando: pm2 stop app.js La aplicaci\u00f3n se desconectar\u00e1 y la salida por terminal mostrar\u00e1 todos los procesos con un estado stopped . En vez de tener pasar siempre las configuraciones cuando ejecuta la aplicaci\u00f3n con pm2 start app.js -i 0 , podr\u00edamos facilitarnos la tarea y guardarlas en un archivo de configuraci\u00f3n separado, llamado Ecosystem . Este archivo tambi\u00e9n nos permite establecer configuraciones espec\u00edficas para diferentes aplicaciones. Crearemos el archivo Ecosystem con el siguiente comando: Que generar\u00e1 un archivo llamado ecosystem.config.js . Para el caso concreto de nuestra aplicaci\u00f3n, necesitamos modificarlo como se muestra a continuaci\u00f3n: module.expor ts = { apps : [ { na me : \"nombre_aplicacion\" , scrip t : \"nombre_aplicacion_sin_cluster.js\" , i nstan ces : 0 , exec_mode : \"cluster\" , }, ], } ; Al configurar exec_mode con el valor cluster , le indica a PM2 que balancee la carga entre cada instancia. instances est\u00e1 configurado a 0 como antes, lo que generar\u00e1 tantos workers como n\u00facleos de CPU. La opci\u00f3n -i o instances se puede establecer con los siguientes valores: 0 o max (en desuso) para \"repartir\" la aplicaci\u00f3n entre todas las CPU -1 para \"repartir\" la aplicaci\u00f3n en todas las CPU - 1 n\u00famero para difundir la aplicaci\u00f3n a trav\u00e9s de un n\u00famero concreto de CPU Ahora podemos ejecutar la aplicaci\u00f3n con: pm2 start ecosystem.config.js La aplicaci\u00f3n se ejecutar\u00e1 en modo cl\u00faster, exactamente como antes. Podremos iniciar, reiniciar, recargar, detener y eliminar una aplicaci\u00f3n con los siguientes comandos, respectivamente: $ pm2 start nombre_aplicacion $ pm2 restart nombre_aplicacion $ pm2 reload nombre_aplicacion $ pm2 stop nombre_aplicacion $ pm2 delete nombre_aplicacion # Cuando usemos el archivo Ecosystem: $ pm2 [ start | restart | reload | stop | delete ] ecosystem.config.js El comando restart elimina y reinicia inmediatamente los procesos, mientras que el comando reload logra un tiempo de inactividad de 0 segundos donde los workers se reinician uno por uno, esperando que aparezca un nuevo worker antes de matar al anterior. Tambi\u00e9n puede verificar el estado, los registros y las m\u00e9tricas de las aplicaciones en ejecuci\u00f3n. Task Investiga los siguientes comandos y explica que salida por terminal nos ofrecen y para qu\u00e9 se utilizan: pm2 ls pm2 logs pm2 monit Warning Documenta la realizaci\u00f3n de toda esta pr\u00e1ctica adecuadamente, con las explicaciones y justificaciones necesarias, las respuestas a las preguntas planteadas y las capturas de pantalla pertinentes. Cuestiones Fij\u00e1os en las siguientes im\u00e1genes: La primera imagen ilustra los resultados de unas pruebas de carga sobre la aplicaci\u00f3n sin cl\u00faster y la segunda sobre la aplicaci\u00f3n clusterizada. \u00bfSabr\u00edas decir por qu\u00e9 en algunos casos concretos, como este, la aplicaci\u00f3n sin clusterizar tiene mejores resultados? Referencias How to install ExpressJS on Debian 11? Improving Node.js Application Performance With Clustering Evaluaci\u00f3n Criterio Puntuaci\u00f3n Pruebas correctas y bien documentadas de despliegue de la aplicaci\u00f3n sin cluster 2 puntos Pruebas correctas y bien documentadas de despliegue de la aplicaci\u00f3n con cluster 2 puntos Pruebas correctas y bien documentadas de todas las opciones con loadtest 2 puntos Pruebas correctas y bien documentadas de todas las opciones con PM2 2 puntos Respuestas correctas a las cuestiones 1 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 1 puntos","title":"Pr\u00e1ctica 3.3 - Despliegue de una aplicaci\u00f3n \"clusterizada\" con Node Express"},{"location":"P3.3-Cluster/#practica-33-despliegue-de-una-aplicacion-clusterizada-con-node-express","text":"","title":"Pr\u00e1ctica 3.3: Despliegue de una aplicaci\u00f3n \"clusterizada\" con Node Express"},{"location":"P3.3-Cluster/#introduccion","text":"Cuando se construye una aplicaci\u00f3n de producci\u00f3n, normalmente se busca la forma de optimizar su rendimiento llegando a una soluci\u00f3n de compromiso. En esta pr\u00e1ctica echaremos un vistazo a un enfoque que puede ofrecer una victoria r\u00e1pida cuando se trata de mejorar la manera en que las aplicaciones Node.js manejan la carga de trabajo. Una instancia de Node.js se ejecuta en un solo hilo, lo que significa que en un sistema multin\u00facleo (como la mayor\u00eda de los ordenadores de hoy en d\u00eda), no todos los n\u00facleos ser\u00e1n utilizados por la aplicaci\u00f3n. Para aprovechar los otros n\u00facleos disponibles, podemos lanzar un cluster de procesos Node.js y distribuir la carga entre ellos. Tener varios hilos para manejar las peticiones mejora el rendimiento (peticiones/segundo) del servidor, ya que varios clientes pueden ser atendidos simult\u00e1neamente. Veremos c\u00f3mo crear procesos hijos con el m\u00f3dulo de cluster de Node.js para, m\u00e1s tarde, ver c\u00f3mo gestionar el cluster con el gestor de procesos PM2.","title":"Introducci\u00f3n"},{"location":"P3.3-Cluster/#un-vistazo-rapido-a-los-clusters","text":"El m\u00f3dulo de cl\u00faster de Node.js permite la creaci\u00f3n de procesos secundarios ( workers ) que se ejecutan simult\u00e1neamente y comparten el mismo puerto de servidor. Cada hijo generado tiene su propio ciclo de eventos y memoria. Los procesos secundarios utilizan IPC (comunicaci\u00f3n entre procesos) para comunicarse con el proceso principal de Node.js. Tener m\u00faltiples procesos para manejar las solicitudes entrantes significa que se pueden procesar varias solicitudes simult\u00e1neamente y si hay una operaci\u00f3n de bloqueo/ejecuci\u00f3n prolongada en un worker , los otros workers pueden continuar administrando otras solicitudes entrantes; la aplicaci\u00f3n no se detendr\u00e1 hasta que finalice la operaci\u00f3n de bloqueo. La ejecuci\u00f3n de varios workers tambi\u00e9n permite actualizar la aplicaci\u00f3n en producci\u00f3n con poco o ning\u00fan tiempo de inactividad. Se pueden realizar cambios en la aplicaci\u00f3n y reiniciar los workers uno por uno, esperando que un proceso secundario se genere por completo antes de reiniciar otro. De esta manera, siempre habr\u00e1 workers ejecut\u00e1ndose mientras se produce la actualizaci\u00f3n. Las conexiones entrantes se distribuyen entre los procesos secundarios de dos maneras: El proceso maestro escucha las conexiones en un puerto y las distribuye entre los workers de forma rotatoria. Este es el enfoque por defecto en todas las plataformas, excepto Windows. El proceso maestro crea un socket de escucha y lo env\u00eda a los workers interesados \u200b\u200bque luego podr\u00e1n aceptar conexiones entrantes directamente.","title":"Un vistazo r\u00e1pido a los clusters"},{"location":"P3.3-Cluster/#usando-los-clusters","text":"","title":"Usando los clusters"},{"location":"P3.3-Cluster/#primero-sin-cluster","text":"Para ver las ventajas que ofrece la agrupaci\u00f3n en cl\u00fasteres, comenzaremos con una aplicaci\u00f3n de prueba en Node.js que no usa cl\u00fasteres y la compararemos con una que s\u00ed los usa, se trata de la siguiente: const express = require ( \"express\" ); const app = express (); const port = 3000 ; app . get ( \"/\" , ( req , res ) => { res . send ( \"Hello World!\" ); }); app . get ( \"/api/:n\" , function ( req , res ) { let n = parseInt ( req . params . n ); let count = 0 ; if ( n > 5000000000 ) n = 5000000000 ; for ( let i = 0 ; i <= n ; i ++ ) { count += i ; } res . send ( `Final count is ${ count } ` ); }); app . listen ( port , () => { console . log ( `App listening on port ${ port } ` ); }); Se trata de una aplicaci\u00f3n un tanto prefabricada en el sentido de que es algo que jam\u00e1s encontrar\u00edamos en el mundo real. No obstante, nos servir\u00e1 para ilustrar nuestro prop\u00f3sito. Esta aplicaci\u00f3n contiene dos rutas, una ruta ra\u00edz / que devuelve la cadena Hello World! y otra ruta /api/n donde se toma n como par\u00e1metro y va realizando una operaci\u00f3n de suma (el bucle for) cuyo resultado acumula en la variable count que se muestra al final. Si a este par\u00e1metro n , le damos un valor muy alto, nos permitir\u00e1 simular operaciones intensivas y de ejecuci\u00f3n prolongada en el servidor. Le damos como valor l\u00edmite 5000000000 para evitar una operaci\u00f3n demasiado costosa para nuestro ordenador. Task Deb\u00e9is conectaros al servidor Debian mediante SSH Deb\u00e9is crear un directorio para el proyecto de esta aplicaci\u00f3n DENTRO del directorio ejecutar\u00e9is 2 comandos: npm init para crear autom\u00e1ticamente la estructura de carpetas y el archivo package.json (Con ir d\u00e1ndole a <ENTER> a todas las preguntas, os basta) npm install express para instalar express para este proyecto Tras esto, DENTRO del directorio, ya pod\u00e9is iniciar la aplicaci\u00f3n con: node nombre_aplicacion.js Para comprobarlo, pod\u00e9is acceder a http://IP-maq-virtual:3000 o a http://IP-maq-virtual:3000/api/50 donde IP-maq-virtual es la IP del adaptador puente de vuestra Debian. Utilizada un valor de n relativamente peque\u00f1o, como el 50 del ejemplo anterior y comprobar\u00e9is que se ejecutar\u00e1 r\u00e1pidamente, devolviendo una respuesta casi inmediata. Hagamos otra simple comprobaci\u00f3n para valores de n m\u00e1s grandes. Desplegada e iniciada la aplicaci\u00f3n, acceded a la ruta http://IP-maq-virtual:3000/api/5000000000 . Mientras esta solicitud que tarda unos segundos se est\u00e1 procesando, acceded en otra pesta\u00f1a del navegador a http://IP-maq-virtual:3000 o a http://IP-maq-virtual:3000/api/n siendo n el valor que le quer\u00e1is dar. Utilizando las devoloper tools, podemos ver el tiempo que tardan en procesarse las solicitudes: La primera solicitud, al tener un valor de n grande, nos lleva unos cuantos segundos completarla. La segunda solicitud, pese a tener un valor de n que ya hab\u00edamos comprobado que ofrec\u00eda una respuesta casi inmediata, tambi\u00e9n se demora unos segundos. \u00bfPor qu\u00e9 ocurre esto? Porque el \u00fanico subproceso estar\u00e1 ocupado procesando la otra operaci\u00f3n de ejecuci\u00f3n prolongada. El \u00fanico n\u00facleo de la CPU tiene que completar la primera solicitud antes de que pueda encargarse de la otra.","title":"Primero sin cl\u00faster"},{"location":"P3.3-Cluster/#ahora-con-mas-cluster","text":"Ahora usaremos el m\u00f3dulo de cl\u00faster en la aplicaci\u00f3n para generar algunos procesos secundarios y ver c\u00f3mo eso mejora las cosas. A continuaci\u00f3n se muestra la aplicaci\u00f3n modificada: const express = require ( \"express\" ); const port = 3000 ; const cluster = require ( \"cluster\" ); const totalCPUs = require ( \"os\" ). cpus (). length ; if ( cluster . isMaster ) { console . log ( `Number of CPUs is ${ totalCPUs } ` ); console . log ( `Master ${ process . pid } is running` ); // Fork workers. for ( let i = 0 ; i < totalCPUs ; i ++ ) { cluster . fork (); } cluster . on ( \"exit\" , ( worker , code , signal ) => { console . log ( `worker ${ worker . process . pid } died` ); console . log ( \"Let's fork another worker!\" ); cluster . fork (); }); } else { const app = express (); console . log ( `Worker ${ process . pid } started` ); app . get ( \"/\" , ( req , res ) => { res . send ( \"Hello World!\" ); }); app . get ( \"/api/:n\" , function ( req , res ) { let n = parseInt ( req . params . n ); let count = 0 ; if ( n > 5000000000 ) n = 5000000000 ; for ( let i = 0 ; i <= n ; i ++ ) { count += i ; } res . send ( `Final count is ${ count } ` ); }); app . listen ( port , () => { console . log ( `App listening on port ${ port } ` ); }); } Esta aplicaci\u00f3n hace lo mismo que antes pero esta vez estamos generando varios procesos secundarios que compartir\u00e1n el puerto 3000 y que podr\u00e1n manejar las solicitudes enviadas a este puerto. Los procesos de trabajo se generan utilizando el m\u00e9todo child_process.fork() . El m\u00e9todo devuelve un objeto ChildProcess que tiene un canal de comunicaci\u00f3n incorporado que permite que los mensajes se transmitan entre el hijo y su padre. Creamos tantos procesos secundarios como n\u00facleos de CPU hay en la m\u00e1quina en la que se ejecuta la aplicaci\u00f3n. Se recomienda no crear m\u00e1s workers que n\u00facleos l\u00f3gicos en la computadora, ya que esto puede causar una sobrecarga en t\u00e9rminos de costos de programaci\u00f3n. Esto sucede porque el sistema tendr\u00e1 que programar todos los procesos creados para que se vayan ejecutando por turnos en los n\u00facleos. Los workers son creados y administrados por el proceso maestro. Cuando la aplicaci\u00f3n se ejecuta por primera vez, verificamos si es un proceso maestro con isMaster . Esto est\u00e1 determinado por la variable process.env.NODE_UNIQUE_ID . Si process.env.NODE_UNIQUE_ID tiene valor undefined , entonces isMaster ser\u00e1 true . Si el proceso es un maestro, llamamos a cluster.fork() para generar varios procesos. Registramos los ID de proceso maestro y worker . Cuando un proceso secundario muere, generamos uno nuevo para seguir utilizando los n\u00facleos de CPU disponibles. Ahora repetiremos el mismo experimento de antes, primero realizamos una solicitud al servidor con un valor alto n : Y ejecutamos r\u00e1pidamente otra solicitud en otra pesta\u00f1a del navegador, midiendo los tiempos de procesamiento de ambas: Comprobaremos que \u00e9stos se reducen dr\u00e1sticamente. Note Con varios workers disponibles para aceptar solicitudes, se mejoran tanto la disponibilidad del servidor como el rendimiento. Ejecutar una solicitud en una pesta\u00f1a del navegador y ejecutar r\u00e1pidamente otra en una segunda pesta\u00f1a sirve para mostrarnos la mejora que ofrece la agrupaci\u00f3n en cl\u00fasteres para nuestro ejemplo de una forma m\u00e1s o menos r\u00e1pida, pero es un m\u00e9todo un tanto \"chapucero\" y no es una forma adecuada o confiable de determinar las mejoras de rendimiento. En el siguiente apartado echaremos un vistazo a algunos puntos de referencia que demostrar\u00e1n mejor cu\u00e1nto ha mejorado la agrupaci\u00f3n en cl\u00fasteres nuestra aplicaci\u00f3n.","title":"\u00a1Ahora con m\u00e1s cl\u00faster!"},{"location":"P3.3-Cluster/#metricas-de-rendimiento","text":"Realizaremos una prueba de carga en nuestras dos aplicaciones para ver c\u00f3mo cada una maneja una gran cantidad de conexiones entrantes. Usaremos el paquete loadtest para esto. El paquete loadtest nos permite simular una gran cantidad de conexiones simult\u00e1neas a nuestra API para que podamos medir su rendimiento. Para usar loadtest , primero debemos instalarlo globalmente. Tras conectaros por SSH al servidor Debian: npm install -g loadtest Luego ejecutamos la aplicaci\u00f3n que queremos probar ( node nombre_aplicacion.js ). Comenzaremos probando la versi\u00f3n que no utiliza la agrupaci\u00f3n en cl\u00fasteres. Mientras ejecutamos la aplicaci\u00f3n, en otro terminal realizamos la siguiente prueba de carga: loadtest http://localhost:3000/api/500000 -n 1000 -c 100 El comando anterior enviar\u00e1 1000 solicitudes a la URL dada, de las cuales 100 son concurrentes. El siguiente es el resultado de ejecutar el comando anterior: Vemos que con la misma solicitud (con n= 500000) el servidor ha podido manejar 404 solicitudes por segundo con una latencia media de 232.4 milisegundos (el tiempo promedio que tarda en completar una sola solicitud). Intent\u00e9moslo de nuevo, pero esta vez con m\u00e1s solicitudes (y sin cl\u00fasteres): Vemos que las m\u00e9tricas arrojan resultados a\u00fan peores. Ahora detenemos nuestra aplicaci\u00f3n sin cl\u00fasters y ejecutamos la que s\u00ed los tiene ( node nombre_aplicacion_cluster.js ). Ejecutaremos exactamente las mismas pruebas con el objetivo de realizar una comparaci\u00f3n: Es obvio que los cl\u00fasters permiten manejar una mayor cantidad de peticiones por segundo con una menor latencia.","title":"M\u00e9tricas de rendimiento"},{"location":"P3.3-Cluster/#uso-de-pm2-para-administrar-un-cluster-de-nodejs","text":"En nuestra aplicaci\u00f3n, hemos usado el m\u00f3dulo cluster de Node.js para crear y administrar manualmente los procesos. Primero hemos determinado la cantidad de workers (usando la cantidad de n\u00facleos de CPU como referencia), luego los hemos generado y, finalmente, escuchamos si hay workers muertos para poder generar nuevos. En nuestra aplicaci\u00f3n de ejemplo muy sencilla, tuvimos que escribir una cantidad considerable de c\u00f3digo solo para administraci\u00f3n la agrupaci\u00f3n en cl\u00fasteres. En una aplicaci\u00f3n de producci\u00f3n es bastante probable que se deba escribir a\u00fan m\u00e1s c\u00f3digo. Existe una herramienta que nos puede ayudar a administrar todo esto un poco mejor: el administrador de procesos PM2 . PM2 es un administrador de procesos de producci\u00f3n para aplicaciones Node.js con un balanceador de carga incorporado. Cuando est\u00e1 configurado correctamente, PM2 ejecuta autom\u00e1ticamente la aplicaci\u00f3n en modo de cl\u00faster, generando workers y se encarga de generar nuevos workers cuando uno de ellos muera. PM2 facilita la parada, eliminaci\u00f3n e inicio de procesos, adem\u00e1s de disponer de algunas herramientas de monitorizaci\u00f3n que pueden ayudarnos a monitorizar y ajustar el rendimiento de su aplicaci\u00f3n. Para usar PM2 , primero instalamos globalmente en nuestra Debian: npm install pm2 -g Vamos a utilizarlo con nuestra primera aplicaci\u00f3n, la que no estab \"clusterizada\" en el c\u00f3digo. Para ello ejecutaremos el siguiente comando: pm2 start nombre_aplicacion_sin_cluster.js -i 0 Donde: -i le indicar\u00e1 a PM2 que inicie la aplicaci\u00f3n en cluster_mode (a diferencia de fork_mode ). Si se establece a 0, PM2 generar\u00e1 autom\u00e1ticamente tantos workers como n\u00facleos de CPU haya. Y as\u00ed, nuestra aplicaci\u00f3n se ejecuta en modo de cl\u00faster, sin necesidad de cambios de c\u00f3digo. Task Ejecuta y documenta con capturas de pantallas, las mismas pruebas que antes pero utilizando PM2 y comprueba si se obtienen los mismos resultados. Por detr\u00e1s, PM2 tambi\u00e9n utiliza el m\u00f3dulo cluster de Node.js, as\u00ed como otras herramientas que facilitan la gesti\u00f3n de procesos. En el Terminal, obtendremos una tabla que muestra algunos detalles de los procesos generados: Podemos detener la aplicaci\u00f3n con el siguiente comando: pm2 stop app.js La aplicaci\u00f3n se desconectar\u00e1 y la salida por terminal mostrar\u00e1 todos los procesos con un estado stopped . En vez de tener pasar siempre las configuraciones cuando ejecuta la aplicaci\u00f3n con pm2 start app.js -i 0 , podr\u00edamos facilitarnos la tarea y guardarlas en un archivo de configuraci\u00f3n separado, llamado Ecosystem . Este archivo tambi\u00e9n nos permite establecer configuraciones espec\u00edficas para diferentes aplicaciones. Crearemos el archivo Ecosystem con el siguiente comando: Que generar\u00e1 un archivo llamado ecosystem.config.js . Para el caso concreto de nuestra aplicaci\u00f3n, necesitamos modificarlo como se muestra a continuaci\u00f3n: module.expor ts = { apps : [ { na me : \"nombre_aplicacion\" , scrip t : \"nombre_aplicacion_sin_cluster.js\" , i nstan ces : 0 , exec_mode : \"cluster\" , }, ], } ; Al configurar exec_mode con el valor cluster , le indica a PM2 que balancee la carga entre cada instancia. instances est\u00e1 configurado a 0 como antes, lo que generar\u00e1 tantos workers como n\u00facleos de CPU. La opci\u00f3n -i o instances se puede establecer con los siguientes valores: 0 o max (en desuso) para \"repartir\" la aplicaci\u00f3n entre todas las CPU -1 para \"repartir\" la aplicaci\u00f3n en todas las CPU - 1 n\u00famero para difundir la aplicaci\u00f3n a trav\u00e9s de un n\u00famero concreto de CPU Ahora podemos ejecutar la aplicaci\u00f3n con: pm2 start ecosystem.config.js La aplicaci\u00f3n se ejecutar\u00e1 en modo cl\u00faster, exactamente como antes. Podremos iniciar, reiniciar, recargar, detener y eliminar una aplicaci\u00f3n con los siguientes comandos, respectivamente: $ pm2 start nombre_aplicacion $ pm2 restart nombre_aplicacion $ pm2 reload nombre_aplicacion $ pm2 stop nombre_aplicacion $ pm2 delete nombre_aplicacion # Cuando usemos el archivo Ecosystem: $ pm2 [ start | restart | reload | stop | delete ] ecosystem.config.js El comando restart elimina y reinicia inmediatamente los procesos, mientras que el comando reload logra un tiempo de inactividad de 0 segundos donde los workers se reinician uno por uno, esperando que aparezca un nuevo worker antes de matar al anterior. Tambi\u00e9n puede verificar el estado, los registros y las m\u00e9tricas de las aplicaciones en ejecuci\u00f3n. Task Investiga los siguientes comandos y explica que salida por terminal nos ofrecen y para qu\u00e9 se utilizan: pm2 ls pm2 logs pm2 monit Warning Documenta la realizaci\u00f3n de toda esta pr\u00e1ctica adecuadamente, con las explicaciones y justificaciones necesarias, las respuestas a las preguntas planteadas y las capturas de pantalla pertinentes.","title":"Uso de PM2 para administrar un cl\u00faster de Node.js"},{"location":"P3.3-Cluster/#cuestiones","text":"Fij\u00e1os en las siguientes im\u00e1genes: La primera imagen ilustra los resultados de unas pruebas de carga sobre la aplicaci\u00f3n sin cl\u00faster y la segunda sobre la aplicaci\u00f3n clusterizada. \u00bfSabr\u00edas decir por qu\u00e9 en algunos casos concretos, como este, la aplicaci\u00f3n sin clusterizar tiene mejores resultados?","title":"Cuestiones"},{"location":"P3.3-Cluster/#referencias","text":"How to install ExpressJS on Debian 11? Improving Node.js Application Performance With Clustering","title":"Referencias"},{"location":"P3.3-Cluster/#evaluacion","text":"Criterio Puntuaci\u00f3n Pruebas correctas y bien documentadas de despliegue de la aplicaci\u00f3n sin cluster 2 puntos Pruebas correctas y bien documentadas de despliegue de la aplicaci\u00f3n con cluster 2 puntos Pruebas correctas y bien documentadas de todas las opciones con loadtest 2 puntos Pruebas correctas y bien documentadas de todas las opciones con PM2 2 puntos Respuestas correctas a las cuestiones 1 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 1 puntos","title":"Evaluaci\u00f3n"},{"location":"P3.4-Heroku/","text":"Pr\u00e1ctica 3.4: Despliegue de una aplicaci\u00f3n Node.js en Heroku (PaaS) y una aplicaci\u00f3n React en Netlify (PaaS) Introducci\u00f3n En la pr\u00e1ctica anterior hemos visto c\u00f3mo desplegar una aplicaci\u00f3n de Node.js sobre un servidor Express en local (en nuestro propio servidor Debian). La pr\u00e1ctica anterior podr\u00eda asemejarse a las pruebas que realiza un desarrollador antes de pasar su aplicaci\u00f3n al entorno de producci\u00f3n. Ya sabemos que entendemos el despliegue o deployment como el proceso de mover nuestro c\u00f3digo t\u00edpicamente de un sistema de control de versiones a una plataforma de hosting donde se aloja y es servida a los usuarios finales. A la hora de desplegar la aplicaci\u00f3n en producci\u00f3n, podr\u00eda utilizarse el m\u00e9todo de copiar los archivos al servidor concreto v\u00eda el vetusto FTP, SSH u otros y desplegarla para dejarla funcionando. No obstante, esta pr\u00e1ctica se acerca m\u00e1s a la realidad ya que utilizaremos un repositorio de Github y una plataforma de PaaS (Platform as a Service) como Heroku o Netlify para desplegar adecuadamente nuestra aplicaci\u00f3n en producci\u00f3n. \u00bfQu\u00e9 es Github? A pesar de que trataremos un poco m\u00e1s en profundidad Github en el siguiente tema, daremos una breve explicaci\u00f3n aqu\u00ed. GitHub es un servicio basado en la nube que aloja un sistema de control de versiones (VCS) llamado Git. \u00c9ste permite a los desarrolladores colaborar y realizar cambios en proyectos compartidos, a la vez que mantienen un seguimiento detallado de su progreso. El control de versiones es un sistema que ayuda a rastrear y gestionar los cambios realizados en un archivo o conjunto de archivos. Utilizado principalmente por ingenieros de software para hacer un seguimiento de las modificaciones realizadas en el c\u00f3digo fuente, el sistema de control de versiones les permite analizar todos los cambios y revertirlos sin repercusiones si se comete un error. \u00bfQu\u00e9 es Heroku? Heroku es una soluci\u00f3n de Plataforma como Servicio (PaaS) basada en la nube para que el cliente solo se preocupe de desarrollar su aplicaci\u00f3n mientras Heroku se encarga de la infraestructura que hay detr\u00e1s. Para proporcionar este servicio se dispone de unos contenedores virtuales que son los encargados de mantener y ejecutar las aplicaciones. Estos contenedores virtuales son totalmente escalables bajo demanda. Tanto en n\u00famero como en capacidades. Una ventaja de elegir Heroku es su capacidad de soportar m\u00faltiples lenguajes de programaci\u00f3n. Los principales a utilizar son: Node.js, Ruby, Python, Java, PHP, Go, Scala y Clojure. Aunque esta cantidad de lenguajes puede aumentar en el caso de utilizar Heroku Buildpacks, que permiten compilar las aplicaciones en multitud de ellos m\u00e1s. Note Tanto Github , como Heroku , como Netlify pueden ser controlados desde el terminal de nuestro Linux, por lo que seguiremos el procedimiento de contectarnos v\u00eda SSH a nuestro Debian y realizar las operaciones por terminal. \u00bfQu\u00e9 es Netlify? Netlify es un proveedor de alojamiento en la nube que proporciona servicios de backend sin servidor ( serverless ) para sitios web est\u00e1ticos. Est\u00e1 dise\u00f1ado para maximizar la productividad en el sentido de que permite a los desarrolladores (especialmente orientados al frontend), y a los ingenieros construir, probar y desplegar r\u00e1pidamente sitios web/aplicaciones. Funciona conect\u00e1ndose a un repositorio de GitHub, de donde extrae el c\u00f3digo fuente. A continuaci\u00f3n, ejecutar\u00e1 un proceso de construcci\u00f3n para pre-renderizar las p\u00e1ginas de nuestro sitio web/aplicaci\u00f3n en archivos est\u00e1ticos. Hay numerosas razones a favor de usar Netlify, aqu\u00ed est\u00e1n algunas de ellas: Netlify hace que sea incre\u00edblemente sencillo desplegar un sitio web - de hecho, la forma m\u00e1s sencilla de lograrlo es utilizar GitHub, GitLab o Bitbucket para configurar el despliegue continuo. Netlify hace que sea s\u00faper f\u00e1cil lanzar un sitio web con su soluci\u00f3n de gesti\u00f3n de DNS incorporada. Podr\u00edamos desplegar f\u00e1cilmente s\u00f3lo una rama espec\u00edfica de nuestro proyecto Git - esto es \u00fatil para probar nuevas caracter\u00edsticas que pueden o no llegar a la rama maestra/principal, o para determinar r\u00e1pidamente c\u00f3mo un PR (Pull Request) afectar\u00e1 a su sitio. Netlify te permite previsualizar cualquier despliegue que hagas o quieras hacer - esto te permite a ti y a tu equipo ver c\u00f3mo se ver\u00e1n los cambios en producci\u00f3n sin tener que desplegarlos en tu sitio existente. Netlify proporciona una pr\u00e1ctica funci\u00f3n de env\u00edo de formularios que nos permite recoger informaci\u00f3n de los usuarios. Creaci\u00f3n de nuestra aplicaci\u00f3n para Heroku Tras loguearnos por SSH en nuestro Debian, nos crearemos un directorio para albergar la aplicac\u00f3n con el nombre que queramos. En ese directorio, crearemos los 3 archivos (dos .html y un .js )que conformar\u00e1n nuestra sencilla aplicaci\u00f3n de ejemplo: head.html tail.html aplicacion.js <!DOCTYPE html> < html > < head > < title > Hola Mundo </ title > </ head > < body > < h1 > Esta es la pagina principal </ h1 > < p >< a href = \"/tailPage\" > Ir a la siguiente pagina </ a ></ p > </ body > <!DOCTYPE html> < html > < head > < title > Hola Mundo </ title > </ head > < body > < h1 > FUNCIONA </ h1 > </ body > var http = require ( 'http' ); var fs = require ( 'fs' ); // para obtener los datos del archivo html var port = process . env . PORT || 8080 ; //Para que funcione en Heroku ya que da error 137 con el puerto 3000 http . createServer ( function ( req , res ) { res . writeHead ( 200 , { 'Content-Type' : 'text/html' }); // req.url almacena el path o ruta de la URL var url = req . url ; if ( url === \"/\" ) { // fs.readFile busca el archivo HTML // el primer par\u00e1metro es el path al archivo HTML // y el segundo es el callback de la funci\u00f3n // si el archivo no se encuentra, la funci\u00f3n devuelve un error // si el archivo se encuentra, el contenido del mismo se encuentra en pgres fs . readFile ( \"head.html\" , function ( err , pgres ) { if ( err ) res . write ( \"HEAD.HTML NOT FOUND\" ); else { // Las siguientes 3 lineas // tienen la funci\u00f3n de enviar el archivo html // y finalizar el proceso de respuesta res . writeHead ( 200 , { 'Content-Type' : 'text/html' }); res . write ( pgres ); res . end (); } }); } else if ( url === \"/tailPage\" ) { fs . readFile ( \"tail.html\" , function ( err , pgres ) { if ( err ) res . write ( \"TAIL.HTML NOT FOUND\" ); else { res . writeHead ( 200 , { 'Content-Type' : 'text/html' }); res . write ( pgres ); res . end (); } }); } }). listen ( port , function () { console . log ( \"SERVER STARTED PORT: 8080\" ); }); Ahora, tal y como hacemos siempre a la hora de crear nuestra aplicaci\u00f3n Node.js , con el fin de crear el archivo package.json , utilizaremos en el terminal el comando: npm init Podemos probar que nuestra aplicaci\u00f3n funciona perfectamente en local: node aplicacion.js Y tras ello, debemos poder acceder, desde nuestra m\u00e1quina anfitriona a http://IP-maq-virtual:8080 Ya con la aplicaci\u00f3n creada y comprobada, podremos desplegarla en m\u00faltiples plataformas en la nube, como AWS, GCP, Azure, Digital Ocean, Heroku... \u00a1Ojo! Para que nos funcione en Heroku, en el archivo package.json que se nos ha creado al hacer el npm init debemos hacerle una modificaci\u00f3n. En el bloque scripts, debemos borrar lo que haya dentro y dejar \u00fanicamente dentro de \u00e9l : \"start\" : \"node aplicacion.js\" De forma que Heroku sepa que comando utilizar para iniciar la aplicaci\u00f3n tras desplegarla. Proceso de despliegue en Heroku Para trabajar con Heroku desde nuestro terminal, debemos instalar el propio CLI de Heroku. Consultando la documentaci\u00f3n , vemos que hemos de ejecutar: curl https://cli-assets.heroku.com/install.sh | sh Y comprobamos que se ha instalado correctamente consultando su versi\u00f3n: heroku -v Lo siguiente ser\u00e1 loguearnos en nuestra cuenta de Heroku mediante el terminal, para ello: heroku login Esto en teor\u00eda nos abre una pesta\u00f1a del navegador para loguearnos en nuestra cuenta. Puesto que estamos conectados por SSH a nuestra Debian, no suceder\u00e1 esto ya que el \u00fanico puerto por el que nos comunicamos es por el 22. Necesitar\u00edamos un t\u00fanel SSH para redirigir los puertos de la m\u00e1quina Debian remota a la nuestra y que nos abriese el navegador en nuestra m\u00e1quina. Puesto que esto escapa de los objetivos del m\u00f3dulo y con el fin de agilizar el proceso, simplemente copiaremos la URL y la pegaremos en nuestro navegador para loguearnos. Antes de continuar, conviene asegurarnos de que tenemos la \u00faltima versi\u00f3n de git en nuestra Debian: sudo apt-get update && sudo apt-get install git Ahora, dentro del directorio que hab\u00edamos creado previamente para nuestra aplicaci\u00f3n, se trata de seguir unos sencillos pasos: Tip Aqu\u00ed aparece explicado con lenguaje llano m\u00e1s adelante en el m\u00f3dulo ya hablaremos con mayor propiedad de estas acciones con git Nos aseguramos de que nuestro directorio no es a\u00fan un repositorio: git status Y lo iniciamos: git init Ahora a\u00f1adimos todos los archivos presentes en el directorio ( . ) para ser enviados al repositorio: git add . Y los preparamos para que sean envidos al repositorio: git commit -m \"Comentario explicativo del commit\" Creamos nuestra aplicaci\u00f3n en Heroku: heroku create Esto crear\u00e1 un git remoto que conectar\u00e1 con nuestro repositorio git local Desplegamos nuestra aplicaci\u00f3n en el server de Heroku : git push heroku master Y comprobamos que la instancia est\u00e1 corriendo: heroku ps:scale web=1 El comando heroku open abrir\u00eda nuestra aplicaci\u00f3n en el navegador. Sin embargo, por el problema explicado antes de estar conectados por SSH, esto no ocurrir\u00e1. No obstante, podemos acceder a nuestra aplicaci\u00f3n de otra forma r\u00e1pida y sencilla desde nuestro dashboard de Heroku: Localizamos nuestra aplicaci\u00f3n: Y tras hacer click en ella, localizamos el bot\u00f3n que nos permite abrirla y volvemos a hacer click: Comprobando que nuestra aplicaci\u00f3n, efectivametne se ha desplegado en Heroku y funciona a la perfecci\u00f3n: Aplicaci\u00f3n para Netlify Puesto que el inter\u00e9s en este m\u00f3dulo radica en el proceso de despliegue, suponiendo que la parte de desarrollo ya es abordada en otros m\u00f3dulos, vamos a utilizar una aplicaci\u00f3n de ejemplo que nos ahorre tiempo para centrarnos en el despliegue. Nos clonaremos este repositorio: git clone https://github.com/StackAbuse/color-shades-generator Proceso de despliegue en Netlify Por mera curiosidad y ambici\u00f3n de aprendizaje, vamos a ver dos m\u00e9todos de despliegue en Netlify: Despliegue manual desde el CLI de Netlify, es decir, desde el terminal, a partir de un directorio local de nuestra m\u00e1quina. Despliegue desde un c\u00f3digo publicado en uno de nuestros repositorios de Github El primero nos permitir\u00e1 conocer el CLI de Netlify y el segundo nos acercara m\u00e1s a una experiencia real de despliegue. Task Vuestra primera tarea ser\u00e1 registraros en Netlify con vuestro email (no con vuestra cuenta de Github) y decirle que no cuando os pida enlazar con vuestra cuenta de Github (lo haremos m\u00e1s adelante). Despliegue mediante CLI Una vez registrados, debemos instalar el CLI de Netlify para ejecutar sus comandos desde el terminal: sudo npm install netlify-cli -g Est\u00e1 claro que para realizar acciones de deploy, Netlify nos solicitar\u00e1 una autenticaci\u00f3n, esto se hace mediante el comando: netlify login El cual nos muestra una pantalla del navegador para que concedamos la autorizaci\u00f3n pertinente. Sin embargo, recordemos el problema de que estamos conectados por SSH a nuestro servidor y no tenemos la posibilidad del uso de un entorno gr\u00e1fico. En este caso, siguiendo las instrucciones de la documentaci\u00f3n : Generamos el token de acceso Lo establecemos como variable de ambiente: Y nos logueamos netlify login Bueno, tenemos el c\u00f3digo de nuestra aplicaci\u00f3n, tenemos nuestra cuenta en Netlify y tenemos el CLI necesario para ejecutar comandos desde el terminal en esa cuenta... \u00bfPodemos proceder al despliegue sin mayores complicaciones? La respuesta es NO , como buenos desarrolladores y en base a experiencias anteriores, ya sab\u00e9is que hay que hacer un build de la aplicaci\u00f3n para, posteriormente, desplegarla. Vamos a ello. En primer lugar, como sabemos, debemos instalar todas las dependencias que vienen indicadas en el archivo package.json : npm install Y cuando ya las tengamos instaladas podemos proceder a realizar el build: npm run build Esto nos crear\u00e1 una nueva carpeta llamada build que contendr\u00e1 la aplicaci\u00f3n que debemos desplegar. Y ya podemos hacer un pre-deploy de la aplicaci\u00f3n de la que hemos hecho build antes: netlify deploy Nos har\u00e1 algunas preguntas para el desplieuge: Indicamos que queremos crear y configurar un nuevo site El Team lo dejamos por defecto Le indicamos el nombre que queremos emplear para la web ( nombre-practica3-4 ) y el directorio a utilizar para el deploy (directorio ./build ). Y si nos indica que todo ha ido bien e incluso podemos ver el \"borrador\" (Website Draft URL) de la web que nos aporta, podemos pasarla a producci\u00f3n finalmente tal y como nos indica la misma salida del comando: If everything looks good on your draft URL, deploy it to your main site URL with the --prod flag. netlify deploy --prod Warning No olvides desplegar finalmente en producci\u00f3n y comprobar que puedes acceder a la URL. Despliegue mediante conexi\u00f3n con Github En primer lugar, vamos a eliminar el site que hemos desplegado antes en Netlify para evitarnos cualquier problema y/o conflicto: En segundo lugar, vamos a borrar el directorio donde se halla el repositorio clonado en el paso anterior para as\u00ed poder empezar de 0: rm -rf directorio_repositorio Como queremos simular que hemos picado el c\u00f3digo a man o en local y lo vamos a subir a Github por primera vez, nos descargaremos los fuentes en formato .zip sin que tenga ninguna referencia a Github: wget https://github.com/StackAbuse/color-shades-generator/archive/refs/heads/main.zip Creamos una carpeta nueva y descomprimimos dentro el zip: mkdir practica3.4 unzip main.zip -d practica3.4/ Entramos en la carpeta donde est\u00e1 el c\u00f3digo: cd practica3.4/color-shades-generator-main/ Ahora debemos crear un repositorio completamente vac\u00edo en Github que se llame practicaTresCuatro : Y tras ello, volviendo al terminal a la carpeta donde est\u00e1bamos, la iniciamos como repositorio, a\u00f1adimos todo el contenido de la misma para el commit, hacemos el commit con el mensaje correspondiente y creamos la rama main: $ git init $ git add . $ git commit -m \"Subiendo el c\u00f3digo...\" $ git branch -M main Y ahora s\u00f3lo queda referenciar nuestra carpeta al repositorio reci\u00e9n creado en Github y hacer un push para subir todo el contenido del commit a \u00e9l: $ git remote add origin https://github.com/username/practicaTresCuatro.git $ git push -u origin main Ahora que ya tenemos subido el c\u00f3digo a GitHub, de alguna manera debemos enganchar o enlazar nuestra cuenta de Github con la de Netlify para que \u00e9ste \u00faltimo pueda traerse el c\u00f3digo de all\u00ed, hacer el build y desplegarlo. As\u00ed pues, entramos en nuestro dashboard de Netlify y le damos a importar proyecto existente de git : Le indicamos que concretamente de Github: Y nos saltar\u00e1 una ventana pidiendo que autoricemos a Netlify a acceder a nuestros repositorios de Github: Y luego le indicaremos que no acceda a todos nuestros repositorios sino s\u00f3lo al repositorio que necesitamos, que es donde tenemos el c\u00f3digo de nuestra aplicaci\u00f3n: Y ya quedar\u00e1 todo listo: Y desplegamos la aplicaci\u00f3n: Netlify se encargar\u00e1 de hacer el build de forma autom\u00e1tica tal y como hemos visto en la imagen de arriba, con el comando npm run build , publicando el contenido del directorio build . Atenci\u00f3n Tras el deploy, en \"Site settings\" pode\u00eds y deb\u00e9is cambiar el nombre de la aplicaci\u00f3n por nombre-practica3-4, donde nombre es vuestro nombre. Lo que hemos conseguido de esta forma es que, cualquier cambio que hagamos en el proyecto y del que hagamos commit y push en Github, autom\u00e1ticamente genere un nuevo despliegue en Netlify. Es el principio de lo que m\u00e1s adelante veremos como despliegue continuo . Comprobemos que realmente es as\u00ed: Dentro de la carpeta public encontramos el archivo robots.txt , cuyo cometido es indicar a los rastreadores de los buscadores a qu\u00e9 URLs del sitio pueden acceder. A este archivo se puede acceder a trav\u00e9s de la URL del site: Dentro de la carpeta public , utilizando el editor de texto que prefir\u00e1is en vuestro terminal, modificad el archivo robots.txt para que excluya un directorio que se llame nombre_apellido , utilizando obviamente vuestro nombre y apellido. User-agent: * Disallow: /nombre_y_apellido/ Haz un nuevo commit y push (del caso anterior, recuerda el commando git previo para a\u00f1adir los archivos a hacer commit) Comprueba en el dashboard de Netlify que se ha producido un nuevo deploy de la aplicaci\u00f3n hace escasos segundos Accede a https://url_de_la_aplicacion/robots.txt y comprueba que, efectivamente, se ve reflejado el cambio Cuestiones Investiga y explica que es un Dyno en terminolog\u00eda Heroku. En Heroku no todo es de color de rosa, tiene sus limitaciones y desventajas. Busca, investiga y explica algunas de ellas detalladamente. Task Documenta la realizaci\u00f3n de toda esta pr\u00e1ctica adecuadamente, con las explicaciones y justificaciones necesarias y las capturas de pantalla pertinentes. Referencias \u00bfQu\u00e9 es Github? \u00bfQu\u00e9 es Heroku? Deploying Node.js applications List of all limitations in Heroku platform How to deploy your website to Netlify for free 4 Ways To Deploy Your Static Site with Netlify Guide to Deploying a React App to Netlify Evaluaci\u00f3n Criterio Puntuaci\u00f3n Despliegue correcto y bien documentado en Heroku 2 puntos Despliegue correcto y bien documentado en Netlify mediante CLI 0.75 puntos Despliegue correcto y bien documentado en Netlify de forma manual desde el dashboard 2 puntos Cambio de nombre del site 0.25 puntos Comprobaci\u00f3n correcta y bien documentada de despliegue autom\u00e1tico al hacer push en Github 3 puntos Respuestas correctas a las cuestiones 1 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 1 puntos","title":"Pr\u00e1ctica 3.4 - Despliegue de una aplicaci\u00f3n Node.js en Heroku (PaaS) y Netlify (Paas)"},{"location":"P3.4-Heroku/#practica-34-despliegue-de-una-aplicacion-nodejs-en-heroku-paas-y-una-aplicacion-react-en-netlify-paas","text":"","title":"Pr\u00e1ctica 3.4: Despliegue de una aplicaci\u00f3n Node.js en Heroku (PaaS) y una aplicaci\u00f3n React en Netlify (PaaS)"},{"location":"P3.4-Heroku/#introduccion","text":"En la pr\u00e1ctica anterior hemos visto c\u00f3mo desplegar una aplicaci\u00f3n de Node.js sobre un servidor Express en local (en nuestro propio servidor Debian). La pr\u00e1ctica anterior podr\u00eda asemejarse a las pruebas que realiza un desarrollador antes de pasar su aplicaci\u00f3n al entorno de producci\u00f3n. Ya sabemos que entendemos el despliegue o deployment como el proceso de mover nuestro c\u00f3digo t\u00edpicamente de un sistema de control de versiones a una plataforma de hosting donde se aloja y es servida a los usuarios finales. A la hora de desplegar la aplicaci\u00f3n en producci\u00f3n, podr\u00eda utilizarse el m\u00e9todo de copiar los archivos al servidor concreto v\u00eda el vetusto FTP, SSH u otros y desplegarla para dejarla funcionando. No obstante, esta pr\u00e1ctica se acerca m\u00e1s a la realidad ya que utilizaremos un repositorio de Github y una plataforma de PaaS (Platform as a Service) como Heroku o Netlify para desplegar adecuadamente nuestra aplicaci\u00f3n en producci\u00f3n.","title":"Introducci\u00f3n"},{"location":"P3.4-Heroku/#que-es-github","text":"A pesar de que trataremos un poco m\u00e1s en profundidad Github en el siguiente tema, daremos una breve explicaci\u00f3n aqu\u00ed. GitHub es un servicio basado en la nube que aloja un sistema de control de versiones (VCS) llamado Git. \u00c9ste permite a los desarrolladores colaborar y realizar cambios en proyectos compartidos, a la vez que mantienen un seguimiento detallado de su progreso. El control de versiones es un sistema que ayuda a rastrear y gestionar los cambios realizados en un archivo o conjunto de archivos. Utilizado principalmente por ingenieros de software para hacer un seguimiento de las modificaciones realizadas en el c\u00f3digo fuente, el sistema de control de versiones les permite analizar todos los cambios y revertirlos sin repercusiones si se comete un error.","title":"\u00bfQu\u00e9 es Github?"},{"location":"P3.4-Heroku/#que-es-heroku","text":"Heroku es una soluci\u00f3n de Plataforma como Servicio (PaaS) basada en la nube para que el cliente solo se preocupe de desarrollar su aplicaci\u00f3n mientras Heroku se encarga de la infraestructura que hay detr\u00e1s. Para proporcionar este servicio se dispone de unos contenedores virtuales que son los encargados de mantener y ejecutar las aplicaciones. Estos contenedores virtuales son totalmente escalables bajo demanda. Tanto en n\u00famero como en capacidades. Una ventaja de elegir Heroku es su capacidad de soportar m\u00faltiples lenguajes de programaci\u00f3n. Los principales a utilizar son: Node.js, Ruby, Python, Java, PHP, Go, Scala y Clojure. Aunque esta cantidad de lenguajes puede aumentar en el caso de utilizar Heroku Buildpacks, que permiten compilar las aplicaciones en multitud de ellos m\u00e1s. Note Tanto Github , como Heroku , como Netlify pueden ser controlados desde el terminal de nuestro Linux, por lo que seguiremos el procedimiento de contectarnos v\u00eda SSH a nuestro Debian y realizar las operaciones por terminal.","title":"\u00bfQu\u00e9 es Heroku?"},{"location":"P3.4-Heroku/#que-es-netlify","text":"Netlify es un proveedor de alojamiento en la nube que proporciona servicios de backend sin servidor ( serverless ) para sitios web est\u00e1ticos. Est\u00e1 dise\u00f1ado para maximizar la productividad en el sentido de que permite a los desarrolladores (especialmente orientados al frontend), y a los ingenieros construir, probar y desplegar r\u00e1pidamente sitios web/aplicaciones. Funciona conect\u00e1ndose a un repositorio de GitHub, de donde extrae el c\u00f3digo fuente. A continuaci\u00f3n, ejecutar\u00e1 un proceso de construcci\u00f3n para pre-renderizar las p\u00e1ginas de nuestro sitio web/aplicaci\u00f3n en archivos est\u00e1ticos. Hay numerosas razones a favor de usar Netlify, aqu\u00ed est\u00e1n algunas de ellas: Netlify hace que sea incre\u00edblemente sencillo desplegar un sitio web - de hecho, la forma m\u00e1s sencilla de lograrlo es utilizar GitHub, GitLab o Bitbucket para configurar el despliegue continuo. Netlify hace que sea s\u00faper f\u00e1cil lanzar un sitio web con su soluci\u00f3n de gesti\u00f3n de DNS incorporada. Podr\u00edamos desplegar f\u00e1cilmente s\u00f3lo una rama espec\u00edfica de nuestro proyecto Git - esto es \u00fatil para probar nuevas caracter\u00edsticas que pueden o no llegar a la rama maestra/principal, o para determinar r\u00e1pidamente c\u00f3mo un PR (Pull Request) afectar\u00e1 a su sitio. Netlify te permite previsualizar cualquier despliegue que hagas o quieras hacer - esto te permite a ti y a tu equipo ver c\u00f3mo se ver\u00e1n los cambios en producci\u00f3n sin tener que desplegarlos en tu sitio existente. Netlify proporciona una pr\u00e1ctica funci\u00f3n de env\u00edo de formularios que nos permite recoger informaci\u00f3n de los usuarios.","title":"\u00bfQu\u00e9 es Netlify?"},{"location":"P3.4-Heroku/#creacion-de-nuestra-aplicacion-para-heroku","text":"Tras loguearnos por SSH en nuestro Debian, nos crearemos un directorio para albergar la aplicac\u00f3n con el nombre que queramos. En ese directorio, crearemos los 3 archivos (dos .html y un .js )que conformar\u00e1n nuestra sencilla aplicaci\u00f3n de ejemplo: head.html tail.html aplicacion.js <!DOCTYPE html> < html > < head > < title > Hola Mundo </ title > </ head > < body > < h1 > Esta es la pagina principal </ h1 > < p >< a href = \"/tailPage\" > Ir a la siguiente pagina </ a ></ p > </ body > <!DOCTYPE html> < html > < head > < title > Hola Mundo </ title > </ head > < body > < h1 > FUNCIONA </ h1 > </ body > var http = require ( 'http' ); var fs = require ( 'fs' ); // para obtener los datos del archivo html var port = process . env . PORT || 8080 ; //Para que funcione en Heroku ya que da error 137 con el puerto 3000 http . createServer ( function ( req , res ) { res . writeHead ( 200 , { 'Content-Type' : 'text/html' }); // req.url almacena el path o ruta de la URL var url = req . url ; if ( url === \"/\" ) { // fs.readFile busca el archivo HTML // el primer par\u00e1metro es el path al archivo HTML // y el segundo es el callback de la funci\u00f3n // si el archivo no se encuentra, la funci\u00f3n devuelve un error // si el archivo se encuentra, el contenido del mismo se encuentra en pgres fs . readFile ( \"head.html\" , function ( err , pgres ) { if ( err ) res . write ( \"HEAD.HTML NOT FOUND\" ); else { // Las siguientes 3 lineas // tienen la funci\u00f3n de enviar el archivo html // y finalizar el proceso de respuesta res . writeHead ( 200 , { 'Content-Type' : 'text/html' }); res . write ( pgres ); res . end (); } }); } else if ( url === \"/tailPage\" ) { fs . readFile ( \"tail.html\" , function ( err , pgres ) { if ( err ) res . write ( \"TAIL.HTML NOT FOUND\" ); else { res . writeHead ( 200 , { 'Content-Type' : 'text/html' }); res . write ( pgres ); res . end (); } }); } }). listen ( port , function () { console . log ( \"SERVER STARTED PORT: 8080\" ); }); Ahora, tal y como hacemos siempre a la hora de crear nuestra aplicaci\u00f3n Node.js , con el fin de crear el archivo package.json , utilizaremos en el terminal el comando: npm init Podemos probar que nuestra aplicaci\u00f3n funciona perfectamente en local: node aplicacion.js Y tras ello, debemos poder acceder, desde nuestra m\u00e1quina anfitriona a http://IP-maq-virtual:8080 Ya con la aplicaci\u00f3n creada y comprobada, podremos desplegarla en m\u00faltiples plataformas en la nube, como AWS, GCP, Azure, Digital Ocean, Heroku... \u00a1Ojo! Para que nos funcione en Heroku, en el archivo package.json que se nos ha creado al hacer el npm init debemos hacerle una modificaci\u00f3n. En el bloque scripts, debemos borrar lo que haya dentro y dejar \u00fanicamente dentro de \u00e9l : \"start\" : \"node aplicacion.js\" De forma que Heroku sepa que comando utilizar para iniciar la aplicaci\u00f3n tras desplegarla.","title":"Creaci\u00f3n de nuestra aplicaci\u00f3n para Heroku"},{"location":"P3.4-Heroku/#proceso-de-despliegue-en-heroku","text":"Para trabajar con Heroku desde nuestro terminal, debemos instalar el propio CLI de Heroku. Consultando la documentaci\u00f3n , vemos que hemos de ejecutar: curl https://cli-assets.heroku.com/install.sh | sh Y comprobamos que se ha instalado correctamente consultando su versi\u00f3n: heroku -v Lo siguiente ser\u00e1 loguearnos en nuestra cuenta de Heroku mediante el terminal, para ello: heroku login Esto en teor\u00eda nos abre una pesta\u00f1a del navegador para loguearnos en nuestra cuenta. Puesto que estamos conectados por SSH a nuestra Debian, no suceder\u00e1 esto ya que el \u00fanico puerto por el que nos comunicamos es por el 22. Necesitar\u00edamos un t\u00fanel SSH para redirigir los puertos de la m\u00e1quina Debian remota a la nuestra y que nos abriese el navegador en nuestra m\u00e1quina. Puesto que esto escapa de los objetivos del m\u00f3dulo y con el fin de agilizar el proceso, simplemente copiaremos la URL y la pegaremos en nuestro navegador para loguearnos. Antes de continuar, conviene asegurarnos de que tenemos la \u00faltima versi\u00f3n de git en nuestra Debian: sudo apt-get update && sudo apt-get install git Ahora, dentro del directorio que hab\u00edamos creado previamente para nuestra aplicaci\u00f3n, se trata de seguir unos sencillos pasos: Tip Aqu\u00ed aparece explicado con lenguaje llano m\u00e1s adelante en el m\u00f3dulo ya hablaremos con mayor propiedad de estas acciones con git Nos aseguramos de que nuestro directorio no es a\u00fan un repositorio: git status Y lo iniciamos: git init Ahora a\u00f1adimos todos los archivos presentes en el directorio ( . ) para ser enviados al repositorio: git add . Y los preparamos para que sean envidos al repositorio: git commit -m \"Comentario explicativo del commit\" Creamos nuestra aplicaci\u00f3n en Heroku: heroku create Esto crear\u00e1 un git remoto que conectar\u00e1 con nuestro repositorio git local Desplegamos nuestra aplicaci\u00f3n en el server de Heroku : git push heroku master Y comprobamos que la instancia est\u00e1 corriendo: heroku ps:scale web=1 El comando heroku open abrir\u00eda nuestra aplicaci\u00f3n en el navegador. Sin embargo, por el problema explicado antes de estar conectados por SSH, esto no ocurrir\u00e1. No obstante, podemos acceder a nuestra aplicaci\u00f3n de otra forma r\u00e1pida y sencilla desde nuestro dashboard de Heroku: Localizamos nuestra aplicaci\u00f3n: Y tras hacer click en ella, localizamos el bot\u00f3n que nos permite abrirla y volvemos a hacer click: Comprobando que nuestra aplicaci\u00f3n, efectivametne se ha desplegado en Heroku y funciona a la perfecci\u00f3n:","title":"Proceso de despliegue en Heroku"},{"location":"P3.4-Heroku/#aplicacion-para-netlify","text":"Puesto que el inter\u00e9s en este m\u00f3dulo radica en el proceso de despliegue, suponiendo que la parte de desarrollo ya es abordada en otros m\u00f3dulos, vamos a utilizar una aplicaci\u00f3n de ejemplo que nos ahorre tiempo para centrarnos en el despliegue. Nos clonaremos este repositorio: git clone https://github.com/StackAbuse/color-shades-generator","title":"Aplicaci\u00f3n para Netlify"},{"location":"P3.4-Heroku/#proceso-de-despliegue-en-netlify","text":"Por mera curiosidad y ambici\u00f3n de aprendizaje, vamos a ver dos m\u00e9todos de despliegue en Netlify: Despliegue manual desde el CLI de Netlify, es decir, desde el terminal, a partir de un directorio local de nuestra m\u00e1quina. Despliegue desde un c\u00f3digo publicado en uno de nuestros repositorios de Github El primero nos permitir\u00e1 conocer el CLI de Netlify y el segundo nos acercara m\u00e1s a una experiencia real de despliegue. Task Vuestra primera tarea ser\u00e1 registraros en Netlify con vuestro email (no con vuestra cuenta de Github) y decirle que no cuando os pida enlazar con vuestra cuenta de Github (lo haremos m\u00e1s adelante).","title":"Proceso de despliegue en Netlify"},{"location":"P3.4-Heroku/#despliegue-mediante-cli","text":"Una vez registrados, debemos instalar el CLI de Netlify para ejecutar sus comandos desde el terminal: sudo npm install netlify-cli -g Est\u00e1 claro que para realizar acciones de deploy, Netlify nos solicitar\u00e1 una autenticaci\u00f3n, esto se hace mediante el comando: netlify login El cual nos muestra una pantalla del navegador para que concedamos la autorizaci\u00f3n pertinente. Sin embargo, recordemos el problema de que estamos conectados por SSH a nuestro servidor y no tenemos la posibilidad del uso de un entorno gr\u00e1fico. En este caso, siguiendo las instrucciones de la documentaci\u00f3n : Generamos el token de acceso Lo establecemos como variable de ambiente: Y nos logueamos netlify login Bueno, tenemos el c\u00f3digo de nuestra aplicaci\u00f3n, tenemos nuestra cuenta en Netlify y tenemos el CLI necesario para ejecutar comandos desde el terminal en esa cuenta... \u00bfPodemos proceder al despliegue sin mayores complicaciones? La respuesta es NO , como buenos desarrolladores y en base a experiencias anteriores, ya sab\u00e9is que hay que hacer un build de la aplicaci\u00f3n para, posteriormente, desplegarla. Vamos a ello. En primer lugar, como sabemos, debemos instalar todas las dependencias que vienen indicadas en el archivo package.json : npm install Y cuando ya las tengamos instaladas podemos proceder a realizar el build: npm run build Esto nos crear\u00e1 una nueva carpeta llamada build que contendr\u00e1 la aplicaci\u00f3n que debemos desplegar. Y ya podemos hacer un pre-deploy de la aplicaci\u00f3n de la que hemos hecho build antes: netlify deploy Nos har\u00e1 algunas preguntas para el desplieuge: Indicamos que queremos crear y configurar un nuevo site El Team lo dejamos por defecto Le indicamos el nombre que queremos emplear para la web ( nombre-practica3-4 ) y el directorio a utilizar para el deploy (directorio ./build ). Y si nos indica que todo ha ido bien e incluso podemos ver el \"borrador\" (Website Draft URL) de la web que nos aporta, podemos pasarla a producci\u00f3n finalmente tal y como nos indica la misma salida del comando: If everything looks good on your draft URL, deploy it to your main site URL with the --prod flag. netlify deploy --prod Warning No olvides desplegar finalmente en producci\u00f3n y comprobar que puedes acceder a la URL.","title":"Despliegue mediante CLI"},{"location":"P3.4-Heroku/#despliegue-mediante-conexion-con-github","text":"En primer lugar, vamos a eliminar el site que hemos desplegado antes en Netlify para evitarnos cualquier problema y/o conflicto: En segundo lugar, vamos a borrar el directorio donde se halla el repositorio clonado en el paso anterior para as\u00ed poder empezar de 0: rm -rf directorio_repositorio Como queremos simular que hemos picado el c\u00f3digo a man o en local y lo vamos a subir a Github por primera vez, nos descargaremos los fuentes en formato .zip sin que tenga ninguna referencia a Github: wget https://github.com/StackAbuse/color-shades-generator/archive/refs/heads/main.zip Creamos una carpeta nueva y descomprimimos dentro el zip: mkdir practica3.4 unzip main.zip -d practica3.4/ Entramos en la carpeta donde est\u00e1 el c\u00f3digo: cd practica3.4/color-shades-generator-main/ Ahora debemos crear un repositorio completamente vac\u00edo en Github que se llame practicaTresCuatro : Y tras ello, volviendo al terminal a la carpeta donde est\u00e1bamos, la iniciamos como repositorio, a\u00f1adimos todo el contenido de la misma para el commit, hacemos el commit con el mensaje correspondiente y creamos la rama main: $ git init $ git add . $ git commit -m \"Subiendo el c\u00f3digo...\" $ git branch -M main Y ahora s\u00f3lo queda referenciar nuestra carpeta al repositorio reci\u00e9n creado en Github y hacer un push para subir todo el contenido del commit a \u00e9l: $ git remote add origin https://github.com/username/practicaTresCuatro.git $ git push -u origin main Ahora que ya tenemos subido el c\u00f3digo a GitHub, de alguna manera debemos enganchar o enlazar nuestra cuenta de Github con la de Netlify para que \u00e9ste \u00faltimo pueda traerse el c\u00f3digo de all\u00ed, hacer el build y desplegarlo. As\u00ed pues, entramos en nuestro dashboard de Netlify y le damos a importar proyecto existente de git : Le indicamos que concretamente de Github: Y nos saltar\u00e1 una ventana pidiendo que autoricemos a Netlify a acceder a nuestros repositorios de Github: Y luego le indicaremos que no acceda a todos nuestros repositorios sino s\u00f3lo al repositorio que necesitamos, que es donde tenemos el c\u00f3digo de nuestra aplicaci\u00f3n: Y ya quedar\u00e1 todo listo: Y desplegamos la aplicaci\u00f3n: Netlify se encargar\u00e1 de hacer el build de forma autom\u00e1tica tal y como hemos visto en la imagen de arriba, con el comando npm run build , publicando el contenido del directorio build . Atenci\u00f3n Tras el deploy, en \"Site settings\" pode\u00eds y deb\u00e9is cambiar el nombre de la aplicaci\u00f3n por nombre-practica3-4, donde nombre es vuestro nombre. Lo que hemos conseguido de esta forma es que, cualquier cambio que hagamos en el proyecto y del que hagamos commit y push en Github, autom\u00e1ticamente genere un nuevo despliegue en Netlify. Es el principio de lo que m\u00e1s adelante veremos como despliegue continuo . Comprobemos que realmente es as\u00ed: Dentro de la carpeta public encontramos el archivo robots.txt , cuyo cometido es indicar a los rastreadores de los buscadores a qu\u00e9 URLs del sitio pueden acceder. A este archivo se puede acceder a trav\u00e9s de la URL del site: Dentro de la carpeta public , utilizando el editor de texto que prefir\u00e1is en vuestro terminal, modificad el archivo robots.txt para que excluya un directorio que se llame nombre_apellido , utilizando obviamente vuestro nombre y apellido. User-agent: * Disallow: /nombre_y_apellido/ Haz un nuevo commit y push (del caso anterior, recuerda el commando git previo para a\u00f1adir los archivos a hacer commit) Comprueba en el dashboard de Netlify que se ha producido un nuevo deploy de la aplicaci\u00f3n hace escasos segundos Accede a https://url_de_la_aplicacion/robots.txt y comprueba que, efectivamente, se ve reflejado el cambio","title":"Despliegue mediante conexi\u00f3n con Github"},{"location":"P3.4-Heroku/#cuestiones","text":"Investiga y explica que es un Dyno en terminolog\u00eda Heroku. En Heroku no todo es de color de rosa, tiene sus limitaciones y desventajas. Busca, investiga y explica algunas de ellas detalladamente. Task Documenta la realizaci\u00f3n de toda esta pr\u00e1ctica adecuadamente, con las explicaciones y justificaciones necesarias y las capturas de pantalla pertinentes.","title":"Cuestiones"},{"location":"P3.4-Heroku/#referencias","text":"\u00bfQu\u00e9 es Github? \u00bfQu\u00e9 es Heroku? Deploying Node.js applications List of all limitations in Heroku platform How to deploy your website to Netlify for free 4 Ways To Deploy Your Static Site with Netlify Guide to Deploying a React App to Netlify","title":"Referencias"},{"location":"P3.4-Heroku/#evaluacion","text":"Criterio Puntuaci\u00f3n Despliegue correcto y bien documentado en Heroku 2 puntos Despliegue correcto y bien documentado en Netlify mediante CLI 0.75 puntos Despliegue correcto y bien documentado en Netlify de forma manual desde el dashboard 2 puntos Cambio de nombre del site 0.25 puntos Comprobaci\u00f3n correcta y bien documentada de despliegue autom\u00e1tico al hacer push en Github 3 puntos Respuestas correctas a las cuestiones 1 puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 1 puntos","title":"Evaluaci\u00f3n"},{"location":"P3.5-Flask/","text":"Pr\u00e1ctica 3.5: Despliegue de una aplicaci\u00f3n Flask (Python) Prerrequisitos Servidor Debian con los siguientes paquetes instalados: Nginx Gunicorn Pipenv Introducci\u00f3n \u00bfQu\u00e9 es un framework? Actualmente en el desarrollo moderno de aplicaciones web se utilizan distintos Frameworks que son herramientas que nos dan un esquema de trabajo y una serie de utilidades y funciones que nos facilita y nos abstrae de la construcci\u00f3n de p\u00e1ginas web din\u00e1micas. En general los Frameworks est\u00e1n asociado a lenguajes de programaci\u00f3n (Ruby on Rails (Ruby), Symphony (PHP)), en el mundo de Python el m\u00e1s conocido es Django pero Flask es una opci\u00f3n que quiz\u00e1s no tenga una curva de aprendizaje tan elevada pero nos posibilita la creaci\u00f3n de aplicaciones web igual de complejas de las que se pueden crear en Django. Flask En la actualidad existen muchas opciones para crear p\u00e1ginas web y muchos lenguajes (PHP, JAVA), y en este caso Flask nos permite crear de una manera muy sencilla aplicaciones web con Python. Flask es un \u201cmicro\u201d Framework escrito en Python y concebido para facilitar el desarrollo de Aplicaciones Web bajo el patr\u00f3n MVC. La palabra \u201cmicro\u201d no designa a que sea un proyecto peque\u00f1o o que nos permita hacer p\u00e1ginas web peque\u00f1as sino que al instalar Flask tenemos las herramientas necesarias para crear una aplicaci\u00f3n web funcional pero si se necesita en alg\u00fan momento una nueva funcionalidad hay un conjunto muy grande extensiones (plugins) que se pueden instalar con Flask que le van dotando de funcionalidad. De principio en la instalaci\u00f3n no se tienen todas las funcionalidades que se pueden necesitar pero de una manera muy sencilla se pueden extender el proyecto con nuevas funcionalidades por medio de plugins. El patr\u00f3n MVC es una manera o una forma de trabajar que permite diferenciar y separar lo que es el modelo de datos (los datos que van a tener la App que normalmente est\u00e1n guardados en BD), la vista (p\u00e1gina HTML) y el controlador (donde se gestiona las peticiones de la app web). Gunicorn Cuando se implementa una aplicaci\u00f3n web basada en Python, normalmente se tienen estas tres piezas: Servidor web (Nginx, Apache) Servidor de aplicaciones WSGI (Gunicorn, uWSGI, mod_wsgi, Waitress) Aplicaci\u00f3n web (Django, Flask, Pyramid, FastAPI) Los servidores web procesan y distribuyen las solicitudes de los navegadores y otros clientes y env\u00edan respuestas a los mismos. WSGI (Web Server Gateway Interface) proporciona un conjunto de reglas para estandarizar el comportamiento y la comunicaci\u00f3n entre servidores web y aplicaciones web. Mediante el uso de servidores y aplicaciones web compatibles con WSGI, los desarrolladores pueden concentrar su tiempo y energ\u00eda en el desarrollo de aplicaciones web en lugar de administrar la comunicaci\u00f3n entre la aplicaci\u00f3n y el servidor web. Finalmente, Gunicorn, que es la abreviatura de Green Unicorn, es un servidor de aplicaciones WSGI que se encuentra entre el servidor web y su aplicaci\u00f3n web, gestionando la comunicaci\u00f3n entre los dos. Acepta solicitudes del servidor y las traduce (a trav\u00e9s de WSGI) en algo que la aplicaci\u00f3n web puede entender antes de pasarla a la aplicaci\u00f3n web real. Env\u00eda respuestas desde la aplicaci\u00f3n web al servidor. Tambi\u00e9n se encarga de ejecutar varias instancias de la aplicaci\u00f3n web, reinici\u00e1ndolas seg\u00fan sea necesario y distribuyendo solicitudes a instancias saludables. Gestor de paquetes pip pip es el comando para instalar paquetes de Python integrados en las fuentes desde la versi\u00f3n 3.4. Este comando automatiza la conexi\u00f3n al sitio https://pypi.org/, la descarga, la instalaci\u00f3n e incluso la compilaci\u00f3n del m\u00f3dulo solicitado. Adem\u00e1s, se ocupa de las dependencias de cada paquete. Entornos virtuales en Python Un entorno virtual es una forma de tener m\u00faltiples instancias paralelas del int\u00e9rprete de Python, cada una con diferentes conjuntos de paquetes y diferentes configuraciones. Cada entorno virtual contiene una copia independiente del int\u00e9rprete de Python, incluyendo copias de sus utilidades de soporte. Los paquetes instalados en cada entorno virtual s\u00f3lo se ven en ese entorno virtual y en ning\u00fan otro. Incluso los paquetes grandes y complejos con binarios dependientes de la plataforma pueden ser acorralados entre s\u00ed en entornos virtuales. De esta forma, tendremos entornos independientes entre s\u00ed, parecido a como ocurr\u00eda con los directorios de los proyectos de Node.js . De este modo, los entornos virtuales de Python nos permiten instalar un paquete de Python en una ubicaci\u00f3n aislada en lugar de instalarlo de manera global. Pipenv Pipenv es una herramienta que apunta a traer todo lo mejor del mundo de empaquetado (bundler, composer, npm, cargo, yarn, etc.) al mundo de Python. Autom\u00e1ticamente crea y maneja un entorno virtual para tus proyectos, tambi\u00e9n permite agregar/eliminar paquetes desde tu Pipfile as\u00ed como como instalar/desinstalar paquetes. Tambi\u00e9n genera lo m\u00e1s importante , el archivo Pipfile.lock , que es usado para producir determinado build. Procedimiento completo para el despliegue Instalamos el gestor de paquetes de Python pip: sudo apt-get update sudo apt-get install python3-pip Instalamos el paquete pipenv para gestionar los entornos virtuales: pip3 install pipenv Y comprobamos que est\u00e1 instalado correctamente mostrando su versi\u00f3n: PATH = $PATH :/home/raul/.local/bin pipenv --version Creamos el directorio en el que almacenaremos nuestro proyecto: sudo mkdir /var/www/nombre_mi_aplicacion Al crearlo con sudo , los permisos pertenecen a root: Hay que cambiarlo para que el due\u00f1o sea nuestro usuario ( raul-debian en mi caso) y pertenezca al grupo www-data , el usuario usado por defecto por el servidor web para correr: sudo chown -R $USER :www-data /var/www/mi_aplicacion Establecemos los permisos adecuados a este directorio, para que pueda ser le\u00eddo por todo el mundo: chmod -R 775 /var/www/mi_aplicacion Warning Es indispensable asignar estos permisos, de otra forma obtendr\u00edamos un error al acceder a la aplicaci\u00f3n cuando pongamos en marcha Nginx Dentro del directorio de nuestra aplicaci\u00f3n, creamos un archivo oculto .env que contendr\u00e1 las variables de entorno necesarias: touch .env Editamos el archivo y a\u00f1adimos las variables, indicando cu\u00e1l es el archivo .py de la aplicaci\u00f3n y el entorno, que en nuestro caso ser\u00e1 producci\u00f3n: Nota En el mundo laboral real, se supone que la aplicaci\u00f3n previamente ha pasado por los entornos de dev, test y preprod para el desarrollo y prueba de la misma, antes de pasarla a producci\u00f3n. Iniciamos ahora nuestro entorno virtual. Pipenv cargar\u00e1 las variables de entorno desde el fichero .env de forma autom\u00e1tica: pipenv shell Veremos que se nos inicia el entorno virtual, cosa que comprobamos porque aparece su nombre al inicio del prompt del shell: Usamos pipenv para instalar las dependencias necesarias para nuestro proyecto: pipenv install flask gunicorn Vamos ahora a crear la aplicaci\u00f3n Flask m\u00e1s simple posible, a modo de PoC (proof of concept o prueba de concepto). El archivo que contendr\u00e1 la aplicaci\u00f3n propiamente dicha ser\u00e1 application.py y wsgi.py se encargar\u00e1 \u00fanicamente de iniciarla y dejarla corriendo: touch application.py wsgi.py Y tras crear los archivos, los editamos para dejarlos as\u00ed: Corramos ahora nuestra aplicaci\u00f3n a modo de comprobaci\u00f3n con el servidor web integrado de Flask. Si especificamos la direcci\u00f3n 0.0.0.0 lo que le estamos diciendo al servidor es que escuche en todas sus interfaces, si las tuviera: Ahora podremos acceder a la aplicaci\u00f3n desde nuestro ordenador, nuestra m\u00e1quina anfitri\u00f3n, introduciendo en un navegador web: http://IP-maq-virtual:5000 : Tras la comprobaci\u00f3n, paramos el servidor con CTRL+C Comprobemos ahora que Gunicorn funciona correctamente tambi\u00e9n. Si os ha funcionado el servidor de desarrollo de Flask, pod\u00e9is usar el siguiente comando para probar que la alicaci\u00f3n funciona correctamente usando Gunicorn, accediendo con vuestro navegador de la misma forma que en el paso anterior: gunicorn --workers 4 --bind 0 .0.0.0:5000 wsgi:app Donde: --workers N establece el n\u00famero de workers o hilos que queremos utilizar, como ocurr\u00eda con Node Express. Depender\u00e1 del n\u00famero de cores que le hayamos dado a la CPU de nuestra m\u00e1quina virtual. --bind 0.0.0.0:5000 hace que el servidor escuche peticiones por todas sus interfaces de red y en el puerto 5000 wsgi:app es el nombre del archivo con extensi\u00f3n .py y app es la instancia de la aplicaci\u00f3n Flask dentro del archivo. Todav\u00eda dentro de nuestro entorno virtual, debemos tomar nota de cual es el path o ruta desde la que se ejecuta gunicorn para poder configurar m\u00e1s adelante un servicio del sistema. Podemos averigurarlo as\u00ed: Tip Y tras ello debemos salir de nuestro entorno virtual con el sencillo comando deactivate Puesto que ya debemos tener instalado Nginx en nuestro sistema, lo ininciamos y comprobamos que su estado sea activo: sudo systemctl start nginx sudo systemctl status nginx Ya fuera de nuestro entorno virtual, crearemos un archivo para que systemd corra Gunicorn como un servicio del sistema m\u00e1s: Donde: User : Establece el usuario que tiene permisos sobre el directorio del proyecto (el que pusist\u00e9is en el paso 5) Group : Establece el grupo que tiene permisos sobre el directorio del proyecto (el que pusist\u00e9is en el paso 5) Environment : Establece el directorio bin (donde se guardan los binarios ejecutables) dentro del entorno virtual (lo vist\u00e9is en el paso 14) WorkingDirectory : Establece el directorio base donde reside nuestro proyecto ExecStart : Establece el path donde se encuentra el ejecutable de gunicorn dentro del entorno virtual, as\u00ed como las opciones y comandos con los que se iniciar\u00e1 Warning Deb\u00e9is cambiar los valores para que coincidan con los de vuestro caso particular. Ahora, como cada vez que se crea un servicio nuevo de systemd , se habilita y se inicia: systemctl enable nombre_mi_servicio systemctl start nombre_mi_servicio Recordad que el nombre del servicio es el nombre del archivo que creast\u00e9is en el paso anterior. Pasemos ahora a configurar Nginx , que es algo que ya deber\u00edamos tener dominado de cap\u00edtulos anteriores. Creamos un archivo con el nombre de nuestra aplicaci\u00f3n y dentro estableceremos la configuraci\u00f3n para ese sitio web. El archivo, como record\u00e1is, debe estar en /etc/nginx/sites-available/nombre_aplicacion y tras ello lo editamos para que quede: server { listen 80; server_name mi_aplicacion www.mi_aplicacion; #(1) access_log /var/log/nginx/mi_aplicacion.access.log; #(2) error_log /var/log/nginx/mi_aplicacion.error.log; location / { include proxy_params; proxy_pass http://unix:/var/www/nombre_aplicacion/nombre_aplicacion.sock; #(3) } } Nombre del dominio, ya veremos m\u00e1s adelante como el DNS resolver\u00e1 este nombre para acceder a nuestra aplicaci\u00f3n. D\u00f3nde estar\u00e1n ubicados los logs de acceso y de errores. Bloque donde se le indica a Nginx que haga de proxy inverso hacia el socket creado en nuestra propia m\u00e1quina por gunicorn para acceder a nuestra aplicaci\u00f3n Flask. Recordemos que ahora debemos crear un link simb\u00f3lico del archivo de sitios webs disponibles al de sitios web activos: sudo ln -s /etc/nginx/sites-available/nombre_aplicacion /etc/nginx/sites-enabled/ Y nos aseguramos de que se ha creado dicho link simb\u00f3lico: ls -l /etc/nginx/sites-enabled/ | grep nombre_aplicacion Nos aseguramos de que la configuraci\u00f3n de Nginx no contiene errores, reiniciamos Nginx y comprobamos que se estado es activo: nginx -t sudo systemctl restart nginx sudo systemctl status nginx Ya no podremos acceder por IP a nuestra aplicaci\u00f3n ya que ahora est\u00e1 siendo servida por Gunicorn y Nginx, necesitamos acceder por su server_name . Puesto que a\u00fan no hemos tratado con el DNS, vamos a editar el archivo /etc/hosts de nuestra m\u00e1quina anfitriona para que asocie la IP de la m\u00e1quina virtual, a nuestro server_name . Este archivo, en Linux, est\u00e1 en: /etc/hosts Y en Windows: C:\\Windows\\System32\\drivers\\etc\\hosts Y deberemos a\u00f1adirle la l\u00ednea: 192.168.X.X myproject www.myproject donde deb\u00e9is sustituir la IP por la que tenga vuestra m\u00e1quina virtual. El \u00faltimo paso es comprobar que todo el desplieuge se ha realizado de forma correcta y est\u00e1 funcionando, para ello accedemos desde nuestra m\u00e1quina anfitri\u00f3n a: http://nombre_aplicacion O: http://www.nombre_aplicacion Y deber\u00eda mostraros la misma p\u00e1gina que en el paso 14: Ejercicio Repite todo el proceso con la aplicaci\u00f3n del siguiente repositorio: https://github.com/raul-profesor/Practica-3.5 Recuerda que deber\u00e1s clonar el repositorio en tu directorio /var/www : git clone https://github.com/raul-profesor/Practica-3.5 Y, tras activar el entorno virtual dentro del directorio del repositorio clonado , para instalar las dependencias del proyecto de la aplicaci\u00f3n deber\u00e1s hacer: pipenv install -r requirements.txt Y un \u00faltimo detalle, si mir\u00e1is el c\u00f3digo del proyecto, quee es muy sencillo, ver\u00e9is que Gunicorn debe iniciarse ahora as\u00ed: gunicorn --workers 4 --bind 0.0.0.0:5000 wsgi:app Y el resto ser\u00eda proceder tal y como hemos hecho en esta pr\u00e1ctica. Warning Documenta adecuadamente con explicaciones y capturas de pantalla los procesos de despliegue de ambas aplicaciones en Flask, as\u00ed como las respuestas a las cuestiones planteadas. Cuestiones Cuestion 1 Busca, lee, entiende y explica qu\u00e9 es y para que sirve un servidor WSGI Tareas de ampliaci\u00f3n Ampliaci\u00f3n Despliega cualquiera de las dos aplicaciones Flask presentadas aqu\u00ed en Heroku. Referencias \u00bfQu\u00e9 es Flask? Deploy Flask The Easy Way With Gunicorn and Nginx! Deploy flask app with Nginx using Gunicorn Evaluaci\u00f3n Criterio Puntuaci\u00f3n Despliegue correcto, completo y bien documentado del primer ejemplo 2 puntos Despliegue correcto, completo y bien documentado del segundo ejemplo 5 puntos Respuestas correctas a las cuestiones 1 puntos Tarea de ampliaci\u00f3n 1 punto Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Pr\u00e1ctica 3.5 - Despliegue de una aplicaci\u00f3n Flask"},{"location":"P3.5-Flask/#practica-35-despliegue-de-una-aplicacion-flask-python","text":"","title":"Pr\u00e1ctica 3.5: Despliegue de una aplicaci\u00f3n Flask (Python)"},{"location":"P3.5-Flask/#prerrequisitos","text":"Servidor Debian con los siguientes paquetes instalados: Nginx Gunicorn Pipenv","title":"Prerrequisitos"},{"location":"P3.5-Flask/#introduccion","text":"","title":"Introducci\u00f3n"},{"location":"P3.5-Flask/#que-es-un-framework","text":"Actualmente en el desarrollo moderno de aplicaciones web se utilizan distintos Frameworks que son herramientas que nos dan un esquema de trabajo y una serie de utilidades y funciones que nos facilita y nos abstrae de la construcci\u00f3n de p\u00e1ginas web din\u00e1micas. En general los Frameworks est\u00e1n asociado a lenguajes de programaci\u00f3n (Ruby on Rails (Ruby), Symphony (PHP)), en el mundo de Python el m\u00e1s conocido es Django pero Flask es una opci\u00f3n que quiz\u00e1s no tenga una curva de aprendizaje tan elevada pero nos posibilita la creaci\u00f3n de aplicaciones web igual de complejas de las que se pueden crear en Django.","title":"\u00bfQu\u00e9 es un framework?"},{"location":"P3.5-Flask/#flask","text":"En la actualidad existen muchas opciones para crear p\u00e1ginas web y muchos lenguajes (PHP, JAVA), y en este caso Flask nos permite crear de una manera muy sencilla aplicaciones web con Python. Flask es un \u201cmicro\u201d Framework escrito en Python y concebido para facilitar el desarrollo de Aplicaciones Web bajo el patr\u00f3n MVC. La palabra \u201cmicro\u201d no designa a que sea un proyecto peque\u00f1o o que nos permita hacer p\u00e1ginas web peque\u00f1as sino que al instalar Flask tenemos las herramientas necesarias para crear una aplicaci\u00f3n web funcional pero si se necesita en alg\u00fan momento una nueva funcionalidad hay un conjunto muy grande extensiones (plugins) que se pueden instalar con Flask que le van dotando de funcionalidad. De principio en la instalaci\u00f3n no se tienen todas las funcionalidades que se pueden necesitar pero de una manera muy sencilla se pueden extender el proyecto con nuevas funcionalidades por medio de plugins. El patr\u00f3n MVC es una manera o una forma de trabajar que permite diferenciar y separar lo que es el modelo de datos (los datos que van a tener la App que normalmente est\u00e1n guardados en BD), la vista (p\u00e1gina HTML) y el controlador (donde se gestiona las peticiones de la app web).","title":"Flask"},{"location":"P3.5-Flask/#gunicorn","text":"Cuando se implementa una aplicaci\u00f3n web basada en Python, normalmente se tienen estas tres piezas: Servidor web (Nginx, Apache) Servidor de aplicaciones WSGI (Gunicorn, uWSGI, mod_wsgi, Waitress) Aplicaci\u00f3n web (Django, Flask, Pyramid, FastAPI) Los servidores web procesan y distribuyen las solicitudes de los navegadores y otros clientes y env\u00edan respuestas a los mismos. WSGI (Web Server Gateway Interface) proporciona un conjunto de reglas para estandarizar el comportamiento y la comunicaci\u00f3n entre servidores web y aplicaciones web. Mediante el uso de servidores y aplicaciones web compatibles con WSGI, los desarrolladores pueden concentrar su tiempo y energ\u00eda en el desarrollo de aplicaciones web en lugar de administrar la comunicaci\u00f3n entre la aplicaci\u00f3n y el servidor web. Finalmente, Gunicorn, que es la abreviatura de Green Unicorn, es un servidor de aplicaciones WSGI que se encuentra entre el servidor web y su aplicaci\u00f3n web, gestionando la comunicaci\u00f3n entre los dos. Acepta solicitudes del servidor y las traduce (a trav\u00e9s de WSGI) en algo que la aplicaci\u00f3n web puede entender antes de pasarla a la aplicaci\u00f3n web real. Env\u00eda respuestas desde la aplicaci\u00f3n web al servidor. Tambi\u00e9n se encarga de ejecutar varias instancias de la aplicaci\u00f3n web, reinici\u00e1ndolas seg\u00fan sea necesario y distribuyendo solicitudes a instancias saludables.","title":"Gunicorn"},{"location":"P3.5-Flask/#gestor-de-paquetes-pip","text":"pip es el comando para instalar paquetes de Python integrados en las fuentes desde la versi\u00f3n 3.4. Este comando automatiza la conexi\u00f3n al sitio https://pypi.org/, la descarga, la instalaci\u00f3n e incluso la compilaci\u00f3n del m\u00f3dulo solicitado. Adem\u00e1s, se ocupa de las dependencias de cada paquete.","title":"Gestor de paquetes pip"},{"location":"P3.5-Flask/#entornos-virtuales-en-python","text":"Un entorno virtual es una forma de tener m\u00faltiples instancias paralelas del int\u00e9rprete de Python, cada una con diferentes conjuntos de paquetes y diferentes configuraciones. Cada entorno virtual contiene una copia independiente del int\u00e9rprete de Python, incluyendo copias de sus utilidades de soporte. Los paquetes instalados en cada entorno virtual s\u00f3lo se ven en ese entorno virtual y en ning\u00fan otro. Incluso los paquetes grandes y complejos con binarios dependientes de la plataforma pueden ser acorralados entre s\u00ed en entornos virtuales. De esta forma, tendremos entornos independientes entre s\u00ed, parecido a como ocurr\u00eda con los directorios de los proyectos de Node.js . De este modo, los entornos virtuales de Python nos permiten instalar un paquete de Python en una ubicaci\u00f3n aislada en lugar de instalarlo de manera global.","title":"Entornos virtuales en Python"},{"location":"P3.5-Flask/#pipenv","text":"Pipenv es una herramienta que apunta a traer todo lo mejor del mundo de empaquetado (bundler, composer, npm, cargo, yarn, etc.) al mundo de Python. Autom\u00e1ticamente crea y maneja un entorno virtual para tus proyectos, tambi\u00e9n permite agregar/eliminar paquetes desde tu Pipfile as\u00ed como como instalar/desinstalar paquetes. Tambi\u00e9n genera lo m\u00e1s importante , el archivo Pipfile.lock , que es usado para producir determinado build.","title":"Pipenv"},{"location":"P3.5-Flask/#procedimiento-completo-para-el-despliegue","text":"Instalamos el gestor de paquetes de Python pip: sudo apt-get update sudo apt-get install python3-pip Instalamos el paquete pipenv para gestionar los entornos virtuales: pip3 install pipenv Y comprobamos que est\u00e1 instalado correctamente mostrando su versi\u00f3n: PATH = $PATH :/home/raul/.local/bin pipenv --version Creamos el directorio en el que almacenaremos nuestro proyecto: sudo mkdir /var/www/nombre_mi_aplicacion Al crearlo con sudo , los permisos pertenecen a root: Hay que cambiarlo para que el due\u00f1o sea nuestro usuario ( raul-debian en mi caso) y pertenezca al grupo www-data , el usuario usado por defecto por el servidor web para correr: sudo chown -R $USER :www-data /var/www/mi_aplicacion Establecemos los permisos adecuados a este directorio, para que pueda ser le\u00eddo por todo el mundo: chmod -R 775 /var/www/mi_aplicacion Warning Es indispensable asignar estos permisos, de otra forma obtendr\u00edamos un error al acceder a la aplicaci\u00f3n cuando pongamos en marcha Nginx Dentro del directorio de nuestra aplicaci\u00f3n, creamos un archivo oculto .env que contendr\u00e1 las variables de entorno necesarias: touch .env Editamos el archivo y a\u00f1adimos las variables, indicando cu\u00e1l es el archivo .py de la aplicaci\u00f3n y el entorno, que en nuestro caso ser\u00e1 producci\u00f3n: Nota En el mundo laboral real, se supone que la aplicaci\u00f3n previamente ha pasado por los entornos de dev, test y preprod para el desarrollo y prueba de la misma, antes de pasarla a producci\u00f3n. Iniciamos ahora nuestro entorno virtual. Pipenv cargar\u00e1 las variables de entorno desde el fichero .env de forma autom\u00e1tica: pipenv shell Veremos que se nos inicia el entorno virtual, cosa que comprobamos porque aparece su nombre al inicio del prompt del shell: Usamos pipenv para instalar las dependencias necesarias para nuestro proyecto: pipenv install flask gunicorn Vamos ahora a crear la aplicaci\u00f3n Flask m\u00e1s simple posible, a modo de PoC (proof of concept o prueba de concepto). El archivo que contendr\u00e1 la aplicaci\u00f3n propiamente dicha ser\u00e1 application.py y wsgi.py se encargar\u00e1 \u00fanicamente de iniciarla y dejarla corriendo: touch application.py wsgi.py Y tras crear los archivos, los editamos para dejarlos as\u00ed: Corramos ahora nuestra aplicaci\u00f3n a modo de comprobaci\u00f3n con el servidor web integrado de Flask. Si especificamos la direcci\u00f3n 0.0.0.0 lo que le estamos diciendo al servidor es que escuche en todas sus interfaces, si las tuviera: Ahora podremos acceder a la aplicaci\u00f3n desde nuestro ordenador, nuestra m\u00e1quina anfitri\u00f3n, introduciendo en un navegador web: http://IP-maq-virtual:5000 : Tras la comprobaci\u00f3n, paramos el servidor con CTRL+C Comprobemos ahora que Gunicorn funciona correctamente tambi\u00e9n. Si os ha funcionado el servidor de desarrollo de Flask, pod\u00e9is usar el siguiente comando para probar que la alicaci\u00f3n funciona correctamente usando Gunicorn, accediendo con vuestro navegador de la misma forma que en el paso anterior: gunicorn --workers 4 --bind 0 .0.0.0:5000 wsgi:app Donde: --workers N establece el n\u00famero de workers o hilos que queremos utilizar, como ocurr\u00eda con Node Express. Depender\u00e1 del n\u00famero de cores que le hayamos dado a la CPU de nuestra m\u00e1quina virtual. --bind 0.0.0.0:5000 hace que el servidor escuche peticiones por todas sus interfaces de red y en el puerto 5000 wsgi:app es el nombre del archivo con extensi\u00f3n .py y app es la instancia de la aplicaci\u00f3n Flask dentro del archivo. Todav\u00eda dentro de nuestro entorno virtual, debemos tomar nota de cual es el path o ruta desde la que se ejecuta gunicorn para poder configurar m\u00e1s adelante un servicio del sistema. Podemos averigurarlo as\u00ed: Tip Y tras ello debemos salir de nuestro entorno virtual con el sencillo comando deactivate Puesto que ya debemos tener instalado Nginx en nuestro sistema, lo ininciamos y comprobamos que su estado sea activo: sudo systemctl start nginx sudo systemctl status nginx Ya fuera de nuestro entorno virtual, crearemos un archivo para que systemd corra Gunicorn como un servicio del sistema m\u00e1s: Donde: User : Establece el usuario que tiene permisos sobre el directorio del proyecto (el que pusist\u00e9is en el paso 5) Group : Establece el grupo que tiene permisos sobre el directorio del proyecto (el que pusist\u00e9is en el paso 5) Environment : Establece el directorio bin (donde se guardan los binarios ejecutables) dentro del entorno virtual (lo vist\u00e9is en el paso 14) WorkingDirectory : Establece el directorio base donde reside nuestro proyecto ExecStart : Establece el path donde se encuentra el ejecutable de gunicorn dentro del entorno virtual, as\u00ed como las opciones y comandos con los que se iniciar\u00e1 Warning Deb\u00e9is cambiar los valores para que coincidan con los de vuestro caso particular. Ahora, como cada vez que se crea un servicio nuevo de systemd , se habilita y se inicia: systemctl enable nombre_mi_servicio systemctl start nombre_mi_servicio Recordad que el nombre del servicio es el nombre del archivo que creast\u00e9is en el paso anterior. Pasemos ahora a configurar Nginx , que es algo que ya deber\u00edamos tener dominado de cap\u00edtulos anteriores. Creamos un archivo con el nombre de nuestra aplicaci\u00f3n y dentro estableceremos la configuraci\u00f3n para ese sitio web. El archivo, como record\u00e1is, debe estar en /etc/nginx/sites-available/nombre_aplicacion y tras ello lo editamos para que quede: server { listen 80; server_name mi_aplicacion www.mi_aplicacion; #(1) access_log /var/log/nginx/mi_aplicacion.access.log; #(2) error_log /var/log/nginx/mi_aplicacion.error.log; location / { include proxy_params; proxy_pass http://unix:/var/www/nombre_aplicacion/nombre_aplicacion.sock; #(3) } } Nombre del dominio, ya veremos m\u00e1s adelante como el DNS resolver\u00e1 este nombre para acceder a nuestra aplicaci\u00f3n. D\u00f3nde estar\u00e1n ubicados los logs de acceso y de errores. Bloque donde se le indica a Nginx que haga de proxy inverso hacia el socket creado en nuestra propia m\u00e1quina por gunicorn para acceder a nuestra aplicaci\u00f3n Flask. Recordemos que ahora debemos crear un link simb\u00f3lico del archivo de sitios webs disponibles al de sitios web activos: sudo ln -s /etc/nginx/sites-available/nombre_aplicacion /etc/nginx/sites-enabled/ Y nos aseguramos de que se ha creado dicho link simb\u00f3lico: ls -l /etc/nginx/sites-enabled/ | grep nombre_aplicacion Nos aseguramos de que la configuraci\u00f3n de Nginx no contiene errores, reiniciamos Nginx y comprobamos que se estado es activo: nginx -t sudo systemctl restart nginx sudo systemctl status nginx Ya no podremos acceder por IP a nuestra aplicaci\u00f3n ya que ahora est\u00e1 siendo servida por Gunicorn y Nginx, necesitamos acceder por su server_name . Puesto que a\u00fan no hemos tratado con el DNS, vamos a editar el archivo /etc/hosts de nuestra m\u00e1quina anfitriona para que asocie la IP de la m\u00e1quina virtual, a nuestro server_name . Este archivo, en Linux, est\u00e1 en: /etc/hosts Y en Windows: C:\\Windows\\System32\\drivers\\etc\\hosts Y deberemos a\u00f1adirle la l\u00ednea: 192.168.X.X myproject www.myproject donde deb\u00e9is sustituir la IP por la que tenga vuestra m\u00e1quina virtual. El \u00faltimo paso es comprobar que todo el desplieuge se ha realizado de forma correcta y est\u00e1 funcionando, para ello accedemos desde nuestra m\u00e1quina anfitri\u00f3n a: http://nombre_aplicacion O: http://www.nombre_aplicacion Y deber\u00eda mostraros la misma p\u00e1gina que en el paso 14: Ejercicio Repite todo el proceso con la aplicaci\u00f3n del siguiente repositorio: https://github.com/raul-profesor/Practica-3.5 Recuerda que deber\u00e1s clonar el repositorio en tu directorio /var/www : git clone https://github.com/raul-profesor/Practica-3.5 Y, tras activar el entorno virtual dentro del directorio del repositorio clonado , para instalar las dependencias del proyecto de la aplicaci\u00f3n deber\u00e1s hacer: pipenv install -r requirements.txt Y un \u00faltimo detalle, si mir\u00e1is el c\u00f3digo del proyecto, quee es muy sencillo, ver\u00e9is que Gunicorn debe iniciarse ahora as\u00ed: gunicorn --workers 4 --bind 0.0.0.0:5000 wsgi:app Y el resto ser\u00eda proceder tal y como hemos hecho en esta pr\u00e1ctica. Warning Documenta adecuadamente con explicaciones y capturas de pantalla los procesos de despliegue de ambas aplicaciones en Flask, as\u00ed como las respuestas a las cuestiones planteadas.","title":"Procedimiento completo para el despliegue"},{"location":"P3.5-Flask/#cuestiones","text":"Cuestion 1 Busca, lee, entiende y explica qu\u00e9 es y para que sirve un servidor WSGI","title":"Cuestiones"},{"location":"P3.5-Flask/#tareas-de-ampliacion","text":"Ampliaci\u00f3n Despliega cualquiera de las dos aplicaciones Flask presentadas aqu\u00ed en Heroku.","title":"Tareas de ampliaci\u00f3n"},{"location":"P3.5-Flask/#referencias","text":"\u00bfQu\u00e9 es Flask? Deploy Flask The Easy Way With Gunicorn and Nginx! Deploy flask app with Nginx using Gunicorn","title":"Referencias"},{"location":"P3.5-Flask/#evaluacion","text":"Criterio Puntuaci\u00f3n Despliegue correcto, completo y bien documentado del primer ejemplo 2 puntos Despliegue correcto, completo y bien documentado del segundo ejemplo 5 puntos Respuestas correctas a las cuestiones 1 puntos Tarea de ampliaci\u00f3n 1 punto Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 2 puntos","title":"Evaluaci\u00f3n"},{"location":"P4.1-DNS/","text":"Pr\u00e1ctica 4.1 - Configuraci\u00f3n de un servidor DNS Nota importante Es muy importante que antes de empezar esta pr\u00e1ctica elimin\u00e9is las entradas que hab\u00e9is ido introduciendo hasta ahora en vuestro archivo /etc/hosts para asegurarnos que realmente la resoluci\u00f3n de nombres va a nuestro servidor DNS. Si no hac\u00e9is esto, resolver\u00e1 los nombres, pensar\u00e9is que est\u00e1 bien pero en realidad estar\u00e1 mal. Instalaci\u00f3n de servidor DNS Bind es el est\u00e1ndar de facto para servidores DNS. Es una herramienta de software libre y se distribuye con la mayor\u00eda de plataformas Unix y Linux, donde tambi\u00e9n se le conoce con el sobrenombre de named (name daemon). Bind9 es la versi\u00f3n recomendada para usarse y es la que emplearemos. Para instalar el servidor DNS en Ubuntu Server, usaremos los repositorios oficiales. Por ello, podremos instalarlo como cualquier paquete en Ubuntu: sudo apt-get install bind9 bind9utils bind9-doc Configuraci\u00f3n del servidor Puesto que en clase s\u00f3lo vamos a utilizar IPv4, vamos a dec\u00edrselo a Bind, en su archivo general de configuraci\u00f3n. Este archivo named se encuentra en el directorio: /etc/default Y para indicarle que s\u00f3lo use IPv4, debemos modificar la l\u00ednea siguiente con el texto resaltado: OPTIONS = \"-u bind -4\" El archivo de configuraci\u00f3n principal named.conf de Bind est\u00e1 en el directorio: /etc/bind Si lo consultamos veremos lo siguiente: Este archivo sirve simplemente para aglutinar o agrupar a los archivos de configuraci\u00f3n que usaremos. Estos 3 includes hacen referencia a los 3 diferentes archivos donde deberemos realizar la verdadera configuraci\u00f3n, ubicados en el mismo directorio. configuraci\u00f3n named.conf.options Es una buena pr\u00e1ctica que hag\u00e1is siempre una copia de seguridad de un archivo de configuraci\u00f3n cada vez que vay\u00e1is a realizar alg\u00fan cambio: sudo cp /etc/bind/named.conf.options /etc/bind/named.conf.options.backup Ahora editaremos el archivo named.conf.options e incluiremos los siguientes contenidos: Por motivos de seguridad, vamos a incluir una lista de acceso para que s\u00f3lo puedan hacer consultas recursivas al servidor aquellos hosts que nosotros decidamos. En nuestro caso, los hosts confiables ser\u00e1n los de la red 192.168.X.0/24 (donde la X depende de vuestra red de casa). As\u00ed pues, justo antes del bloque options {\u2026} , al principio del archivo, a\u00f1adiremos algo as\u00ed: Si nos fijamos el servidor por defecto ya viene configurado para ser un DNS cach\u00e9. El directorio donde se cachear\u00e1n o guardar\u00e1n las zonas es /var/cache/bind . /var/cache/bind Que s\u00f3lo se permitan las consultas recursivas a los hosts que hemos decidido en la lista de acceso anterior No permitir transferencia de zonas a nadie, de momento Configurar el servidor para que escuche consultas DNS en el puerto 53 (por defecto DNS utiliza puerto 53 UDP) y en la IP de su interfaz de la red privada. Deber\u00e9is colocar la IP de la interfaz de vuestra Debian , puesto que resolver\u00e1 las consultas DNS del cliente/s de esa red. Permitir las consultas recursivas, ya que en el primer punto ya le hemos dicho que s\u00f3lo puedan hacerlas los hosts de la ACL. Adem\u00e1s, vamos a comentar la l\u00ednea que pone listen-on-v6 { any; }; puesto que no vamos a responder a consultas de IPv6. Para comentarla basta a\u00f1adir al principio de la l\u00ednea dos barras // . Tambi\u00e9n podr\u00eda hacerse con una almohadilla pero aparecer\u00eda resaltado con color ya que estos comentarios los suele utilizar el administrador para aclarar alg\u00fan aspecto de la configuraci\u00f3n. Podemos comprobar si nuestra configuraci\u00f3n es correcta con el comando: Si hay alg\u00fan error, nos lo har\u00e1 saber. En caso contrario, nos devuelve a la l\u00ednea de comandos. Reiniciamos el servidor y comprobamos su estado: Configuraci\u00f3n named.conf.local En este archivo configuraremos aspectos relativos a nuestras zonas. Vamos a declarar la zona \u201cdeaw.es\u201d. Por ahora simplemente indicaremos que el servidor DNS es maestro para esta zona y donde estar\u00e1 ubicado el archivo de zona que crearemos m\u00e1s adelante: Creaci\u00f3n del archivo de zona Vamos a crear el archivo de zona de resoluci\u00f3n directa justo en el directorio que hemos indicado antes y con el mismo nombre que hemos indicado antes. El contenido ser\u00e1 algo as\u00ed (procurad respetar el formato): Recordad de teor\u00eda que los registros SOA son para detallar aspectos de la zona autoritativa, los NS para indicar los servidores DNS de la zona y los A las IPs respectivas. Donde aparecen las X deb\u00e9is poner vuestras IPs privadas correspondientes, tanto de vuestro servidor como de vuestro cliente.. Creaci\u00f3n del archivo de zona para la resoluci\u00f3n inversa Recordad que deben existir ambos archivos de zona, uno para la resoluci\u00f3n directa y otro para la inversa. Vamos pues a crear el archivo de zona inversa. En primer lugar, debemos a\u00f1adir las l\u00edneas correspondientes a esta zona inversa en el archivo named.conf.local , igual que hemos hecho antes con la zona de resoluci\u00f3n directa: Donde la X es el tercer byte de vuestra red. Y la configuraci\u00f3n de la zona de resoluci\u00f3n inversa: Podemos comprobar que la configuraci\u00f3n de las zonas es correcta con el comando adecuado. Comprobaci\u00f3n de las configuraciones Para comprobar la configuraci\u00f3n de la zona de resoluci\u00f3n directa: m Y para comprobar la configuraci\u00f3n de la zona de resoluci\u00f3n inversa: Si todo est\u00e1 bien, devolver\u00e1 OK. En caso de haber alg\u00fan error, nos informar\u00e1 de ello. Reiniciamos el servicio y comprobamos el estado: Atenci\u00f3n Es muy importante que el cliente est\u00e9 configurado para usar como servidor DNS el que acabamos de instalar y configurar. Ya sea Windows, ya sea Linux, deb\u00e9is cambiar vuestra configuraci\u00f3n de red para que la m\u00e1quina con la que hag\u00e1is las pruebas utilice este servidor DNS como el principal. Comprobaci\u00f3n de las resoluciones y de las consultas Podemos comprobar desde los clientes, con dig o nslookup las resoluciones directas e inversas: Comprobaci\u00f3n usando dig Comprobaci\u00f3n usando nslookup Tarea a realizar Configura el DNS para que resuelva el nombre de vuestro sitio web de la pr\u00e1ctica 3.5 y de la 3.2. Recuerda que con nuestra configuraci\u00f3n actual, para acceder a los sitios web necesitaremos a\u00f1adir el dominio deaw.es en el nombre. Atenci\u00f3n para el informe Documenta toda la pr\u00e1ctica con las capturas de pantalla correspondientes de cada configuraci\u00f3n y comprobaci\u00f3n.. Cuestiones finales Cuesti\u00f3n 1 \u00bfQu\u00e9 pasar\u00e1 si un cliente de una red diferente a la tuya intenta hacer uso de tu DNS de alguna manera, le funcionar\u00e1?\u00bfPor qu\u00e9, en qu\u00e9 parte de la configuraci\u00f3n puede verse? Cuesti\u00f3n 2 \u00bfPor qu\u00e9 tenemos que permitir las consultas recursivas en la configuraci\u00f3n? Cuesti\u00f3n 3 El servidor DNS que acab\u00e1is de montar, \u00bfes autoritativo?\u00bfPor qu\u00e9? Cuesti\u00f3n 4 \u00bfD\u00f3nde podemos encontrar la directiva $ORIGIN y para qu\u00e9 sirve? Cuesti\u00f3n 5 \u00bfUna zona es id\u00e9ntico a un dominio? Cuesti\u00f3n 6 \u00bfPueden editarse los archivos de zona de un servidor esclavo/secundario? Cuesti\u00f3n 7 \u00bfPor qu\u00e9 podr\u00eda querer tener m\u00e1s de un servidor esclavo para una misma zona? Cuesti\u00f3n 8 \u00bfCu\u00e1ntos servidores ra\u00edz existen? Cuesti\u00f3n 9 \u00bfQu\u00e9 es una consulta iterativa de referencia? Cuesti\u00f3n 10 En una resoluci\u00f3n inversa, \u00bfa qu\u00e9 nombre se mapear\u00eda la direcci\u00f3n IP 172.16.34.56? Evaluaci\u00f3n Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta del servidor y zona DNS 3 puntos Evidencias de las comprobaciones del correcto funcionamento 2 puntos Se ha utilizado SSH 0.5 puntos Introducci\u00f3n de IPs de ejercicios anteriores para la resoluci\u00f3n DNS 1 Cuestiones finales 2.5 puntos puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 1 punto","title":"Pr\u00e1ctica 4.1 - Configuraci\u00f3n de un servidor DNS"},{"location":"P4.1-DNS/#practica-41-configuracion-de-un-servidor-dns","text":"Nota importante Es muy importante que antes de empezar esta pr\u00e1ctica elimin\u00e9is las entradas que hab\u00e9is ido introduciendo hasta ahora en vuestro archivo /etc/hosts para asegurarnos que realmente la resoluci\u00f3n de nombres va a nuestro servidor DNS. Si no hac\u00e9is esto, resolver\u00e1 los nombres, pensar\u00e9is que est\u00e1 bien pero en realidad estar\u00e1 mal.","title":"Pr\u00e1ctica 4.1 - Configuraci\u00f3n de un servidor DNS"},{"location":"P4.1-DNS/#instalacion-de-servidor-dns","text":"Bind es el est\u00e1ndar de facto para servidores DNS. Es una herramienta de software libre y se distribuye con la mayor\u00eda de plataformas Unix y Linux, donde tambi\u00e9n se le conoce con el sobrenombre de named (name daemon). Bind9 es la versi\u00f3n recomendada para usarse y es la que emplearemos. Para instalar el servidor DNS en Ubuntu Server, usaremos los repositorios oficiales. Por ello, podremos instalarlo como cualquier paquete en Ubuntu: sudo apt-get install bind9 bind9utils bind9-doc","title":"Instalaci\u00f3n de servidor DNS"},{"location":"P4.1-DNS/#configuracion-del-servidor","text":"Puesto que en clase s\u00f3lo vamos a utilizar IPv4, vamos a dec\u00edrselo a Bind, en su archivo general de configuraci\u00f3n. Este archivo named se encuentra en el directorio: /etc/default Y para indicarle que s\u00f3lo use IPv4, debemos modificar la l\u00ednea siguiente con el texto resaltado: OPTIONS = \"-u bind -4\" El archivo de configuraci\u00f3n principal named.conf de Bind est\u00e1 en el directorio: /etc/bind Si lo consultamos veremos lo siguiente: Este archivo sirve simplemente para aglutinar o agrupar a los archivos de configuraci\u00f3n que usaremos. Estos 3 includes hacen referencia a los 3 diferentes archivos donde deberemos realizar la verdadera configuraci\u00f3n, ubicados en el mismo directorio.","title":"Configuraci\u00f3n del servidor"},{"location":"P4.1-DNS/#configuracion-namedconfoptions","text":"Es una buena pr\u00e1ctica que hag\u00e1is siempre una copia de seguridad de un archivo de configuraci\u00f3n cada vez que vay\u00e1is a realizar alg\u00fan cambio: sudo cp /etc/bind/named.conf.options /etc/bind/named.conf.options.backup Ahora editaremos el archivo named.conf.options e incluiremos los siguientes contenidos: Por motivos de seguridad, vamos a incluir una lista de acceso para que s\u00f3lo puedan hacer consultas recursivas al servidor aquellos hosts que nosotros decidamos. En nuestro caso, los hosts confiables ser\u00e1n los de la red 192.168.X.0/24 (donde la X depende de vuestra red de casa). As\u00ed pues, justo antes del bloque options {\u2026} , al principio del archivo, a\u00f1adiremos algo as\u00ed: Si nos fijamos el servidor por defecto ya viene configurado para ser un DNS cach\u00e9. El directorio donde se cachear\u00e1n o guardar\u00e1n las zonas es /var/cache/bind . /var/cache/bind Que s\u00f3lo se permitan las consultas recursivas a los hosts que hemos decidido en la lista de acceso anterior No permitir transferencia de zonas a nadie, de momento Configurar el servidor para que escuche consultas DNS en el puerto 53 (por defecto DNS utiliza puerto 53 UDP) y en la IP de su interfaz de la red privada. Deber\u00e9is colocar la IP de la interfaz de vuestra Debian , puesto que resolver\u00e1 las consultas DNS del cliente/s de esa red. Permitir las consultas recursivas, ya que en el primer punto ya le hemos dicho que s\u00f3lo puedan hacerlas los hosts de la ACL. Adem\u00e1s, vamos a comentar la l\u00ednea que pone listen-on-v6 { any; }; puesto que no vamos a responder a consultas de IPv6. Para comentarla basta a\u00f1adir al principio de la l\u00ednea dos barras // . Tambi\u00e9n podr\u00eda hacerse con una almohadilla pero aparecer\u00eda resaltado con color ya que estos comentarios los suele utilizar el administrador para aclarar alg\u00fan aspecto de la configuraci\u00f3n. Podemos comprobar si nuestra configuraci\u00f3n es correcta con el comando: Si hay alg\u00fan error, nos lo har\u00e1 saber. En caso contrario, nos devuelve a la l\u00ednea de comandos. Reiniciamos el servidor y comprobamos su estado:","title":"configuraci\u00f3n named.conf.options"},{"location":"P4.1-DNS/#configuracion-namedconflocal","text":"En este archivo configuraremos aspectos relativos a nuestras zonas. Vamos a declarar la zona \u201cdeaw.es\u201d. Por ahora simplemente indicaremos que el servidor DNS es maestro para esta zona y donde estar\u00e1 ubicado el archivo de zona que crearemos m\u00e1s adelante:","title":"Configuraci\u00f3n named.conf.local"},{"location":"P4.1-DNS/#creacion-del-archivo-de-zona","text":"Vamos a crear el archivo de zona de resoluci\u00f3n directa justo en el directorio que hemos indicado antes y con el mismo nombre que hemos indicado antes. El contenido ser\u00e1 algo as\u00ed (procurad respetar el formato): Recordad de teor\u00eda que los registros SOA son para detallar aspectos de la zona autoritativa, los NS para indicar los servidores DNS de la zona y los A las IPs respectivas. Donde aparecen las X deb\u00e9is poner vuestras IPs privadas correspondientes, tanto de vuestro servidor como de vuestro cliente..","title":"Creaci\u00f3n del archivo de zona"},{"location":"P4.1-DNS/#creacion-del-archivo-de-zona-para-la-resolucion-inversa","text":"Recordad que deben existir ambos archivos de zona, uno para la resoluci\u00f3n directa y otro para la inversa. Vamos pues a crear el archivo de zona inversa. En primer lugar, debemos a\u00f1adir las l\u00edneas correspondientes a esta zona inversa en el archivo named.conf.local , igual que hemos hecho antes con la zona de resoluci\u00f3n directa: Donde la X es el tercer byte de vuestra red. Y la configuraci\u00f3n de la zona de resoluci\u00f3n inversa: Podemos comprobar que la configuraci\u00f3n de las zonas es correcta con el comando adecuado.","title":"Creaci\u00f3n del archivo de zona para la resoluci\u00f3n inversa"},{"location":"P4.1-DNS/#comprobacion-de-las-configuraciones","text":"Para comprobar la configuraci\u00f3n de la zona de resoluci\u00f3n directa: m Y para comprobar la configuraci\u00f3n de la zona de resoluci\u00f3n inversa: Si todo est\u00e1 bien, devolver\u00e1 OK. En caso de haber alg\u00fan error, nos informar\u00e1 de ello. Reiniciamos el servicio y comprobamos el estado: Atenci\u00f3n Es muy importante que el cliente est\u00e9 configurado para usar como servidor DNS el que acabamos de instalar y configurar. Ya sea Windows, ya sea Linux, deb\u00e9is cambiar vuestra configuraci\u00f3n de red para que la m\u00e1quina con la que hag\u00e1is las pruebas utilice este servidor DNS como el principal.","title":"Comprobaci\u00f3n de las configuraciones"},{"location":"P4.1-DNS/#comprobacion-de-las-resoluciones-y-de-las-consultas","text":"Podemos comprobar desde los clientes, con dig o nslookup las resoluciones directas e inversas: Comprobaci\u00f3n usando dig Comprobaci\u00f3n usando nslookup","title":"Comprobaci\u00f3n de las resoluciones y de las consultas"},{"location":"P4.1-DNS/#tarea-a-realizar","text":"Configura el DNS para que resuelva el nombre de vuestro sitio web de la pr\u00e1ctica 3.5 y de la 3.2. Recuerda que con nuestra configuraci\u00f3n actual, para acceder a los sitios web necesitaremos a\u00f1adir el dominio deaw.es en el nombre. Atenci\u00f3n para el informe Documenta toda la pr\u00e1ctica con las capturas de pantalla correspondientes de cada configuraci\u00f3n y comprobaci\u00f3n..","title":"Tarea a realizar"},{"location":"P4.1-DNS/#cuestiones-finales","text":"Cuesti\u00f3n 1 \u00bfQu\u00e9 pasar\u00e1 si un cliente de una red diferente a la tuya intenta hacer uso de tu DNS de alguna manera, le funcionar\u00e1?\u00bfPor qu\u00e9, en qu\u00e9 parte de la configuraci\u00f3n puede verse? Cuesti\u00f3n 2 \u00bfPor qu\u00e9 tenemos que permitir las consultas recursivas en la configuraci\u00f3n? Cuesti\u00f3n 3 El servidor DNS que acab\u00e1is de montar, \u00bfes autoritativo?\u00bfPor qu\u00e9? Cuesti\u00f3n 4 \u00bfD\u00f3nde podemos encontrar la directiva $ORIGIN y para qu\u00e9 sirve? Cuesti\u00f3n 5 \u00bfUna zona es id\u00e9ntico a un dominio? Cuesti\u00f3n 6 \u00bfPueden editarse los archivos de zona de un servidor esclavo/secundario? Cuesti\u00f3n 7 \u00bfPor qu\u00e9 podr\u00eda querer tener m\u00e1s de un servidor esclavo para una misma zona? Cuesti\u00f3n 8 \u00bfCu\u00e1ntos servidores ra\u00edz existen? Cuesti\u00f3n 9 \u00bfQu\u00e9 es una consulta iterativa de referencia? Cuesti\u00f3n 10 En una resoluci\u00f3n inversa, \u00bfa qu\u00e9 nombre se mapear\u00eda la direcci\u00f3n IP 172.16.34.56?","title":"Cuestiones finales"},{"location":"P4.1-DNS/#evaluacion","text":"Criterio Puntuaci\u00f3n Configuraci\u00f3n correcta del servidor y zona DNS 3 puntos Evidencias de las comprobaciones del correcto funcionamento 2 puntos Se ha utilizado SSH 0.5 puntos Introducci\u00f3n de IPs de ejercicios anteriores para la resoluci\u00f3n DNS 1 Cuestiones finales 2.5 puntos puntos Se ha prestado especial atenci\u00f3n al formato del documento, utilizando la plantilla actualizada y haciendo un correcto uso del lenguaje t\u00e9cnico 1 punto","title":"Evaluaci\u00f3n"},{"location":"P5.1/","text":"Enunciado ejercicios Git y GitHub Repositorio DEAW Crear un repositorio en vuestro GitHub llamado DEAW . Clonar vuestro repositorio en local. README Crear (si no lo hab\u00e9is creado ya) en vuestro repositorio local un documento README.md . Note Escribir un peque\u00f1o texto en este README a prop\u00f3sito del repositorio y el m\u00f3dulo para el que se utilizar\u00e1\u00b7 Commit inicial Realizar un commit inicial con el comentario Comenzamos con los ejercicios de Git Push inicial Subir los cambios al repositorio remoto. Ignorar archivos Crear en el repositorio local un fichero llamado privado.txt . Crear en el repositorio local una carpeta llamada privada . Realizar los cambios oportunos para que tanto el archivo como la carpeta sean ignorados por git. A\u00f1adir fichero 1.txt A\u00f1adir fichero 1.txt al repositorio local. Crear el tag v0.1 Crear un tag v0.1 . Subir el tag v0.1 Subir los cambios al repositorio remoto. Cuenta de GitHub Poner una foto en vuestro perfil de GitHub. Poner el doble factor de autentificaci\u00f3n en vuestra cuenta de GitHub. Uso social de GitHub Preguntar los nombres de usuario de GitHub de 2 de tus compa\u00f1eros de clase, b\u00fascalos, y sigueles. Seguir los repositorios DEAW del resto de tus compa\u00f1eros. A\u00f1adir una estrella a los repositorios DEAW del resto de tus compa\u00f1eros. Crear una tabla Crear una tabla de este estilo en el fichero README.md con la informaci\u00f3n de varios de tus compa\u00f1eros de clase: NOMBRE GITHUB Nombre del compa\u00f1ero 1 enlace a github 1 Nombre del compa\u00f1ero 2 enlace a github 1 Nombre del compa\u00f1ero 3 enlace a github 3 Colaboradores Poner a github.com/raul-profesor como colaborador del repositorio DEAW Notas Este ejercicio es continuaci\u00f3n del anterior por lo que tendr\u00e9is que seguir trabajando en el repositorio DEAW . Tambi\u00e9n tendre\u00eds que ir poniendo los comandos que hab\u00e9is tenido que utilizar durante todos los ejercicios y las explicaciones y capturas de pantalla que consider\u00e9is necesarias en el informe. Crear una rama v0.2 Crear una rama v0.2 . Posiciona tu carpeta de trabajo en esta rama. A\u00f1adir fichero 2.txt A\u00f1adir un fichero 2.txt en la rama v0.2 . Crear rama remota v0.2 Subir los cambios al repositorio remoto. Merge directo Posicionarse en la rama master . Hacer un merge de la rama v0.2 en la rama master . Merge con conflicto En la rama master poner Hola en el fichero 1.txt y hacer commit. Posicionarse en la rama v0.2 y poner Adios en el fichero \"1.txt\" y hacer commit. Posicionarse de nuevo en la rama master y hacer un merge con la rama v0.2 Listado de ramas Listar las ramas con merge y las ramas sin merge. Arreglar conflicto Arreglar el conflicto anterior y hacer un commit. Borrar rama Crear un tag v0.2 Borrar la rama v0.2 Listado de cambios Listar los distintos commits con sus ramas y sus tags. Referencias Pro Git book, written by Scott Chacon and Ben Straub and published by Apress","title":"Ejercicios Git y Github"},{"location":"P5.1/#enunciado-ejercicios-git-y-github","text":"","title":"Enunciado ejercicios Git y GitHub"},{"location":"P5.1/#repositorio-deaw","text":"Crear un repositorio en vuestro GitHub llamado DEAW . Clonar vuestro repositorio en local.","title":"Repositorio DEAW"},{"location":"P5.1/#readme","text":"Crear (si no lo hab\u00e9is creado ya) en vuestro repositorio local un documento README.md . Note Escribir un peque\u00f1o texto en este README a prop\u00f3sito del repositorio y el m\u00f3dulo para el que se utilizar\u00e1\u00b7","title":"README"},{"location":"P5.1/#commit-inicial","text":"Realizar un commit inicial con el comentario Comenzamos con los ejercicios de Git","title":"Commit inicial"},{"location":"P5.1/#push-inicial","text":"Subir los cambios al repositorio remoto.","title":"Push inicial"},{"location":"P5.1/#ignorar-archivos","text":"Crear en el repositorio local un fichero llamado privado.txt . Crear en el repositorio local una carpeta llamada privada . Realizar los cambios oportunos para que tanto el archivo como la carpeta sean ignorados por git.","title":"Ignorar archivos"},{"location":"P5.1/#anadir-fichero-1txt","text":"A\u00f1adir fichero 1.txt al repositorio local.","title":"A\u00f1adir fichero 1.txt"},{"location":"P5.1/#crear-el-tag-v01","text":"Crear un tag v0.1 .","title":"Crear el tag v0.1"},{"location":"P5.1/#subir-el-tag-v01","text":"Subir los cambios al repositorio remoto.","title":"Subir el tag v0.1"},{"location":"P5.1/#cuenta-de-github","text":"Poner una foto en vuestro perfil de GitHub. Poner el doble factor de autentificaci\u00f3n en vuestra cuenta de GitHub.","title":"Cuenta de GitHub"},{"location":"P5.1/#uso-social-de-github","text":"Preguntar los nombres de usuario de GitHub de 2 de tus compa\u00f1eros de clase, b\u00fascalos, y sigueles. Seguir los repositorios DEAW del resto de tus compa\u00f1eros. A\u00f1adir una estrella a los repositorios DEAW del resto de tus compa\u00f1eros.","title":"Uso social de GitHub"},{"location":"P5.1/#crear-una-tabla","text":"Crear una tabla de este estilo en el fichero README.md con la informaci\u00f3n de varios de tus compa\u00f1eros de clase: NOMBRE GITHUB Nombre del compa\u00f1ero 1 enlace a github 1 Nombre del compa\u00f1ero 2 enlace a github 1 Nombre del compa\u00f1ero 3 enlace a github 3","title":"Crear una tabla"},{"location":"P5.1/#colaboradores","text":"Poner a github.com/raul-profesor como colaborador del repositorio DEAW","title":"Colaboradores"},{"location":"P5.1/#notas","text":"Este ejercicio es continuaci\u00f3n del anterior por lo que tendr\u00e9is que seguir trabajando en el repositorio DEAW . Tambi\u00e9n tendre\u00eds que ir poniendo los comandos que hab\u00e9is tenido que utilizar durante todos los ejercicios y las explicaciones y capturas de pantalla que consider\u00e9is necesarias en el informe.","title":"Notas"},{"location":"P5.1/#crear-una-rama-v02","text":"Crear una rama v0.2 . Posiciona tu carpeta de trabajo en esta rama.","title":"Crear una rama v0.2"},{"location":"P5.1/#anadir-fichero-2txt","text":"A\u00f1adir un fichero 2.txt en la rama v0.2 .","title":"A\u00f1adir fichero 2.txt"},{"location":"P5.1/#crear-rama-remota-v02","text":"Subir los cambios al repositorio remoto.","title":"Crear rama remota v0.2"},{"location":"P5.1/#merge-directo","text":"Posicionarse en la rama master . Hacer un merge de la rama v0.2 en la rama master .","title":"Merge directo"},{"location":"P5.1/#merge-con-conflicto","text":"En la rama master poner Hola en el fichero 1.txt y hacer commit. Posicionarse en la rama v0.2 y poner Adios en el fichero \"1.txt\" y hacer commit. Posicionarse de nuevo en la rama master y hacer un merge con la rama v0.2","title":"Merge con conflicto"},{"location":"P5.1/#listado-de-ramas","text":"Listar las ramas con merge y las ramas sin merge.","title":"Listado de ramas"},{"location":"P5.1/#arreglar-conflicto","text":"Arreglar el conflicto anterior y hacer un commit.","title":"Arreglar conflicto"},{"location":"P5.1/#borrar-rama","text":"Crear un tag v0.2 Borrar la rama v0.2","title":"Borrar rama"},{"location":"P5.1/#listado-de-cambios","text":"Listar los distintos commits con sus ramas y sus tags.","title":"Listado de cambios"},{"location":"P5.1/#referencias","text":"Pro Git book, written by Scott Chacon and Ben Straub and published by Apress","title":"Referencias"},{"location":"P5.2/","text":"Nota Cuando se habla de zona de intercambio temporal o zona staging , estamos hablando de un add Si ten\u00e9is dudas para realizar estos ejercicios, pod\u00e9is consultar la siguiente web Enunciados Ejercicios de creaci\u00f3n y actualizaci\u00f3n de repositorios Ejercicio 1 Configurar Git definiendo el nombre del usuario, el correo electr\u00f3nico y activar el coloreado de la salida. Mostrar la configuraci\u00f3n final. Ejercicio 2 Crear un repositorio nuevo con el nombre libro y mostrar su contenido. Ejercicio 3 Comprobar el estado del repositorio. Crear un fichero indice.txt con el siguiente contenido: Cap\u00edtulo 1: Introducci\u00f3n a Git Cap\u00edtulo 2: Flujo de trabajo b\u00e1sico Cap\u00edtulo 3: Repositorios remotos Comprobar de nuevo el estado del repositorio. A\u00f1adir el fichero a la zona de intercambio temporal. Volver a comprobar una vez m\u00e1s el estado del repositorio. Ejercicio 4 Realizar un commit de los \u00faltimos cambios con el mensaje \u201cA\u00f1adido \u00edndice del libro.\u201d y ver el estado del repositorio. Ejercicio 5 Cambiar el fichero indice.txt para que contenga lo siguiente: Cap\u00edtulo 1: Introducci\u00f3n a Git Cap\u00edtulo 2: Flujo de trabajo b\u00e1sico Cap\u00edtulo 3: Gesti\u00f3n de ramas Cap\u00edtulo 4: Repositorios remotos Mostrar los cambios con respecto a la \u00faltima versi\u00f3n guardada en el repositorio. Hacer un commit de los cambios con el mensaje \u201c A\u00f1adido cap\u00edtulo 3 sobre gesti\u00f3n de ramas \u201d. Ejercicio 6 Mostrar los cambios de la \u00faltima versi\u00f3n del repositorio con respecto a la anterior. Cambiar el mensaje del \u00faltimo commit por \u201c A\u00f1adido cap\u00edtulo 3 sobre gesti\u00f3n de ramas al \u00edndice. \u201d Volver a mostrar los \u00faltimos cambios del repositorio. Ejercicios de manejo del historial de cambios Ejercicio 1 Mostrar el historial de cambios del repositorio. Crear la carpeta capitulos y crear dentro de ella el fichero capitulo1.txt con el siguiente texto. Git es un sistema de control de versiones ideado por Linus Torvalds. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit de los cambios con el mensaje \u201cA\u00f1adido cap\u00edtulo 1.\u201d Volver a mostrar el historial de cambios del repositorio. Ejercicio 2 Crear el fichero capitulo2.txt en la carpeta capitulos con el siguiente texto. El flujo de trabajo b\u00e1sico con Git consiste en: 1- Hacer cambios en el repositorio. 2- A\u00f1adir los cambios a la zona de intercambio temporal. 3- Hacer un commit de los cambios. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit de los cambios con el mensaje \u201c A\u00f1adido cap\u00edtulo 2.\u201d Mostrar las diferencias entre la \u00faltima versi\u00f3n y dos versiones anteriores. Ejercicio 3 Crear el fichero capitulo3.txt en la carpeta capitulos con el siguiente texto. Git permite la creaci\u00f3n de ramas lo que permite tener distintas versiones del mismo proyecto y trabajar de manera simultanea en ellas. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit de los cambios con el mensaje \u201cA\u00f1adido cap\u00edtulo 3.\u201d Mostrar las diferencias entre la primera y la \u00faltima versi\u00f3n del repositorio. Ejercicio 4 A\u00f1adir al final del fichero indice.txt la siguiente l\u00ednea: Cap\u00edtulo 5: Conceptos avanzados A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit de los cambios con el mensaje \u201cA\u00f1adido cap\u00edtulo 5 al \u00edndice.\u201d. Mostrar qui\u00e9n ha hecho cambios sobre el fichero indice.txt . Ejercicios de deshacer cambios Ejercicio 1 Eliminar la \u00faltima l\u00ednea del fichero indice.txt y guardarlo. Comprobar el estado del repositorio. Deshacer los cambios realizados en el fichero indice.txt para volver a la versi\u00f3n anterior del fichero. Volver a comprobar el estado del repositorio. Ejercicio 2 Eliminar la \u00faltima l\u00ednea del fichero indice.txt y guardarlo. A\u00f1adir los cambios a la zona de intercambio temporal. Comprobar de nuevo el estado del repositorio. Quitar los cambios de la zona de intercambio temporal, pero mantenerlos en el directorio de trabajo. Comprobar de nuevo el estado del repositorio. Deshacer los cambios realizados en el fichero indice.txt para volver a la versi\u00f3n anterior del fichero. Volver a comprobar el estado del repositorio. Ejercicio 3 Eliminar la \u00faltima l\u00ednea del fichero indice.txt y guardarlo. Eliminar el fichero capitulos/capitulo3.txt . A\u00f1adir un fichero nuevo capitulos/capitulo4.txt vac\u00edo. A\u00f1adir los cambios a la zona de intercambio temporal. Comprobar de nuevo el estado del repositorio. Quitar los cambios de la zona de intercambio temporal, pero mantenerlos en el directorio de trabajo. Comprobar de nuevo el estado del repositorio. Deshacer los cambios realizados para volver a la versi\u00f3n del repositorio. Volver a comprobar el estado del repositorio. Ejercicio 4 Eliminar la \u00faltima l\u00ednea del fichero indice.txt y guardarlo. Eliminar el fichero capitulos/capitulo3.txt . A\u00f1adir los cambios a la zona de intercambio temporal y hacer un commit con el mensaje \u201c Borrado accidental. \u201d Comprobar el historial del repositorio. Deshacer el \u00faltimo commit pero mantener los cambios anteriores en el directorio de trabajo y la zona de intercambio temporal. Comprobar el historial y el estado del repositorio. Volver a hacer el commit con el mismo mensaje de antes. Deshacer el \u00faltimo commit y los cambios anteriores del directorio de trabajo volviendo a la versi\u00f3n anterior del repositorio. Comprobar de nuevo el historial y el estado del repositorio. Ejercicios de gesti\u00f3n de ramas Ejercicio 1 Crear una nueva rama bibliografia y mostrar las ramas del repositorio. Ejercicio 2 Crear el fichero capitulos/capitulo4.txt y a\u00f1adir el texto siguiente En este cap\u00edtulo veremos c\u00f3mo usar GitHub para alojar repositorios en remoto. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit con el mensaje \u201c A\u00f1adido cap\u00edtulo 4. \u201d Mostrar la historia del repositorio incluyendo todas las ramas. Ejercicio 3 Cambiar a la rama bibliografia. Crear el fichero bibliografia.txt y a\u00f1adir la siguiente referencia Chacon, S. and Straub, B. Pro Git. Apress. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit con el mensaje \u201c A\u00f1adida primera referencia bibliogr\u00e1fica. \u201d Mostrar la historia del repositorio incluyendo todas las ramas. Ejercicio 4 Fusionar la rama bibliografia con la rama master. Mostrar la historia del repositorio incluyendo todas las ramas. Eliminar la rama bibliografia. Mostrar de nuevo la historia del repositorio incluyendo todas las ramas. Ejercicio 5 Crear la rama bibliografia. Cambiar a la rama bibliografia. Cambiar el fichero bibliografia.txt para que contenga las siguientes referencias: Scott Chacon and Ben Straub. Pro Git. Apress. Ryan Hodson. Ry\u2019s Git Tutorial. Smashwords (2014) A\u00f1adir los cambios a la zona de intercambio temporal y hacer un commit con el mensaje \u201c A\u00f1adida nueva referencia bibliogr\u00e1fica. \u201d Cambiar a la rama master. Cambiar el fichero bibliografia.txt para que contenga las siguientes referencias: Chacon, S. and Straub, B. Pro Git. Apress. Loeliger, J. and McCullough, M. Version control with Git. O\u2019Reilly. A\u00f1adir los cambios a la zona de intercambio temporal y hacer un commit con el mensaje \u201c A\u00f1adida nueva referencia bibliogr\u00e1fica .\u201d Fusionar la rama bibliografia con la rama master. Resolver el conflicto dejando el fichero bibliografia.txt con las referencias: Chacon, S. and Straub, B. Pro Git. Apress. Loeliger, J. and McCullough, M. Version control with Git. O\u2019Reilly. Hodson, R. Ry\u2019s Git Tutorial. Smashwords (2014) A\u00f1adir los cambios a la zona de intercambio temporal y hacer un commit con el mensaje \u201c Resuelto conflicto de bibliograf\u00eda. \u201d Mostrar la historia del repositorio incluyendo todas las ramas. Ejercicios de repositorios remotos Ejercicio 1 Crear un nuevo repositorio p\u00fablico en GitHub con el nombre libro-git . A\u00f1adirlo al repositorio local del libro. Mostrar todos los repositorios remotos configurados. Ejercicio 2 A\u00f1adir los cambios del repositorio local al repositorio remoto de GitHub. Acceder a GitHub y comprobar que se han subido los cambios mostrando el historial de versiones. Ejercicio 3 Colaborar en el repositorio remoto libro-git de otro usuario. Clonar su repositorio libro-git . A\u00f1adir el fichero autores.txt que contenga el nombre del usuario y su correo electr\u00f3nico. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit con el mensaje \u201c A\u00f1adido autor. \u201d Subir los cambios al repositorio remoto. Ejercicio 4 Hacer una bifurcaci\u00f3n del repositorio remoto asalber/libro-git en GitHub. Clonar el repositorio creado en la cuenta de GitHub del usuario. Crear una nueva rama autoria y activarla. A\u00f1adir el nombre del usuario y su correo al fichero autores.txt . A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit con el mensaje \u201c A\u00f1adido nuevo autor. \u201d Subir los cambios de la rama autoria al repositorio remoto en GitHub. Hacer un Pull Request de los cambios en la rama autoria.","title":"Ejercicios Git y Github (II)"},{"location":"P5.2/#enunciados","text":"","title":"Enunciados"},{"location":"P5.2/#ejercicios-de-creacion-y-actualizacion-de-repositorios","text":"","title":"Ejercicios de creaci\u00f3n y actualizaci\u00f3n de repositorios"},{"location":"P5.2/#ejercicio-1","text":"Configurar Git definiendo el nombre del usuario, el correo electr\u00f3nico y activar el coloreado de la salida. Mostrar la configuraci\u00f3n final.","title":"Ejercicio 1"},{"location":"P5.2/#ejercicio-2","text":"Crear un repositorio nuevo con el nombre libro y mostrar su contenido.","title":"Ejercicio 2"},{"location":"P5.2/#ejercicio-3","text":"Comprobar el estado del repositorio. Crear un fichero indice.txt con el siguiente contenido: Cap\u00edtulo 1: Introducci\u00f3n a Git Cap\u00edtulo 2: Flujo de trabajo b\u00e1sico Cap\u00edtulo 3: Repositorios remotos Comprobar de nuevo el estado del repositorio. A\u00f1adir el fichero a la zona de intercambio temporal. Volver a comprobar una vez m\u00e1s el estado del repositorio.","title":"Ejercicio 3"},{"location":"P5.2/#ejercicio-4","text":"Realizar un commit de los \u00faltimos cambios con el mensaje \u201cA\u00f1adido \u00edndice del libro.\u201d y ver el estado del repositorio.","title":"Ejercicio 4"},{"location":"P5.2/#ejercicio-5","text":"Cambiar el fichero indice.txt para que contenga lo siguiente: Cap\u00edtulo 1: Introducci\u00f3n a Git Cap\u00edtulo 2: Flujo de trabajo b\u00e1sico Cap\u00edtulo 3: Gesti\u00f3n de ramas Cap\u00edtulo 4: Repositorios remotos Mostrar los cambios con respecto a la \u00faltima versi\u00f3n guardada en el repositorio. Hacer un commit de los cambios con el mensaje \u201c A\u00f1adido cap\u00edtulo 3 sobre gesti\u00f3n de ramas \u201d.","title":"Ejercicio 5"},{"location":"P5.2/#ejercicio-6","text":"Mostrar los cambios de la \u00faltima versi\u00f3n del repositorio con respecto a la anterior. Cambiar el mensaje del \u00faltimo commit por \u201c A\u00f1adido cap\u00edtulo 3 sobre gesti\u00f3n de ramas al \u00edndice. \u201d Volver a mostrar los \u00faltimos cambios del repositorio.","title":"Ejercicio 6"},{"location":"P5.2/#ejercicios-de-manejo-del-historial-de-cambios","text":"","title":"Ejercicios de manejo del historial de cambios"},{"location":"P5.2/#ejercicio-1_1","text":"Mostrar el historial de cambios del repositorio. Crear la carpeta capitulos y crear dentro de ella el fichero capitulo1.txt con el siguiente texto. Git es un sistema de control de versiones ideado por Linus Torvalds. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit de los cambios con el mensaje \u201cA\u00f1adido cap\u00edtulo 1.\u201d Volver a mostrar el historial de cambios del repositorio.","title":"Ejercicio 1"},{"location":"P5.2/#ejercicio-2_1","text":"Crear el fichero capitulo2.txt en la carpeta capitulos con el siguiente texto. El flujo de trabajo b\u00e1sico con Git consiste en: 1- Hacer cambios en el repositorio. 2- A\u00f1adir los cambios a la zona de intercambio temporal. 3- Hacer un commit de los cambios. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit de los cambios con el mensaje \u201c A\u00f1adido cap\u00edtulo 2.\u201d Mostrar las diferencias entre la \u00faltima versi\u00f3n y dos versiones anteriores.","title":"Ejercicio 2"},{"location":"P5.2/#ejercicio-3_1","text":"Crear el fichero capitulo3.txt en la carpeta capitulos con el siguiente texto. Git permite la creaci\u00f3n de ramas lo que permite tener distintas versiones del mismo proyecto y trabajar de manera simultanea en ellas. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit de los cambios con el mensaje \u201cA\u00f1adido cap\u00edtulo 3.\u201d Mostrar las diferencias entre la primera y la \u00faltima versi\u00f3n del repositorio.","title":"Ejercicio 3"},{"location":"P5.2/#ejercicio-4_1","text":"A\u00f1adir al final del fichero indice.txt la siguiente l\u00ednea: Cap\u00edtulo 5: Conceptos avanzados A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit de los cambios con el mensaje \u201cA\u00f1adido cap\u00edtulo 5 al \u00edndice.\u201d. Mostrar qui\u00e9n ha hecho cambios sobre el fichero indice.txt .","title":"Ejercicio 4"},{"location":"P5.2/#ejercicios-de-deshacer-cambios","text":"","title":"Ejercicios de deshacer cambios"},{"location":"P5.2/#ejercicio-1_2","text":"Eliminar la \u00faltima l\u00ednea del fichero indice.txt y guardarlo. Comprobar el estado del repositorio. Deshacer los cambios realizados en el fichero indice.txt para volver a la versi\u00f3n anterior del fichero. Volver a comprobar el estado del repositorio.","title":"Ejercicio 1"},{"location":"P5.2/#ejercicio-2_2","text":"Eliminar la \u00faltima l\u00ednea del fichero indice.txt y guardarlo. A\u00f1adir los cambios a la zona de intercambio temporal. Comprobar de nuevo el estado del repositorio. Quitar los cambios de la zona de intercambio temporal, pero mantenerlos en el directorio de trabajo. Comprobar de nuevo el estado del repositorio. Deshacer los cambios realizados en el fichero indice.txt para volver a la versi\u00f3n anterior del fichero. Volver a comprobar el estado del repositorio.","title":"Ejercicio 2"},{"location":"P5.2/#ejercicio-3_2","text":"Eliminar la \u00faltima l\u00ednea del fichero indice.txt y guardarlo. Eliminar el fichero capitulos/capitulo3.txt . A\u00f1adir un fichero nuevo capitulos/capitulo4.txt vac\u00edo. A\u00f1adir los cambios a la zona de intercambio temporal. Comprobar de nuevo el estado del repositorio. Quitar los cambios de la zona de intercambio temporal, pero mantenerlos en el directorio de trabajo. Comprobar de nuevo el estado del repositorio. Deshacer los cambios realizados para volver a la versi\u00f3n del repositorio. Volver a comprobar el estado del repositorio.","title":"Ejercicio 3"},{"location":"P5.2/#ejercicio-4_2","text":"Eliminar la \u00faltima l\u00ednea del fichero indice.txt y guardarlo. Eliminar el fichero capitulos/capitulo3.txt . A\u00f1adir los cambios a la zona de intercambio temporal y hacer un commit con el mensaje \u201c Borrado accidental. \u201d Comprobar el historial del repositorio. Deshacer el \u00faltimo commit pero mantener los cambios anteriores en el directorio de trabajo y la zona de intercambio temporal. Comprobar el historial y el estado del repositorio. Volver a hacer el commit con el mismo mensaje de antes. Deshacer el \u00faltimo commit y los cambios anteriores del directorio de trabajo volviendo a la versi\u00f3n anterior del repositorio. Comprobar de nuevo el historial y el estado del repositorio.","title":"Ejercicio 4"},{"location":"P5.2/#ejercicios-de-gestion-de-ramas","text":"","title":"Ejercicios de gesti\u00f3n de ramas"},{"location":"P5.2/#ejercicio-1_3","text":"Crear una nueva rama bibliografia y mostrar las ramas del repositorio.","title":"Ejercicio 1"},{"location":"P5.2/#ejercicio-2_3","text":"Crear el fichero capitulos/capitulo4.txt y a\u00f1adir el texto siguiente En este cap\u00edtulo veremos c\u00f3mo usar GitHub para alojar repositorios en remoto. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit con el mensaje \u201c A\u00f1adido cap\u00edtulo 4. \u201d Mostrar la historia del repositorio incluyendo todas las ramas.","title":"Ejercicio 2"},{"location":"P5.2/#ejercicio-3_3","text":"Cambiar a la rama bibliografia. Crear el fichero bibliografia.txt y a\u00f1adir la siguiente referencia Chacon, S. and Straub, B. Pro Git. Apress. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit con el mensaje \u201c A\u00f1adida primera referencia bibliogr\u00e1fica. \u201d Mostrar la historia del repositorio incluyendo todas las ramas.","title":"Ejercicio 3"},{"location":"P5.2/#ejercicio-4_3","text":"Fusionar la rama bibliografia con la rama master. Mostrar la historia del repositorio incluyendo todas las ramas. Eliminar la rama bibliografia. Mostrar de nuevo la historia del repositorio incluyendo todas las ramas.","title":"Ejercicio 4"},{"location":"P5.2/#ejercicio-5_1","text":"Crear la rama bibliografia. Cambiar a la rama bibliografia. Cambiar el fichero bibliografia.txt para que contenga las siguientes referencias: Scott Chacon and Ben Straub. Pro Git. Apress. Ryan Hodson. Ry\u2019s Git Tutorial. Smashwords (2014) A\u00f1adir los cambios a la zona de intercambio temporal y hacer un commit con el mensaje \u201c A\u00f1adida nueva referencia bibliogr\u00e1fica. \u201d Cambiar a la rama master. Cambiar el fichero bibliografia.txt para que contenga las siguientes referencias: Chacon, S. and Straub, B. Pro Git. Apress. Loeliger, J. and McCullough, M. Version control with Git. O\u2019Reilly. A\u00f1adir los cambios a la zona de intercambio temporal y hacer un commit con el mensaje \u201c A\u00f1adida nueva referencia bibliogr\u00e1fica .\u201d Fusionar la rama bibliografia con la rama master. Resolver el conflicto dejando el fichero bibliografia.txt con las referencias: Chacon, S. and Straub, B. Pro Git. Apress. Loeliger, J. and McCullough, M. Version control with Git. O\u2019Reilly. Hodson, R. Ry\u2019s Git Tutorial. Smashwords (2014) A\u00f1adir los cambios a la zona de intercambio temporal y hacer un commit con el mensaje \u201c Resuelto conflicto de bibliograf\u00eda. \u201d Mostrar la historia del repositorio incluyendo todas las ramas.","title":"Ejercicio 5"},{"location":"P5.2/#ejercicios-de-repositorios-remotos","text":"","title":"Ejercicios de repositorios remotos"},{"location":"P5.2/#ejercicio-1_4","text":"Crear un nuevo repositorio p\u00fablico en GitHub con el nombre libro-git . A\u00f1adirlo al repositorio local del libro. Mostrar todos los repositorios remotos configurados.","title":"Ejercicio 1"},{"location":"P5.2/#ejercicio-2_4","text":"A\u00f1adir los cambios del repositorio local al repositorio remoto de GitHub. Acceder a GitHub y comprobar que se han subido los cambios mostrando el historial de versiones.","title":"Ejercicio 2"},{"location":"P5.2/#ejercicio-3_4","text":"Colaborar en el repositorio remoto libro-git de otro usuario. Clonar su repositorio libro-git . A\u00f1adir el fichero autores.txt que contenga el nombre del usuario y su correo electr\u00f3nico. A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit con el mensaje \u201c A\u00f1adido autor. \u201d Subir los cambios al repositorio remoto.","title":"Ejercicio 3"},{"location":"P5.2/#ejercicio-4_4","text":"Hacer una bifurcaci\u00f3n del repositorio remoto asalber/libro-git en GitHub. Clonar el repositorio creado en la cuenta de GitHub del usuario. Crear una nueva rama autoria y activarla. A\u00f1adir el nombre del usuario y su correo al fichero autores.txt . A\u00f1adir los cambios a la zona de intercambio temporal. Hacer un commit con el mensaje \u201c A\u00f1adido nuevo autor. \u201d Subir los cambios de la rama autoria al repositorio remoto en GitHub. Hacer un Pull Request de los cambios en la rama autoria.","title":"Ejercicio 4"},{"location":"P5.3/","text":"Pr\u00e1ctica opcional Intenta resolver el m\u00e1ximo de niveles posible de esta p\u00e1gina interactiva dedicada a aprendier branching: https://learngitbranching.js.org/?locale=es_ES","title":"Git avanzado - Aprendiendo branching"},{"location":"P5.3/#practica-opcional","text":"Intenta resolver el m\u00e1ximo de niveles posible de esta p\u00e1gina interactiva dedicada a aprendier branching: https://learngitbranching.js.org/?locale=es_ES","title":"Pr\u00e1ctica opcional"},{"location":"P6.1/","text":"Pr\u00e1ctica 6.1 - Dockerizaci\u00f3n del despliegue de una aplicaci\u00f3n Node.js Introducci\u00f3n En este caso vamos a Dockerizar la aplicaci\u00f3n que ya desplegamos en la pr\u00e1ctica 3.2 . \u00bfPor qu\u00e9 dockerizar ? Si uno trata de informarse , encontrar\u00e1 m\u00faltiples y variadas razones para dockerizar nuestras aplicaciones y servicios. Por citar s\u00f3lo algunas: 1. Configuraci\u00f3n r\u00e1pida del entorno en local para el equipo de desarrollo: si todos los servicios est\u00e1n implementados con contenedores, es muy r\u00e1pida la configuraci\u00f3n de dicho entorno. 2. Evita el cl\u00e1sico \"en mi m\u00e1quina funciona\": gran parte de los problemas de desarrollo provienen de la propia configuraci\u00f3n que los integrantes del equipo de desarrollo tienen de su entorno. Con los servicios en contenedores, esto queda solucionado en gran medida. 3. Despliegues m\u00e1s r\u00e1pidos 4. Mejor control de versiones: como ya sab\u00e9is, se puede etiquetar (tags), lo que ayuda en el CONTROL DE VERSIONES. 5. Rollbacks m\u00e1s f\u00e1ciles: puesto que se tienen las cosas mas controladas por la versi\u00f3n, es m\u00e1s f\u00e1cil revertir el c\u00f3digo. A veces, simplemente apuntando a su versi\u00f3n de trabajo anterior. 6. F\u00e1cil configuraci\u00f3n de m\u00faltiples entornos: como hacen la mayor\u00eda de los equipos de desarrollo, se establece un entorno local, de integraci\u00f3n, de puesta en escena (preprod) y de producci\u00f3n. Esto se hace m\u00e1s f\u00e1cil cuando los servicios est\u00e1n en contenedores y, la mayor\u00eda de las veces, con s\u00f3lo un cambio de VARIABLES DE ENTORNO. 7. Apoyo de la comunidad: existe una fuerte comunidad de ingenieros de software que continuamente contribuyen con grandes im\u00e1genes que pueden ser reutilizadas para desarrollar un gran software. \u00bfPor qu\u00e9 reinventar la rueda, no? Despliegue con Docker En primer lugar, si eliminast\u00e9is el repositorio en su momento, deb\u00e9is volver a clonarlo en vuestra Debian, en caso contrario obviad este paso: git clone https://github.com/contentful/the-example-app.nodejs.git Ahora, puesto que la aplicaci\u00f3n ya viene con el Dockerfile necesario dentro del directorio para construir la imagen y correr el contenedor, vamos a estudiar su contenido. Tarea Completa este Dockerfile con las opciones/directivas adecuadas, leed los comentarios y pod\u00e9is apoyaros en la teor\u00eda , en este cheatsheet , en este otro o en cualquiera que encontr\u00e9is. _____ node:9 #(1) _____ /app #(2) _____ npm install -g contentful-cli #(3) _____ package.json . #(4) _____ npm install #(5) _____ . . #(6) _____ node #(7) _____ 3000 #(8) _____ [\"npm\", \"run\", \"start:dev\"] #(9) Con _____ indicamos que vamos a utilizar la imagen de Docker Hub oficial de Node, en su versi\u00f3n 9 _____ define el directorio sobre el que se ejecutar\u00e1n las subsiguientes instrucciones del Dockerfile _____ ejecuta un comando en una nueva capa de la imagen (podemos tener varios comandos _____ ) _____ como su nombre indica, copia los archivos que le indiquemos dentro del contenedor, en este caso package.json !!!info Recordemos que package.json cumpl\u00eda ciertas funciones importantes: + Centraliza la forma de interactuar con la aplicaci\u00f3n por medio de definici\u00f3n de scripts (indica comandos que podemos correr dentro de nuestro proyecto, asoci\u00e1ndolos a una palabra clave para que npm (o yarn) los reconozca cuando queramos ejecutarlos.) + Gestiona de una forma clara y sencilla las dependencias necesarias para que la aplicaci\u00f3n pueda funcionar correctamente. Con otro _____ ejecutamos el ya conocido comando que nos instala las dependencias que se indican en el archivo que hemos copiado en el paso anterior, el package.json Copiamos todos los archivos de nuestro directorio de trabajo al contenedor Con _____ le indicaremos el usuario con el que correr\u00e1 el contenedor _____ nos permite documentar que puertos est\u00e1n expuestos o a la escucha en el contenedor (s\u00f3lo ser\u00e1 accesible desde otros contenedores) Y finalmente _____ nos permite ejecutar un comando dentro del contenedor. En este caso iniciamos la aplicaci\u00f3n. Nota En Linux, cuando queremos hacer referencia al directorio actual, lo hacemos con un punto . Si dentro de nuestro directorio actual tenemos una carpeta llamada prueba , podemos hacer referencia a ella como ./prueba , ya que el . hace referencia precisamente al directorio donde nos encontramos As\u00ed pues, tener nuestra aplicaci\u00f3n corriendo es cuesti\u00f3n de un par de comandos. Hacemos un build de la imagen de Docker. Le indicamos que \u00e9sta se llama the-example-app.nodejs y que haga el build con el contexto del directorio actual de trabajo, as\u00ed como del Dockerfile que hay en \u00e9l: docker build -t the-example-app.nodejs . Y por \u00faltimo, iniciamos el contenedor con nuestra aplicaci\u00f3n. Ahora s\u00ed, con la opci\u00f3n -p , le indicamos que escuche conexiones entrantes de cualquier m\u00e1quina en el puerto 3000 de nuestra m\u00e1quina anfitri\u00f3n que haremos coincidir con el puerto 3000 del contenedor ( -p 3000:3000 ). Y con la opci\u00f3n -d lo haremos correr en modo demonio, en background: docker run -p 3000 :3000 -d the-example-app.nodejs Tras esto s\u00f3lo queda comprobar que, efectivamente, desde nuestra m\u00e1quina podemos acceder a: http://IP_Maq_Virtual:3000 y que all\u00ed est\u00e1 nuestra aplicaci\u00f3n en funcionamiento. Tarea Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final. Referencias Los beneficios de utilizar Docker y contenedores a la hora de programar Dockerizing Github","title":"Pr\u00e1ctica 6.1 - Dockerizaci\u00f3n del despliegue de una aplicaci\u00f3n con Node.js"},{"location":"P6.1/#practica-61-dockerizacion-del-despliegue-de-una-aplicacion-nodejs","text":"","title":"Pr\u00e1ctica 6.1 - Dockerizaci\u00f3n del despliegue de una aplicaci\u00f3n Node.js"},{"location":"P6.1/#introduccion","text":"En este caso vamos a Dockerizar la aplicaci\u00f3n que ya desplegamos en la pr\u00e1ctica 3.2 .","title":"Introducci\u00f3n"},{"location":"P6.1/#por-que-dockerizar","text":"Si uno trata de informarse , encontrar\u00e1 m\u00faltiples y variadas razones para dockerizar nuestras aplicaciones y servicios. Por citar s\u00f3lo algunas: 1. Configuraci\u00f3n r\u00e1pida del entorno en local para el equipo de desarrollo: si todos los servicios est\u00e1n implementados con contenedores, es muy r\u00e1pida la configuraci\u00f3n de dicho entorno. 2. Evita el cl\u00e1sico \"en mi m\u00e1quina funciona\": gran parte de los problemas de desarrollo provienen de la propia configuraci\u00f3n que los integrantes del equipo de desarrollo tienen de su entorno. Con los servicios en contenedores, esto queda solucionado en gran medida. 3. Despliegues m\u00e1s r\u00e1pidos 4. Mejor control de versiones: como ya sab\u00e9is, se puede etiquetar (tags), lo que ayuda en el CONTROL DE VERSIONES. 5. Rollbacks m\u00e1s f\u00e1ciles: puesto que se tienen las cosas mas controladas por la versi\u00f3n, es m\u00e1s f\u00e1cil revertir el c\u00f3digo. A veces, simplemente apuntando a su versi\u00f3n de trabajo anterior. 6. F\u00e1cil configuraci\u00f3n de m\u00faltiples entornos: como hacen la mayor\u00eda de los equipos de desarrollo, se establece un entorno local, de integraci\u00f3n, de puesta en escena (preprod) y de producci\u00f3n. Esto se hace m\u00e1s f\u00e1cil cuando los servicios est\u00e1n en contenedores y, la mayor\u00eda de las veces, con s\u00f3lo un cambio de VARIABLES DE ENTORNO. 7. Apoyo de la comunidad: existe una fuerte comunidad de ingenieros de software que continuamente contribuyen con grandes im\u00e1genes que pueden ser reutilizadas para desarrollar un gran software. \u00bfPor qu\u00e9 reinventar la rueda, no?","title":"\u00bfPor qu\u00e9 dockerizar?"},{"location":"P6.1/#despliegue-con-docker","text":"En primer lugar, si eliminast\u00e9is el repositorio en su momento, deb\u00e9is volver a clonarlo en vuestra Debian, en caso contrario obviad este paso: git clone https://github.com/contentful/the-example-app.nodejs.git Ahora, puesto que la aplicaci\u00f3n ya viene con el Dockerfile necesario dentro del directorio para construir la imagen y correr el contenedor, vamos a estudiar su contenido. Tarea Completa este Dockerfile con las opciones/directivas adecuadas, leed los comentarios y pod\u00e9is apoyaros en la teor\u00eda , en este cheatsheet , en este otro o en cualquiera que encontr\u00e9is. _____ node:9 #(1) _____ /app #(2) _____ npm install -g contentful-cli #(3) _____ package.json . #(4) _____ npm install #(5) _____ . . #(6) _____ node #(7) _____ 3000 #(8) _____ [\"npm\", \"run\", \"start:dev\"] #(9) Con _____ indicamos que vamos a utilizar la imagen de Docker Hub oficial de Node, en su versi\u00f3n 9 _____ define el directorio sobre el que se ejecutar\u00e1n las subsiguientes instrucciones del Dockerfile _____ ejecuta un comando en una nueva capa de la imagen (podemos tener varios comandos _____ ) _____ como su nombre indica, copia los archivos que le indiquemos dentro del contenedor, en este caso package.json !!!info Recordemos que package.json cumpl\u00eda ciertas funciones importantes: + Centraliza la forma de interactuar con la aplicaci\u00f3n por medio de definici\u00f3n de scripts (indica comandos que podemos correr dentro de nuestro proyecto, asoci\u00e1ndolos a una palabra clave para que npm (o yarn) los reconozca cuando queramos ejecutarlos.) + Gestiona de una forma clara y sencilla las dependencias necesarias para que la aplicaci\u00f3n pueda funcionar correctamente. Con otro _____ ejecutamos el ya conocido comando que nos instala las dependencias que se indican en el archivo que hemos copiado en el paso anterior, el package.json Copiamos todos los archivos de nuestro directorio de trabajo al contenedor Con _____ le indicaremos el usuario con el que correr\u00e1 el contenedor _____ nos permite documentar que puertos est\u00e1n expuestos o a la escucha en el contenedor (s\u00f3lo ser\u00e1 accesible desde otros contenedores) Y finalmente _____ nos permite ejecutar un comando dentro del contenedor. En este caso iniciamos la aplicaci\u00f3n. Nota En Linux, cuando queremos hacer referencia al directorio actual, lo hacemos con un punto . Si dentro de nuestro directorio actual tenemos una carpeta llamada prueba , podemos hacer referencia a ella como ./prueba , ya que el . hace referencia precisamente al directorio donde nos encontramos As\u00ed pues, tener nuestra aplicaci\u00f3n corriendo es cuesti\u00f3n de un par de comandos. Hacemos un build de la imagen de Docker. Le indicamos que \u00e9sta se llama the-example-app.nodejs y que haga el build con el contexto del directorio actual de trabajo, as\u00ed como del Dockerfile que hay en \u00e9l: docker build -t the-example-app.nodejs . Y por \u00faltimo, iniciamos el contenedor con nuestra aplicaci\u00f3n. Ahora s\u00ed, con la opci\u00f3n -p , le indicamos que escuche conexiones entrantes de cualquier m\u00e1quina en el puerto 3000 de nuestra m\u00e1quina anfitri\u00f3n que haremos coincidir con el puerto 3000 del contenedor ( -p 3000:3000 ). Y con la opci\u00f3n -d lo haremos correr en modo demonio, en background: docker run -p 3000 :3000 -d the-example-app.nodejs Tras esto s\u00f3lo queda comprobar que, efectivamente, desde nuestra m\u00e1quina podemos acceder a: http://IP_Maq_Virtual:3000 y que all\u00ed est\u00e1 nuestra aplicaci\u00f3n en funcionamiento. Tarea Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final.","title":"Despliegue con Docker"},{"location":"P6.1/#referencias","text":"Los beneficios de utilizar Docker y contenedores a la hora de programar Dockerizing Github","title":"Referencias"},{"location":"P6.2/","text":"Pr\u00e1ctica 6.2 - Despliegue de una aplicaci\u00f3n PHP con Nginx y MySQL usando Docker y docker-compose Introducci\u00f3n \u00a1Atenci\u00f3n! En caso de que teng\u00e1is problemas, esta pr\u00e1ctica est\u00e1 comprobada y funcionando usando las siguientes versiones: Docker: Docker version 20.10.17, build 100c701 Docker-compose: Docker Compose version v2.10.2 Recordando qu\u00e9 es docker-compose Como vimos en la parte de teor\u00eda para ejecutar nuestra aplicaci\u00f3n en docker creamos un fichero llamado Dockerfile y este fichero contiene una configuraci\u00f3n. Esta configuraci\u00f3n var\u00eda dependiendo de qu\u00e9 queremos poner en el contenedor, ya que no es lo mismo poner una p\u00e1gina web, que una base de datos. Este proceso, de crear todos los Dockerfile y ejecutarlos puede ser bastante tedioso, ya que debemos pensar que una aplicaci\u00f3n de tama\u00f1o mediano es probable que tenga un front end, un back end, quiz\u00e1 algunos background-workers as\u00ed como la base de datos, sistema de cach\u00e9, sistema de colas o de message-broker... por lo que cada uno de nuestros servicios ser\u00e1 un contenedor diferente. Por lo tanto, crear m\u00faltiples Dockerfile y ejecutarlos todo en un script queda largo y feo. Aqu\u00ed es donde entra docker-compose el cual es una herramienta que nos permite definir y correr m\u00faltiples contenedores en Docker. Estos m\u00faltiples contenedores se definen en un fichero denominado docker-compose con la extensi\u00f3n .yml. Luego, con un solo comando, crea e inicia todos los servicios desde su configuraci\u00f3n. Compose funciona en todos los entornos: producci\u00f3n, puesta en escena, desarrollo, pruebas, as\u00ed como flujos de trabajo de CI. Usar Compose es b\u00e1sicamente un proceso de tres pasos: Definir el entorno de nuestra aplicaci\u00f3n con un Dockerfile para que pueda reproducirse en cualquier lugar. Definir los servicios que componen la aplicaci\u00f3n docker-compose.yml para que puedan ejecutarse juntos en un entorno aislado. Ejecutar docker-compose up y Compose inicia y ejecuta toda su aplicaci\u00f3n. Este proceso se denomina orquestaci\u00f3n de contenedores y se lleva a cabo de forma local al interior de los containers, quienes, adem\u00e1s, se encontrar\u00e1n unidos a trav\u00e9s de una red de Docker. Instalaci\u00f3n de docker-compose Proceso de dockerizaci\u00f3n de Nginx+PHP+MySQL 1. Estructura de directorios Para que quede claro todo el proceso que vamos a seguir, la estructura de directorios que nos debe quedar en nuestra Debian al finalizar la pr\u00e1ctica es esta: /usuario/home/practica6-2/ \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 nginx \u2502 \u251c\u2500\u2500 default.conf \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 php \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 www \u2514\u2500\u2500 html \u2514\u2500\u2500 index.php Pod\u00e9is ir creando los directorios y archivos paso a paso o crearlo todo a la vez y luego ir rellenando los archivos vac\u00edos siguiendo un procedimiento como este: mkdir practica6-2 cd practica6-2 touch docker-compose.yml mkdir nginx touch nginx/default.conf ... 2. Creaci\u00f3n de un contenedor Nginx Paara empezar, necesitamos crear y correr un contendor Nginx que permita alojar nuestra aplicaci\u00f3n en PHP. Dentro de la carpeta /usuario/home/practica6-2/ debemos haber creado o crear ahora el archivo docker-compose.yml Y editamos este archivo con el editor de texto que prefiramos, nano por ejemplo: nano docker-compose.yml Y a\u00f1adimos la siguientes l\u00edneas: nginx: image: nginx:latest container_name: nginx-container ports: - 80:80 Y lo guardamos. El archivo que acabamos de crear ser\u00e1 el encargado de descargarse la \u00faltima versi\u00f3n de la imagen de Nginx, crear un contenedor con ella y publicar o escuchar en el puerto 80 del contenedor que tambi\u00e9n se corresponder\u00e1 con el 80 de nuestra m\u00e1quina (80:80). Iniciemos entonces este proceso: docker-compose up -d Con la opci\u00f3n -d (de daemon), estamos indicando que el contenedor se ejecute en background o segundo plano: Para comprobar que el contenedor est\u00e1 corriendo, podemos hacer: docker ps Y deber\u00edamos ver algo como: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c6641e4d5bbf nginx:latest \"/docker-entrypoint.\u2026\" 5 seconds ago Up 3 seconds 0.0.0.0:80->80/tcp, :::80->80/tcp nginx-container Adem\u00e1s, si abrimos el navegador de nuestra m\u00e1quina anfitri\u00f3n y accedemos a http://IP_Maq_Virtual deber\u00edamos ver la p\u00e1gina de bienvenida de Nginx: 3. Creaci\u00f3n de un contenedor PHP Creamos la carpeta y el documento pertinente dentro de ella, si no lo hab\u00edamos hecho antes: mkdir -p /home/usuario/practica6-2/www/html nano /home/usuario/practica6-2/www/html/index.php Y dentro de index.php a\u00f1adimos el siguiente c\u00f3digo: <!DOCTYPE html> < head > < title > \u00a1Hola mundo! </ title > </ head > < body > < h1 > \u00a1Hola mundo! </ h1 > < p > <?php echo 'Estamos corriendo PHP, version: ' . phpversion(); ?> </ p > </ body > Guardad el archivo y cread, si no lo hab\u00edais hecho antes, un directorio llamado nginx dentro del directorio del proyecto: mkdir /home/usuario/practica6-2/nginx Ahora vamos a crear el archivo de configuraci\u00f3n por defecto para que Nginx pueda correr la aplicaci\u00f3n PHP: nano /home/usuario/practica6-2/nginx/default.conf Y dentro de ese archivo, colocaremos la siguiente configuraci\u00f3n: server { listen 80 default_server; root /var/www/html ; index index.html index.php; charset utf-8; location / { try_files $uri $uri/ /index.php ?$query_string; } location = /favicon.ico { access_log off ; log_not_found off ; } location = /robots.txt { access_log off ; log_not_found off ; } access_log off ; error_log /var/log/nginx/error.log error ; sendfile off ; client_max_body_size 100 m; location ~ .php$ { fastcgi_split_path_info ^(.+.php)(/.+)$; fastcgi_pass php:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_intercept_errors off ; fastcgi_buffer_size 16 k; fastcgi_buffers 4 16 k; } location ~ /.ht { deny all ; } } Guardamos el archivo y ahora crearemos el Dockerfile dentro del directorio nginx . En este archivo se copiar\u00e1 el archivo de configuraci\u00f3n de Nginx al contenedor correspondiente. As\u00ed pues: nano /home/usuario/practica6-2/nginx/Dockerfile Y dentro de este archivo: FROM nginx:latest COPY ./default.conf /etc/nginx/conf.d/default.conf Y ahora editamos nuestro archivo docker-compose.yml : services: nginx: build: ./nginx/ container_name: nginx-container ports: - 80 :80 links: - php volumes: - ./www/html/:/var/www/html/ php: image: php:7.0-fpm container_name: php-container expose: - 9000 volumes: - ./www/html/:/var/www/html/ Ahora con este fichero docker-compose.yml se crear\u00e1 un nuevo contenedor PHP-FPM en el puerto 9000, enlazar\u00e1 el contenedor nginx con el contendor php , as\u00ed como crear\u00e1 un volumen y lo montar\u00e1 en el directorio /var/www/html de los contenedores. As\u00ed pues, ejecutaremos el nuevo contenedor volviendo a ejecutando compose. Cuidado pues se debe ejecutar el comando en el mismo directorio donde tengamos nuestro archivo docker-compose.yml : cd /home/usuario/practica6-2 docker-compose up -d Y comprobamos que los contenedores est\u00e1n corriendo: docker ps Debiendo ver algo como: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 82c8baf15221 docker-project_nginx \"/docker-entrypoint.\u2026\" 23 seconds ago Up 22 seconds 0.0.0.0:80->80/tcp, :::80->80/tcp nginx-container 10778c6686d8 php:7.0-fpm \"docker-php-entrypoi\u2026\" 25 seconds ago Up 23 seconds 9000/tcp php-container Y si ahora volvemos a acceder a http://IP_Maq_Virtual , veremos la p\u00e1gina Hola mundo : 4. Creaci\u00f3n de un contenedor para datos Como v\u00e9is, hemos montado el directorio www/html en ambos contenedores, el de nginx y el de php. Sin embargo, esta no es una forma adecuada de hacerlo. En este paso crearemos un contenedor independiente que se encargar\u00e1 de contener los datos y lo enlazaremos con el resto de contenedores. Para llevar a cabo esta tarea, volvemos a editar el docker-compose.yml : nano /usuario/home/practica6-2/docker-compose.yml Y a\u00f1adiremos un nuevo servicio a los que ya ten\u00edamos, quedando as\u00ed: nginx: build: ./nginx/ container_name: nginx-container ports: - 80 :80 links: - php volumes_from: - app-data php: image: php:7.0-fpm container_name: php-container expose: - 9000 volumes_from: - app-data app-data: image: php:7.0-fpm container_name: app-data-container volumes: - ./www/html/:/var/www/html/ command: \"true\" As\u00ed que para recrear y lanzar todos los contenedores ejecutamos de nuevo (recordad, dentro del directorio donde se encuentra el archivo): docker-compose up -d Y volvemos a verificar que est\u00e1n corriendo todos: docker ps -a Debiendo ver algo como: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 849315c7ffc0 docker-project_nginx \"/docker-entrypoint.\u2026\" 27 seconds ago Up 25 seconds 0.0.0.0:80->80/tcp, :::80->80/tcp nginx-container 59a0d7040fd8 php:7.0-fpm \"docker-php-entrypoi\u2026\" 28 seconds ago Up 27 seconds 9000/tcp php-container fbca95944234 php:7.0-fpm \"docker-php-entrypoi\u2026\" 29 seconds ago Exited (0) 28 seconds ago app-data-container 5. Creaci\u00f3n de un contenedor MySQL En esta secci\u00f3n crearemos un contenedor de una base de datos MySQL y lo enlazaremos con el resto de contenedores. Primero, modificaremos la imagen PHP e instalaremos la extensi\u00f3n PHP para MySQL, de tal forma que nos permita conectarnos desde nuestra aplicaci\u00f3n PHP a nuestra BBDD MySQL. Creamos, si no lo ten\u00edamos ya, nuestro directorio php y dentro de \u00e9l, el archivo Dockerfile : mkdir /home/usuario/practica6-2/php nano /home/usuario/practica6-2/php/Dockerfile Y dentro del Dockerfile ponemos: FROM php:7.0-fpm RUN docker-php-ext-install pdo_mysql Y una vez m\u00e1s, debemos editar docker-compose.yml con el objetivo de que se creen el contenedor para MySQL y el contenedor de los datos de MySQL que contendr\u00e1 la base de datos y las tablas: services: nginx: build: ./nginx/ container_name: nginx-container ports: - 80 :80 links: - php volumes_from: - app-data php: build: ./php/ container_name: php-container expose: - 9000 links: - mysql volumes_from: - app-data app-data: image: php:7.0-fpm container_name: app-data-container volumes: - ./www/html/:/var/www/html/ command: \"true\" mysql: image: mysql:5.7 container_name: mysql-container volumes_from: - mysql-data environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: mydb MYSQL_USER: myuser MYSQL_PASSWORD: password mysql-data: image: mysql:5.7 container_name: mysql-data-container volumes: - /var/lib/mysql command: \"true\" Despu\u00e9s de guardar este archivo, editamos el archivo index.php y hacemos algunos cambios para comprobar la conexi\u00f3n a la base de datos. El archivo index.php debe quedar as\u00ed: <!DOCTYPE html> < head > < title > \u00a1Hola mundo! </ title > </ head > < body > < h1 > \u00a1Hola mundo! </ h1 > < p > <?php echo 'Estamos corriendo PHP, version: ' . phpversion(); ?> </ p > <? $database =\"mydb\"; $user = \"myuser\"; $password = \"password\"; $host = \"mysql\"; $connection = new PDO(\"mysql:host={$host};dbname={$database};charset=utf8\", $user, $password); $query = $connection->query(\"SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_TYPE='BASE TABLE'\"); $tables = $query->fetchAll(PDO::FETCH_COLUMN); if (empty($tables)) { echo \"<p>No hay tablas en la base de datos \\\"{$database}\\\".</p>\"; } else { echo \"<p>La base de datos \\\"{$database}\\\" tiene las siguientes tablas:</p>\"; echo \"<ul>\"; foreach ($tables as $table) { echo \"<li>{$table}</li>\"; } echo \"</ul>\"; } ?> </ body > </ html > Guardad el archivo y lanzad los contenedores una vez m\u00e1s: docker-compose up -d Y verificamos que est\u00e1n ejecut\u00e1ndose: docker ps -a Y veremos: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d3e82747fe0d mysql:5.7 \"docker-entrypoint.s\u2026\" 39 seconds ago Up 38 seconds 3306/tcp, 33060/tcp mysql-container 606320e5a7f8 mysql:5.7 \"docker-entrypoint.s\u2026\" 41 seconds ago Exited (0) 39 seconds ago mysql-data-container ca4f63797d11 docker-project_php \"docker-php-entrypoi\u2026\" 2 hours ago Up 2 hours 9000/tcp php-container 849315c7ffc0 docker-project_nginx \"/docker-entrypoint.\u2026\" 2 hours ago Up 2 hours 0.0.0.0:80->80/tcp, :::80->80/tcp nginx-container fbca95944234 php:7.0-fpm \"docker-php-entrypoi\u2026\" 2 hours ago Exited (0) 39 seconds ago app-data- 6. Verificaci\u00f3n de conexi\u00f3n a la base de datos Si ahora accedemos a http://IP_Maq_Virtual , deber\u00edamos obtener la siguiente pantalla: Como pod\u00e9is ver, nos dice que no tenemos ninguna tabla en la base de datos mydb . Sin embargo, el hecho es que realmente s\u00ed existen algunas tablas, s\u00edmplemente no son visibles para un usuario normal. Si quisi\u00e9ramos verlas, debemos editar el archivo index.php y cambiar $user por root y $password a secret . Es decir: nano /home/usuario/www/html/index.php Y cambiar las l\u00edneas: $user = \"root\"; $password = \"secret\"; Guardad el archivo y refrescad la p\u00e1gina. Deber\u00edas obtener ahora una pantalla con todas las tablas de la base de datos, tal que as\u00ed: Tarea Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final. Referencias \u00bfQu\u00e9 es Docker Compose? \u00bfQu\u00e9 demonios es Docker y Docker-Compose? y c\u00f3mo Dockerizar Dotnet Core WebApi y SQL Server en un ambiente de desarrollo ideal Introducci\u00f3n a docker-compose How to Deploy a PHP Application with Nginx and MySQL Using Docker and Docker Compose","title":"Pr\u00e1ctica 6.2 - Despliegue de una aplicaci\u00f3n PHP con Nginx y MySQL usando Docker y docker-compose"},{"location":"P6.2/#practica-62-despliegue-de-una-aplicacion-php-con-nginx-y-mysql-usando-docker-y-docker-compose","text":"","title":"Pr\u00e1ctica 6.2 - Despliegue de una aplicaci\u00f3n PHP con Nginx y MySQL usando Docker y docker-compose"},{"location":"P6.2/#introduccion","text":"\u00a1Atenci\u00f3n! En caso de que teng\u00e1is problemas, esta pr\u00e1ctica est\u00e1 comprobada y funcionando usando las siguientes versiones: Docker: Docker version 20.10.17, build 100c701 Docker-compose: Docker Compose version v2.10.2","title":"Introducci\u00f3n"},{"location":"P6.2/#recordando-que-es-docker-compose","text":"Como vimos en la parte de teor\u00eda para ejecutar nuestra aplicaci\u00f3n en docker creamos un fichero llamado Dockerfile y este fichero contiene una configuraci\u00f3n. Esta configuraci\u00f3n var\u00eda dependiendo de qu\u00e9 queremos poner en el contenedor, ya que no es lo mismo poner una p\u00e1gina web, que una base de datos. Este proceso, de crear todos los Dockerfile y ejecutarlos puede ser bastante tedioso, ya que debemos pensar que una aplicaci\u00f3n de tama\u00f1o mediano es probable que tenga un front end, un back end, quiz\u00e1 algunos background-workers as\u00ed como la base de datos, sistema de cach\u00e9, sistema de colas o de message-broker... por lo que cada uno de nuestros servicios ser\u00e1 un contenedor diferente. Por lo tanto, crear m\u00faltiples Dockerfile y ejecutarlos todo en un script queda largo y feo. Aqu\u00ed es donde entra docker-compose el cual es una herramienta que nos permite definir y correr m\u00faltiples contenedores en Docker. Estos m\u00faltiples contenedores se definen en un fichero denominado docker-compose con la extensi\u00f3n .yml. Luego, con un solo comando, crea e inicia todos los servicios desde su configuraci\u00f3n. Compose funciona en todos los entornos: producci\u00f3n, puesta en escena, desarrollo, pruebas, as\u00ed como flujos de trabajo de CI. Usar Compose es b\u00e1sicamente un proceso de tres pasos: Definir el entorno de nuestra aplicaci\u00f3n con un Dockerfile para que pueda reproducirse en cualquier lugar. Definir los servicios que componen la aplicaci\u00f3n docker-compose.yml para que puedan ejecutarse juntos en un entorno aislado. Ejecutar docker-compose up y Compose inicia y ejecuta toda su aplicaci\u00f3n. Este proceso se denomina orquestaci\u00f3n de contenedores y se lleva a cabo de forma local al interior de los containers, quienes, adem\u00e1s, se encontrar\u00e1n unidos a trav\u00e9s de una red de Docker.","title":"Recordando qu\u00e9 es docker-compose"},{"location":"P6.2/#instalacion-de-docker-compose","text":"","title":"Instalaci\u00f3n de docker-compose"},{"location":"P6.2/#proceso-de-dockerizacion-de-nginxphpmysql","text":"","title":"Proceso de dockerizaci\u00f3n de Nginx+PHP+MySQL"},{"location":"P6.2/#1-estructura-de-directorios","text":"Para que quede claro todo el proceso que vamos a seguir, la estructura de directorios que nos debe quedar en nuestra Debian al finalizar la pr\u00e1ctica es esta: /usuario/home/practica6-2/ \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 nginx \u2502 \u251c\u2500\u2500 default.conf \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 php \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 www \u2514\u2500\u2500 html \u2514\u2500\u2500 index.php Pod\u00e9is ir creando los directorios y archivos paso a paso o crearlo todo a la vez y luego ir rellenando los archivos vac\u00edos siguiendo un procedimiento como este: mkdir practica6-2 cd practica6-2 touch docker-compose.yml mkdir nginx touch nginx/default.conf ...","title":"1. Estructura de directorios"},{"location":"P6.2/#2-creacion-de-un-contenedor-nginx","text":"Paara empezar, necesitamos crear y correr un contendor Nginx que permita alojar nuestra aplicaci\u00f3n en PHP. Dentro de la carpeta /usuario/home/practica6-2/ debemos haber creado o crear ahora el archivo docker-compose.yml Y editamos este archivo con el editor de texto que prefiramos, nano por ejemplo: nano docker-compose.yml Y a\u00f1adimos la siguientes l\u00edneas: nginx: image: nginx:latest container_name: nginx-container ports: - 80:80 Y lo guardamos. El archivo que acabamos de crear ser\u00e1 el encargado de descargarse la \u00faltima versi\u00f3n de la imagen de Nginx, crear un contenedor con ella y publicar o escuchar en el puerto 80 del contenedor que tambi\u00e9n se corresponder\u00e1 con el 80 de nuestra m\u00e1quina (80:80). Iniciemos entonces este proceso: docker-compose up -d Con la opci\u00f3n -d (de daemon), estamos indicando que el contenedor se ejecute en background o segundo plano: Para comprobar que el contenedor est\u00e1 corriendo, podemos hacer: docker ps Y deber\u00edamos ver algo como: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c6641e4d5bbf nginx:latest \"/docker-entrypoint.\u2026\" 5 seconds ago Up 3 seconds 0.0.0.0:80->80/tcp, :::80->80/tcp nginx-container Adem\u00e1s, si abrimos el navegador de nuestra m\u00e1quina anfitri\u00f3n y accedemos a http://IP_Maq_Virtual deber\u00edamos ver la p\u00e1gina de bienvenida de Nginx:","title":"2. Creaci\u00f3n de un contenedor Nginx"},{"location":"P6.2/#3-creacion-de-un-contenedor-php","text":"Creamos la carpeta y el documento pertinente dentro de ella, si no lo hab\u00edamos hecho antes: mkdir -p /home/usuario/practica6-2/www/html nano /home/usuario/practica6-2/www/html/index.php Y dentro de index.php a\u00f1adimos el siguiente c\u00f3digo: <!DOCTYPE html> < head > < title > \u00a1Hola mundo! </ title > </ head > < body > < h1 > \u00a1Hola mundo! </ h1 > < p > <?php echo 'Estamos corriendo PHP, version: ' . phpversion(); ?> </ p > </ body > Guardad el archivo y cread, si no lo hab\u00edais hecho antes, un directorio llamado nginx dentro del directorio del proyecto: mkdir /home/usuario/practica6-2/nginx Ahora vamos a crear el archivo de configuraci\u00f3n por defecto para que Nginx pueda correr la aplicaci\u00f3n PHP: nano /home/usuario/practica6-2/nginx/default.conf Y dentro de ese archivo, colocaremos la siguiente configuraci\u00f3n: server { listen 80 default_server; root /var/www/html ; index index.html index.php; charset utf-8; location / { try_files $uri $uri/ /index.php ?$query_string; } location = /favicon.ico { access_log off ; log_not_found off ; } location = /robots.txt { access_log off ; log_not_found off ; } access_log off ; error_log /var/log/nginx/error.log error ; sendfile off ; client_max_body_size 100 m; location ~ .php$ { fastcgi_split_path_info ^(.+.php)(/.+)$; fastcgi_pass php:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_intercept_errors off ; fastcgi_buffer_size 16 k; fastcgi_buffers 4 16 k; } location ~ /.ht { deny all ; } } Guardamos el archivo y ahora crearemos el Dockerfile dentro del directorio nginx . En este archivo se copiar\u00e1 el archivo de configuraci\u00f3n de Nginx al contenedor correspondiente. As\u00ed pues: nano /home/usuario/practica6-2/nginx/Dockerfile Y dentro de este archivo: FROM nginx:latest COPY ./default.conf /etc/nginx/conf.d/default.conf Y ahora editamos nuestro archivo docker-compose.yml : services: nginx: build: ./nginx/ container_name: nginx-container ports: - 80 :80 links: - php volumes: - ./www/html/:/var/www/html/ php: image: php:7.0-fpm container_name: php-container expose: - 9000 volumes: - ./www/html/:/var/www/html/ Ahora con este fichero docker-compose.yml se crear\u00e1 un nuevo contenedor PHP-FPM en el puerto 9000, enlazar\u00e1 el contenedor nginx con el contendor php , as\u00ed como crear\u00e1 un volumen y lo montar\u00e1 en el directorio /var/www/html de los contenedores. As\u00ed pues, ejecutaremos el nuevo contenedor volviendo a ejecutando compose. Cuidado pues se debe ejecutar el comando en el mismo directorio donde tengamos nuestro archivo docker-compose.yml : cd /home/usuario/practica6-2 docker-compose up -d Y comprobamos que los contenedores est\u00e1n corriendo: docker ps Debiendo ver algo como: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 82c8baf15221 docker-project_nginx \"/docker-entrypoint.\u2026\" 23 seconds ago Up 22 seconds 0.0.0.0:80->80/tcp, :::80->80/tcp nginx-container 10778c6686d8 php:7.0-fpm \"docker-php-entrypoi\u2026\" 25 seconds ago Up 23 seconds 9000/tcp php-container Y si ahora volvemos a acceder a http://IP_Maq_Virtual , veremos la p\u00e1gina Hola mundo :","title":"3. Creaci\u00f3n de un contenedor PHP"},{"location":"P6.2/#4-creacion-de-un-contenedor-para-datos","text":"Como v\u00e9is, hemos montado el directorio www/html en ambos contenedores, el de nginx y el de php. Sin embargo, esta no es una forma adecuada de hacerlo. En este paso crearemos un contenedor independiente que se encargar\u00e1 de contener los datos y lo enlazaremos con el resto de contenedores. Para llevar a cabo esta tarea, volvemos a editar el docker-compose.yml : nano /usuario/home/practica6-2/docker-compose.yml Y a\u00f1adiremos un nuevo servicio a los que ya ten\u00edamos, quedando as\u00ed: nginx: build: ./nginx/ container_name: nginx-container ports: - 80 :80 links: - php volumes_from: - app-data php: image: php:7.0-fpm container_name: php-container expose: - 9000 volumes_from: - app-data app-data: image: php:7.0-fpm container_name: app-data-container volumes: - ./www/html/:/var/www/html/ command: \"true\" As\u00ed que para recrear y lanzar todos los contenedores ejecutamos de nuevo (recordad, dentro del directorio donde se encuentra el archivo): docker-compose up -d Y volvemos a verificar que est\u00e1n corriendo todos: docker ps -a Debiendo ver algo como: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 849315c7ffc0 docker-project_nginx \"/docker-entrypoint.\u2026\" 27 seconds ago Up 25 seconds 0.0.0.0:80->80/tcp, :::80->80/tcp nginx-container 59a0d7040fd8 php:7.0-fpm \"docker-php-entrypoi\u2026\" 28 seconds ago Up 27 seconds 9000/tcp php-container fbca95944234 php:7.0-fpm \"docker-php-entrypoi\u2026\" 29 seconds ago Exited (0) 28 seconds ago app-data-container","title":"4. Creaci\u00f3n de un contenedor para datos"},{"location":"P6.2/#5-creacion-de-un-contenedor-mysql","text":"En esta secci\u00f3n crearemos un contenedor de una base de datos MySQL y lo enlazaremos con el resto de contenedores. Primero, modificaremos la imagen PHP e instalaremos la extensi\u00f3n PHP para MySQL, de tal forma que nos permita conectarnos desde nuestra aplicaci\u00f3n PHP a nuestra BBDD MySQL. Creamos, si no lo ten\u00edamos ya, nuestro directorio php y dentro de \u00e9l, el archivo Dockerfile : mkdir /home/usuario/practica6-2/php nano /home/usuario/practica6-2/php/Dockerfile Y dentro del Dockerfile ponemos: FROM php:7.0-fpm RUN docker-php-ext-install pdo_mysql Y una vez m\u00e1s, debemos editar docker-compose.yml con el objetivo de que se creen el contenedor para MySQL y el contenedor de los datos de MySQL que contendr\u00e1 la base de datos y las tablas: services: nginx: build: ./nginx/ container_name: nginx-container ports: - 80 :80 links: - php volumes_from: - app-data php: build: ./php/ container_name: php-container expose: - 9000 links: - mysql volumes_from: - app-data app-data: image: php:7.0-fpm container_name: app-data-container volumes: - ./www/html/:/var/www/html/ command: \"true\" mysql: image: mysql:5.7 container_name: mysql-container volumes_from: - mysql-data environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: mydb MYSQL_USER: myuser MYSQL_PASSWORD: password mysql-data: image: mysql:5.7 container_name: mysql-data-container volumes: - /var/lib/mysql command: \"true\" Despu\u00e9s de guardar este archivo, editamos el archivo index.php y hacemos algunos cambios para comprobar la conexi\u00f3n a la base de datos. El archivo index.php debe quedar as\u00ed: <!DOCTYPE html> < head > < title > \u00a1Hola mundo! </ title > </ head > < body > < h1 > \u00a1Hola mundo! </ h1 > < p > <?php echo 'Estamos corriendo PHP, version: ' . phpversion(); ?> </ p > <? $database =\"mydb\"; $user = \"myuser\"; $password = \"password\"; $host = \"mysql\"; $connection = new PDO(\"mysql:host={$host};dbname={$database};charset=utf8\", $user, $password); $query = $connection->query(\"SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_TYPE='BASE TABLE'\"); $tables = $query->fetchAll(PDO::FETCH_COLUMN); if (empty($tables)) { echo \"<p>No hay tablas en la base de datos \\\"{$database}\\\".</p>\"; } else { echo \"<p>La base de datos \\\"{$database}\\\" tiene las siguientes tablas:</p>\"; echo \"<ul>\"; foreach ($tables as $table) { echo \"<li>{$table}</li>\"; } echo \"</ul>\"; } ?> </ body > </ html > Guardad el archivo y lanzad los contenedores una vez m\u00e1s: docker-compose up -d Y verificamos que est\u00e1n ejecut\u00e1ndose: docker ps -a Y veremos: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d3e82747fe0d mysql:5.7 \"docker-entrypoint.s\u2026\" 39 seconds ago Up 38 seconds 3306/tcp, 33060/tcp mysql-container 606320e5a7f8 mysql:5.7 \"docker-entrypoint.s\u2026\" 41 seconds ago Exited (0) 39 seconds ago mysql-data-container ca4f63797d11 docker-project_php \"docker-php-entrypoi\u2026\" 2 hours ago Up 2 hours 9000/tcp php-container 849315c7ffc0 docker-project_nginx \"/docker-entrypoint.\u2026\" 2 hours ago Up 2 hours 0.0.0.0:80->80/tcp, :::80->80/tcp nginx-container fbca95944234 php:7.0-fpm \"docker-php-entrypoi\u2026\" 2 hours ago Exited (0) 39 seconds ago app-data-","title":"5. Creaci\u00f3n de un contenedor MySQL"},{"location":"P6.2/#6-verificacion-de-conexion-a-la-base-de-datos","text":"Si ahora accedemos a http://IP_Maq_Virtual , deber\u00edamos obtener la siguiente pantalla: Como pod\u00e9is ver, nos dice que no tenemos ninguna tabla en la base de datos mydb . Sin embargo, el hecho es que realmente s\u00ed existen algunas tablas, s\u00edmplemente no son visibles para un usuario normal. Si quisi\u00e9ramos verlas, debemos editar el archivo index.php y cambiar $user por root y $password a secret . Es decir: nano /home/usuario/www/html/index.php Y cambiar las l\u00edneas: $user = \"root\"; $password = \"secret\"; Guardad el archivo y refrescad la p\u00e1gina. Deber\u00edas obtener ahora una pantalla con todas las tablas de la base de datos, tal que as\u00ed: Tarea Documenta, incluyendo capturas de pantallas, el proceso que has seguido para realizar el despliegue de esta nueva aplicaci\u00f3n, as\u00ed como el resultado final.","title":"6. Verificaci\u00f3n de conexi\u00f3n a la base de datos"},{"location":"P6.2/#referencias","text":"\u00bfQu\u00e9 es Docker Compose? \u00bfQu\u00e9 demonios es Docker y Docker-Compose? y c\u00f3mo Dockerizar Dotnet Core WebApi y SQL Server en un ambiente de desarrollo ideal Introducci\u00f3n a docker-compose How to Deploy a PHP Application with Nginx and MySQL Using Docker and Docker Compose","title":"Referencias"},{"location":"P6.3/","text":"Pr\u00e1ctica 6.3 - Despliegue de servidores web con usuarios autenticados mediante LDAP usando Docker y docker-compose Introducci\u00f3n \u00bfQu\u00e9 es un servicio de directorio LDAP? LDAP (Lightweight Directory Access Protocol) o tambi\u00e9n conocido como \u00abProtocolo Ligero de Acceso a Directorios\u00bb es un protocolo de la capa de aplicaci\u00f3n TCP/IP que permite el acceso a un servicio de directorio ordenado y distribuido, para buscar cualquier informaci\u00f3n en un entorno de red. Aclaraci\u00f3n Un directorio es un conjunto de objetos con atributos que est\u00e1n organizados de manera l\u00f3gica y jer\u00e1rquica, es decir, est\u00e1 en forma de \u00e1rbol y perfectamente ordenado en funci\u00f3n de lo que nosotros queramos, ya sea alfab\u00e9ticamente, por usuarios, direcciones etc. Generalmente un servidor LDAP se encarga de almacenar informaci\u00f3n de autenticaci\u00f3n, es decir, el usuario y la contrase\u00f1a, para posteriormente dar acceso a otro protocolo o servicio del sistema. Adem\u00e1s de almacenar el nombre de usuario y la contrase\u00f1a, tambi\u00e9n puede almacenar otra informaci\u00f3n como datos de contacto del usuario, ubicaci\u00f3n de los recursos de la red local, certificados digitales de los propios usuarios y mucho m\u00e1s. LDAP es un protocolo que nos permite acceder a los recursos de la red local, sin necesidad de crear los diferentes usuarios en el sistema operativo, adem\u00e1s, es mucho m\u00e1s vers\u00e1til. Por ejemplo, LDAP permite realizar tareas de autenticaci\u00f3n y autorizaci\u00f3n a usuarios de diferentes softwares como Docker, OpenVPN, servidores de archivos como los usados por QNAP, Synology o ASUSTOR entre otros, y muchos m\u00e1s usos. LDAP puede ser utilizado tanto por un usuario al que se pide unos credenciales de acceso, como tambi\u00e9n por las aplicaciones para saber si tienen acceso a determinada informaci\u00f3n del sistema o no. Generalmente un servidor LDAP se encuentra en una red privada, es decir, redes de \u00e1rea local, para autenticar las diferentes aplicaciones y usuarios, pero tambi\u00e9n podr\u00eda funcionar sobre redes p\u00fablicas sin ning\u00fan problema. Info En definitiva, LDAP nos proporciona un serivicio de autenticaci\u00f3n y autorizaci\u00f3n para poder acceder a distintos recursos en red, como por ejemplo, a un sitio web. Si recordamos la pr\u00e1ctica 2.2, nuestro usuario se autenticaba utilizando usuarios creados en el mismo sistema operativo (Debian Linux) donde se hab\u00eda instalado el servidor web Nginx. Tenemos, por tanto, la posibilidad de utilizar otra autenticaci\u00f3n centralizada para el mismo cometido con LDAP. Implementaciones de LDAP Microsoft Active Directory utiliza internamente el protocolo LDAP para realizar todas las comunicaciones desde los clientes hasta el servidor o servidores, por lo tanto, se encarga de que los clientes puedan autenticarse y acceder a cualquier dato almacenado, adem\u00e1s, debemos tener en cuenta que este protocolo es multiplataforma, no solamente lo tenemos en sistemas operativos Windows sino que tambi\u00e9n es compatible con Linux, Unix y macOS, todo ello a trav\u00e9s del protocolo. Para que os hag\u00e1is una idea, los siguientes servicios de directorio usan este protocolo para su comunicaci\u00f3n: Active Directory de Microsoft Apache Servicio de directorio de Red Hat OpenLDAP Y muchos otros servicios tambi\u00e9n lo usan, sobre todo el \u00faltimo, OpenLDAP, el cual es una implementaci\u00f3n de c\u00f3digo abierto del protocolo y que se puede instalar en cualquier sistema, ya que est\u00e1 disponible el c\u00f3digo fuente para compilarlo. No obstante, en la mayor\u00eda de distribuciones de Linux lo tenemos disponible en sus repositorios. \u00bfC\u00f3mo se organiza la informaci\u00f3n en LDAP? En LDAP, las entradas est\u00e1n organizadas en una estructura jer\u00e1rquica en \u00e1rbol . Tradicionalmente, esta estructura reflejaba los l\u00edmites geogr\u00e1ficos y organizacionales. Las entradas que representan pa\u00edses aparecen en la parte superior del \u00e1rbol. Debajo de ellos, est\u00e1n las entradas que representan los estados y las organizaciones nacionales. Debajo de est\u00e1s, pueden estar las entradas que representan las unidades organizacionales, empleados, impresoras, documentos o todo aquello que pueda imaginarse. La siguiente figura muestra un ejemplo de un \u00e1rbol de directorio LDAP haciendo uso del nombramiento tradicional. El \u00e1rbol tambi\u00e9n se puede organizar bas\u00e1ndose en los nombres de dominio de Internet. Este tipo de nombramiento es muy popular, ya que permite localizar un servicio de directorio haciendo uso de los DNS. La siguiente figura muestra un ejemplo de un directorio LDAP que hace uso de los nombres basados en dominios. \u00bfC\u00f3mo se referencia la informaci\u00f3n? Una entrada es referenciada por su nombre distinguido (DN), que es construido por el nombre de la propia entrada (llamado Nombre Relativo Distinguido o RDN) y la concatenaci\u00f3n de los nombres de las entradas que le anteceden. Por ejemplo, la entrada para Nuno Gon\u00e7alves en el ejemplo del nombramiento de Internet anterior tiene el siguiente RDN: uid=nuno y su DN ser\u00eda: uid=nuno,ou=estig,dc=ipb,dc=pt. C\u00f3mo se accede a la informaci\u00f3n? LDAP define operaciones para interrogar y actualizar el directorio. Provee operaciones para a\u00f1adir y borrar entradas del directorio, modificar una entrada existente y cambiar el nombre de una entrada. La mayor parte del tiempo, sin embargo, LDAP se utiliza para buscar informaci\u00f3n almacenada en el directorio. Las operaciones de b\u00fasqueda de LDAP permiten buscar entradas que concuerdan con alg\u00fan criterio especificado por un filtro de b\u00fasqueda. La informaci\u00f3n puede ser solicitada desde cada entrada que concuerda con dicho criterio. Por ejemplo, imaginemos que queremos buscar en el sub\u00e1rbol del directorio que est\u00e1 por debajo de dc=ipb,dc=pt a personas con el nombre Nuno Gon\u00e7alves, obteniendo la direcci\u00f3n de correo electr\u00f3nico de cada entrada que concuerde. LDAP permite hacer esto f\u00e1cilmente. O tal vez preferimos buscar las organizaciones que posean la cadena IPB en su nombre, posean n\u00famero de fax y est\u00e9n debajo de la entrada st=Bragan\u00e7a,c=PT. LDAP permite hacer esto tambi\u00e9n. LDAP ofrece una autenticaci\u00f3n y autorizaci\u00f3n optimizadas y una b\u00fasqueda eficaz de datos de direcciones y de usuarios. Debido a sus muchas ventajas para las empresas. LDAP sirve a modo de un est\u00e1ndar de la industria y es compatible con la mayor\u00eda de los productos de software. Las ventajas principales son la rapidez de las consultas y conexiones, un lenguaje de consulta sencillo y un protocolo claramente estructurado M\u00f3dulos en Apache Un m\u00f3dulo es una parte independiente de un programa. La mayor parte de la funcionalidad de Apache est\u00e1 contenida en m\u00f3dulos que pueden incluirse o excluirse. Como decimos, existen una gran cantidad de M\u00f3dulos para utilizarse con Apache, algunos ejemplo son: \"Virtual Hosting\",\"Mod_JK(Java)\" y \"Rewrite\". Una de las principales razones de emplear m\u00f3dulos en Apache, es que no toda instalaci\u00f3n requiere de las mismas funcionalidades, esto es, una instalaci\u00f3n que utilice PHP probablemente no requiera de Tomcat (Java), o bien posiblemente no todas las instalaciones requieran de \"Virtual Hosting\". As\u00ed las cosas, para no incluir todas las funcionalidades de Apache, necesarias e innecesarias para cada ocasi\u00f3n, en un \u00fanico paquete de instalaci\u00f3n que lo har\u00eda demasiado grande en tama\u00f1o y pesado en recursos, se hace uso de los m\u00f3dulos, de tal forma que s\u00f3lo cargaremos en memoria los que nos hagan falta en cada ocasi\u00f3n. Los m\u00f3dulos le permiten a los administradores del servidor activar y desactivar funcionalidades adicionales. Apache tiene m\u00f3dulos de seguridad, almacenamiento en cach\u00e9, reescritura de URL, autenticaci\u00f3n de contrase\u00f1a y m\u00e1s. Info En Apache hay dos tipos de m\u00f3dulos: Est\u00e1ticos: Son a\u00f1adidos al compilar el servidor. Din\u00e1micos: Se cargan din\u00e1micamente al iniciar el servidor. Se puede habilitar cualquiera de los m\u00f3dulos de la lista con el comando a2enmod (nombre del m\u00f3dulo) (usando el sudo si no se es superusuario), y deshabilitar cualquiera de ellos previamente habilitado mediante el comando a2dismod (nombre del m\u00f3dulo) (usando el sudo si no se es superusuario). M\u00f3dulos en Nginx Los m\u00f3dulos est\u00e1ticos existen desde sus inicios en Nginx y los din\u00e1micos desde la versi\u00f3n 1.9.11 (Febrero de 2016). Nginx es, de hecho, una colecci\u00f3n de m\u00f3dulos. Incluso funciones b\u00e1sicas tales como HTTP o servir ficheros est\u00e1ticos dentro de HTTP, est\u00e1n implementadas por m\u00f3dulos. Se puede extender la funcionalidad de Nginx a\u00f1adiendo m\u00f3dulos propios. Esta arquitectura modular permite modificar f\u00e1cilmente Nginx. M\u00f3dulo autenticaci\u00f3n LDAP en Nginx La soluci\u00f3n aprovecha el m\u00f3dulo ngx_http_auth_request_module de Nginx y NGINX, que reenv\u00eda las peticiones de autenticaci\u00f3n a un servicio externo. En la implementaci\u00f3n de referencia, ese servicio es un demonio que llamamos ldap-auth. Est\u00e1 escrito en Python y se comunica con un servidor de autenticaci\u00f3n del Protocolo Ligero de Acceso a Directorios (LDAP) - OpenLDAP por defecto, pero hemos probado el demonio ldap-auth tambi\u00e9n con configuraciones por defecto de Microsoft\u00ae Windows\u00ae Server Active Directory (tanto la versi\u00f3n 2003 como la 2012). Para realizar la autenticaci\u00f3n, el m\u00f3dulo http_auth_request realiza una subconsulta HTTP al demonio ldap-auth, que act\u00faa como intermediario e interpreta la subconsulta para el servidor LDAP - utiliza HTTP para la comunicaci\u00f3n con Nginx y la API apropiada para la comunicaci\u00f3n con el servidor LDAP. Para realizar la autenticaci\u00f3n, el m\u00f3dulo http_auth_request realiza una subconsulta HTTP al demonio ldap-auth, que act\u00faa como intermediario e interpreta la subconsulta para el servidor LDAP - utiliza HTTP para la comunicaci\u00f3n con Nginx y la API apropiada para la comunicaci\u00f3n con el servidor LDAP. A continuaci\u00f3n se describe paso a paso el proceso de autenticaci\u00f3n en la implementaci\u00f3n de referencia. Los detalles se determinan por los ajustes en el archivo de configuraci\u00f3n nginx-ldap-auth.conf; ver Configuraci\u00f3n de la implementaci\u00f3n de referencia m\u00e1s abajo. El diagrama de flujo debajo de los pasos resume el proceso. Un cliente env\u00eda una solicitud HTTP para un recurso protegido alojado en un servidor para el que Nginx est\u00e1 actuando como proxy inverso. Nginx (concretamente, el m\u00f3dulo http_auth_request ) reenv\u00eda la solicitud al demonio ldap-auth , que responde con el c\u00f3digo HTTP 401 porque no se han proporcionado credenciales. Nginx reenv\u00eda la solicitud a http://backend/login , que corresponde al demonio del backend. Escribe el URI de la solicitud original en la cabecera X-Target de la solicitud reenviada. El demonio del backend env\u00eda al cliente un formulario de inicio de sesi\u00f3n (el formulario est\u00e1 definido en el c\u00f3digo Python del demonio). Tal y como se configura en la directiva error_page, NGINX establece el c\u00f3digo HTTP del formulario de inicio de sesi\u00f3n en 200. El usuario rellena los campos Nombre de usuario y Contrase\u00f1a en el formulario y hace clic en el bot\u00f3n Login. Seg\u00fan el c\u00f3digo del formulario, el cliente genera una petici\u00f3n HTTP POST dirigida a /login , que Nginx reenv\u00eda al demonio del backend. El demonio del backend construye una cadena con el formato nombre de usuario:contrase\u00f1a , aplica la codificaci\u00f3n Base64, genera una cookie llamada nginxauth con su valor establecido a la cadena codificada, y env\u00eda la cookie al cliente. Establece el flag httponly para evitar el uso de JavaScript para leer o manipular la cookie (protegiendo contra la vulnerabilidad cross-site scripting [XSS]). El cliente retransmite su solicitud original (del paso 1), esta vez incluyendo la cookie en el campo Cookie de la cabecera HTTP. Nginx reenv\u00eda la solicitud al demonio ldap-auth (como en el paso 2). El demonio ldap-auth decodifica la cookie y env\u00eda el nombre de usuario y la contrase\u00f1a al servidor LDAP en una solicitud de autenticaci\u00f3n. La siguiente acci\u00f3n depende de si el servidor LDAP autentifica con \u00e9xito al usuario: Si la autenticaci\u00f3n tiene \u00e9xito, el demonio ldap-auth env\u00eda el c\u00f3digo HTTP 200 a Nginx. Nginx solicita el recurso al demonio del backend y \u00e9ste devuelve el c\u00f3digo del sitio web. El archivo nginx-ldap-auth.conf incluye directivas para el almacenamiento en cach\u00e9 de los resultados del intento de autenticaci\u00f3n. Si la autenticaci\u00f3n falla, el demonio ldap-auth env\u00eda el c\u00f3digo HTTP 401 a Nginx. Nginx reenv\u00eda la solicitud al demonio del backend de nuevo (como en el paso 3), y el proceso se repite. Despliegue con Docker de NGINX + demonio de autenticaci\u00f3n LDAP + OpenLDAP Para esta pr\u00e1ctica nos crearemos un directorio que contendr\u00e1 nuestro index.html, con un texto muy simple: $ mkdir app $ cat << EOF > app/index.html <html> <body> <h1>\u00a1Hola Mundo!</h1> </body> </html> EOF As\u00ed como otro directorio, con el contenido de la configuraci\u00f3n pertinente de Nginx: $ mkdir conf $ cat << EOF > conf/ldap_nginx.conf server { listen 8080 ; location = / { auth_request /auth-proxy ; } location = /auth-proxy { internal ; proxy_pass http://nginx-ldap:8888 ; # URL y puerto para conectarse al servidor LDAP proxy_set_header X-Ldap-URL \"ldap://openldap:1389\" ; # Base DN proxy_set_header X-Ldap-BaseDN \"dc=example,dc=org\" ; # Bind DN proxy_set_header X-Ldap-BindDN \"cn=admin,dc=example,dc=org\" ; # Bind password proxy_set_header X-Ldap-BindPass \"adminpassword\" ; } } En esta configuraci\u00f3n le decimos a Nginx: Que escuche en el puerto 8080 las peticiones HTTP Que cuando se acceda al sitio web, se solicite autorizaci\u00f3n en el directorio del sitio web llamado /auth-proxy Se crea un nuevo location para ese directorio /auth-proxy y que es donde se realizar\u00e1 la configuraci\u00f3n de c\u00f3mo conectarnos a nuestro openldap, de acuerdo con la documentaci\u00f3n oficial de Nginx a prop\u00f3sito de su m\u00f3dulo de autenticaci\u00f3n: Se indica la URL de nuestro openldap (es el nombre del contenedor que hemos levantado, ya que Docker tiene un DNS propio entre sus contenedores) El DN (Nombre distinguido) base sobre el que se realizar\u00e1n las b\u00fasquedas en openldap El usuario y contrase\u00f1a con el que nos conectaremos al openldap para realizar las consultas Y ahora, procedemos con el siguiente docker-compose.yml : version : '2' services : nginx-ldap : # (1) image : bitnami/nginx-ldap-auth-daemon # (2) ports : # (3) - 8888:8888 nginx : # (4) image : bitnami/nginx ports : - 8080:8080 volumes : # (5) - ./app:/app - ./conf/ldap_nginx.conf:/opt/bitnami/nginx/conf/server_blocks/ldap_nginx.conf openldap : # (6) image : bitnami/openldap ports : - '1389:1389' environment : # (7) - LDAP_ADMIN_USERNAME=admin - LDAP_ADMIN_PASSWORD=adminpassword - LDAP_USERS=customuser - LDAP_PASSWORDS=custompassword Nombre del contenedor Imagen que descargaremos del Dockerhub y a partir de la cual crearemos nuestro contenedor Puerto/s que se publicar\u00e1n para el contenedor Nombre del contenedor Vol\u00famenes o directorios compartidos entre nuestra m\u00e1quina y el contenedor Nombre del contenedor Variables de entorno utilizadas para la configuraci\u00f3n de este contenedor. Incluye credenciales del administrador de openldap, as\u00ed como un usuario que se crear\u00e1 en dicho openldap (pod\u00e9is cambiar usuario y contrase\u00f1a si quer\u00e9is, pero luego deb\u00e9is recordarlas) Tras esto s\u00f3lo queda ejecutar compose: docker-compose up Y comprobar que no se producen errores. Despliegue con Docker de PHP + Apache con autenticaci\u00f3n LDAP Creamos un directorio que se llame Practica6.3 En primer lugar, como es obvio, dentro del directorio creado debemos crear el index.php de nuestra aplicaci\u00f3n: <?php echo 'Well, hello LDAP authenticated user!' ; Dentro de nuestro directorio de trabajo, creado anteriormente, crearemos otro directorio llamado Docker y dentro de \u00e9l, un Dockerfile ( ./Docker/Dockerfile ) Nota En Linux, cuando queremos hacer referencia al directorio actual, lo hacemos con un punto . Si dentro de nuestro directorio actual tenemos una carpeta llamada prueba , podemos hacer referencia a ella como ./prueba , ya que el . hace referencia precisamente al directorio donde nos encontramos. Para completar este Dockerfile con las opciones/directivas adecuadas, leed los comentarios y pod\u00e9is apoyaros en la teor\u00eda o en este cheatsheet : # ./Docker/Dockerfile --> directorio donde se encuentra este archivo # Imagen base sobre la que vamos a trabajar ____ php:7-apache # Activamos el m\u00f3dulo LDAP de Apache ejecutand el siguiente comando ____ a2enmod authnz_ldap # A\u00f1adimos las reglas/configuraci\u00f3n de LDAP al directorio conf-enabled de Apache # (crearemos este archivo en el siguiente paso) ____ Docker/ldap-demo.conf /etc/apache2/conf-enabled/ # A\u00f1adimos ayuda de depuraci\u00f3n (debugging) en la configuraci\u00f3n de apache # En caso de necesitarlo, lo descomentamos para ejecutar el siguiente comando # ____ echo \"LogLevel debug\" >> apache2.conf # Establecemos el directorio de trabajo adecuado ____ /var/www/html/demo # Configuramos Apache para usar la configuraci\u00f3n ldap definida arriba, la copiamos de nuestro ordenador al contenedor ____ Docker/.htaccess ./.htaccess # Copiamos los archivos del proyecto que necesitamos, al contenedor ____ index.php ./ Tarea Completa el Dockerfile. Ahora crearemos el archivo ./Docker/ldap-demo.conf , que es la configuraci\u00f3n LDAP. Aqu\u00ed establecemos los criterios de conexi\u00f3n con el contenedor de Openldap, password y URL. Las directivas PassEnv al principio del archivo nos permiten omitir nuestras credenciales y pasarlas luego como variables de entorno al correr la imagen del contenedor: # ./Docker/ldap-demo.conf PassEnv LDAP_BIND_ON PassEnv LDAP_PASSWORD PassEnv LDAP_URL <AuthnProviderAlias ldap demo> AuthLDAPBindDN ${ LDAP_BIND_ON } AuthLDAPBindPassword ${ LDAP_PASSWORD } AuthLDAPURL ${ LDAP_URL } </AuthnProviderAlias> Creamos el archivo .htaccess: # ./.htaccess AuthBasicProvider demo AuthType Basic AuthName \"Protected Area\" Require valid-user Dentro de nuestro directorio de trabajo, construimos la imagen con el siguiente comando: docker build \\ -t docker-ldap \\ -f ./Docker/dockerfile \\ . Corremos el contenedor indicando las credenciales de nuestra cuenta LDAP mediante variables de entorno con la flag -e . Para este caso, vamos a probar un servidor LDAP externo, csimulando que tuvi\u00e9ramos que integrar nuestro despliegue con un servidor ya existente en la empresa. Utilizaremos un servidor p\u00fablico en Internet dedicado a pruebas: https://www.forumsys.com/2022/05/10/online-ldap-test-server/ docker run \\ -p 3000 :80 \\ --name ldap_demo \\ -e LDAP_BIND_ON = 'cn=read-only-admin,dc=example,dc=com' \\ -e LDAP_PASSWORD = 'password' \\ -e LDAP_URL = 'LDAP://ldap.forumsys.com/dc=example,dc=com' \\ docker-ldap No nos queda m\u00e1s que visitar http://IP-M\u00e1q-Debian:3000/demo . Si todo ha ido bien, nos solicitar\u00e1 nuestras credenciales para loguearnos contra el servidor openldap. Tarea Documenta todo el proceso de la pr\u00e1ctica, siguiendo los pasos y con las capturas y explicaciones pertinentes. Referencias Para qu\u00e9 sirve el protocolo LDAP y c\u00f3mo funciona OpenLDAP conceptos te\u00f3ricos Using Nginx and NGINX to Authenticate Application Users with LDAP Simple Docker/Apache/PHP Authentication with LDAP bitnami/nginx-ldap-auth-daemon nginxinc/nginx-ldap-auth Dockerfile cheatsheet Difference between RUN and CMD in a Dockerfile","title":"Pr\u00e1ctica 6.3- Servidor web con usuarios autenticados mediante servicio de directorio (LDAP)"},{"location":"P6.3/#practica-63-despliegue-de-servidores-web-con-usuarios-autenticados-mediante-ldap-usando-docker-y-docker-compose","text":"","title":"Pr\u00e1ctica 6.3 - Despliegue de servidores web con usuarios autenticados mediante LDAP usando Docker y docker-compose"},{"location":"P6.3/#introduccion","text":"","title":"Introducci\u00f3n"},{"location":"P6.3/#que-es-un-servicio-de-directorio-ldap","text":"LDAP (Lightweight Directory Access Protocol) o tambi\u00e9n conocido como \u00abProtocolo Ligero de Acceso a Directorios\u00bb es un protocolo de la capa de aplicaci\u00f3n TCP/IP que permite el acceso a un servicio de directorio ordenado y distribuido, para buscar cualquier informaci\u00f3n en un entorno de red. Aclaraci\u00f3n Un directorio es un conjunto de objetos con atributos que est\u00e1n organizados de manera l\u00f3gica y jer\u00e1rquica, es decir, est\u00e1 en forma de \u00e1rbol y perfectamente ordenado en funci\u00f3n de lo que nosotros queramos, ya sea alfab\u00e9ticamente, por usuarios, direcciones etc. Generalmente un servidor LDAP se encarga de almacenar informaci\u00f3n de autenticaci\u00f3n, es decir, el usuario y la contrase\u00f1a, para posteriormente dar acceso a otro protocolo o servicio del sistema. Adem\u00e1s de almacenar el nombre de usuario y la contrase\u00f1a, tambi\u00e9n puede almacenar otra informaci\u00f3n como datos de contacto del usuario, ubicaci\u00f3n de los recursos de la red local, certificados digitales de los propios usuarios y mucho m\u00e1s. LDAP es un protocolo que nos permite acceder a los recursos de la red local, sin necesidad de crear los diferentes usuarios en el sistema operativo, adem\u00e1s, es mucho m\u00e1s vers\u00e1til. Por ejemplo, LDAP permite realizar tareas de autenticaci\u00f3n y autorizaci\u00f3n a usuarios de diferentes softwares como Docker, OpenVPN, servidores de archivos como los usados por QNAP, Synology o ASUSTOR entre otros, y muchos m\u00e1s usos. LDAP puede ser utilizado tanto por un usuario al que se pide unos credenciales de acceso, como tambi\u00e9n por las aplicaciones para saber si tienen acceso a determinada informaci\u00f3n del sistema o no. Generalmente un servidor LDAP se encuentra en una red privada, es decir, redes de \u00e1rea local, para autenticar las diferentes aplicaciones y usuarios, pero tambi\u00e9n podr\u00eda funcionar sobre redes p\u00fablicas sin ning\u00fan problema. Info En definitiva, LDAP nos proporciona un serivicio de autenticaci\u00f3n y autorizaci\u00f3n para poder acceder a distintos recursos en red, como por ejemplo, a un sitio web. Si recordamos la pr\u00e1ctica 2.2, nuestro usuario se autenticaba utilizando usuarios creados en el mismo sistema operativo (Debian Linux) donde se hab\u00eda instalado el servidor web Nginx. Tenemos, por tanto, la posibilidad de utilizar otra autenticaci\u00f3n centralizada para el mismo cometido con LDAP.","title":"\u00bfQu\u00e9 es un servicio de directorio LDAP?"},{"location":"P6.3/#implementaciones-de-ldap","text":"Microsoft Active Directory utiliza internamente el protocolo LDAP para realizar todas las comunicaciones desde los clientes hasta el servidor o servidores, por lo tanto, se encarga de que los clientes puedan autenticarse y acceder a cualquier dato almacenado, adem\u00e1s, debemos tener en cuenta que este protocolo es multiplataforma, no solamente lo tenemos en sistemas operativos Windows sino que tambi\u00e9n es compatible con Linux, Unix y macOS, todo ello a trav\u00e9s del protocolo. Para que os hag\u00e1is una idea, los siguientes servicios de directorio usan este protocolo para su comunicaci\u00f3n: Active Directory de Microsoft Apache Servicio de directorio de Red Hat OpenLDAP Y muchos otros servicios tambi\u00e9n lo usan, sobre todo el \u00faltimo, OpenLDAP, el cual es una implementaci\u00f3n de c\u00f3digo abierto del protocolo y que se puede instalar en cualquier sistema, ya que est\u00e1 disponible el c\u00f3digo fuente para compilarlo. No obstante, en la mayor\u00eda de distribuciones de Linux lo tenemos disponible en sus repositorios.","title":"Implementaciones de LDAP"},{"location":"P6.3/#como-se-organiza-la-informacion-en-ldap","text":"En LDAP, las entradas est\u00e1n organizadas en una estructura jer\u00e1rquica en \u00e1rbol . Tradicionalmente, esta estructura reflejaba los l\u00edmites geogr\u00e1ficos y organizacionales. Las entradas que representan pa\u00edses aparecen en la parte superior del \u00e1rbol. Debajo de ellos, est\u00e1n las entradas que representan los estados y las organizaciones nacionales. Debajo de est\u00e1s, pueden estar las entradas que representan las unidades organizacionales, empleados, impresoras, documentos o todo aquello que pueda imaginarse. La siguiente figura muestra un ejemplo de un \u00e1rbol de directorio LDAP haciendo uso del nombramiento tradicional. El \u00e1rbol tambi\u00e9n se puede organizar bas\u00e1ndose en los nombres de dominio de Internet. Este tipo de nombramiento es muy popular, ya que permite localizar un servicio de directorio haciendo uso de los DNS. La siguiente figura muestra un ejemplo de un directorio LDAP que hace uso de los nombres basados en dominios.","title":"\u00bfC\u00f3mo se organiza la informaci\u00f3n en LDAP?"},{"location":"P6.3/#como-se-referencia-la-informacion","text":"Una entrada es referenciada por su nombre distinguido (DN), que es construido por el nombre de la propia entrada (llamado Nombre Relativo Distinguido o RDN) y la concatenaci\u00f3n de los nombres de las entradas que le anteceden. Por ejemplo, la entrada para Nuno Gon\u00e7alves en el ejemplo del nombramiento de Internet anterior tiene el siguiente RDN: uid=nuno y su DN ser\u00eda: uid=nuno,ou=estig,dc=ipb,dc=pt.","title":"\u00bfC\u00f3mo se referencia la informaci\u00f3n?"},{"location":"P6.3/#como-se-accede-a-la-informacion","text":"LDAP define operaciones para interrogar y actualizar el directorio. Provee operaciones para a\u00f1adir y borrar entradas del directorio, modificar una entrada existente y cambiar el nombre de una entrada. La mayor parte del tiempo, sin embargo, LDAP se utiliza para buscar informaci\u00f3n almacenada en el directorio. Las operaciones de b\u00fasqueda de LDAP permiten buscar entradas que concuerdan con alg\u00fan criterio especificado por un filtro de b\u00fasqueda. La informaci\u00f3n puede ser solicitada desde cada entrada que concuerda con dicho criterio. Por ejemplo, imaginemos que queremos buscar en el sub\u00e1rbol del directorio que est\u00e1 por debajo de dc=ipb,dc=pt a personas con el nombre Nuno Gon\u00e7alves, obteniendo la direcci\u00f3n de correo electr\u00f3nico de cada entrada que concuerde. LDAP permite hacer esto f\u00e1cilmente. O tal vez preferimos buscar las organizaciones que posean la cadena IPB en su nombre, posean n\u00famero de fax y est\u00e9n debajo de la entrada st=Bragan\u00e7a,c=PT. LDAP permite hacer esto tambi\u00e9n. LDAP ofrece una autenticaci\u00f3n y autorizaci\u00f3n optimizadas y una b\u00fasqueda eficaz de datos de direcciones y de usuarios. Debido a sus muchas ventajas para las empresas. LDAP sirve a modo de un est\u00e1ndar de la industria y es compatible con la mayor\u00eda de los productos de software. Las ventajas principales son la rapidez de las consultas y conexiones, un lenguaje de consulta sencillo y un protocolo claramente estructurado","title":"C\u00f3mo se accede a la informaci\u00f3n?"},{"location":"P6.3/#modulos-en-apache","text":"Un m\u00f3dulo es una parte independiente de un programa. La mayor parte de la funcionalidad de Apache est\u00e1 contenida en m\u00f3dulos que pueden incluirse o excluirse. Como decimos, existen una gran cantidad de M\u00f3dulos para utilizarse con Apache, algunos ejemplo son: \"Virtual Hosting\",\"Mod_JK(Java)\" y \"Rewrite\". Una de las principales razones de emplear m\u00f3dulos en Apache, es que no toda instalaci\u00f3n requiere de las mismas funcionalidades, esto es, una instalaci\u00f3n que utilice PHP probablemente no requiera de Tomcat (Java), o bien posiblemente no todas las instalaciones requieran de \"Virtual Hosting\". As\u00ed las cosas, para no incluir todas las funcionalidades de Apache, necesarias e innecesarias para cada ocasi\u00f3n, en un \u00fanico paquete de instalaci\u00f3n que lo har\u00eda demasiado grande en tama\u00f1o y pesado en recursos, se hace uso de los m\u00f3dulos, de tal forma que s\u00f3lo cargaremos en memoria los que nos hagan falta en cada ocasi\u00f3n. Los m\u00f3dulos le permiten a los administradores del servidor activar y desactivar funcionalidades adicionales. Apache tiene m\u00f3dulos de seguridad, almacenamiento en cach\u00e9, reescritura de URL, autenticaci\u00f3n de contrase\u00f1a y m\u00e1s. Info En Apache hay dos tipos de m\u00f3dulos: Est\u00e1ticos: Son a\u00f1adidos al compilar el servidor. Din\u00e1micos: Se cargan din\u00e1micamente al iniciar el servidor. Se puede habilitar cualquiera de los m\u00f3dulos de la lista con el comando a2enmod (nombre del m\u00f3dulo) (usando el sudo si no se es superusuario), y deshabilitar cualquiera de ellos previamente habilitado mediante el comando a2dismod (nombre del m\u00f3dulo) (usando el sudo si no se es superusuario).","title":"M\u00f3dulos en Apache"},{"location":"P6.3/#modulos-en-nginx","text":"Los m\u00f3dulos est\u00e1ticos existen desde sus inicios en Nginx y los din\u00e1micos desde la versi\u00f3n 1.9.11 (Febrero de 2016). Nginx es, de hecho, una colecci\u00f3n de m\u00f3dulos. Incluso funciones b\u00e1sicas tales como HTTP o servir ficheros est\u00e1ticos dentro de HTTP, est\u00e1n implementadas por m\u00f3dulos. Se puede extender la funcionalidad de Nginx a\u00f1adiendo m\u00f3dulos propios. Esta arquitectura modular permite modificar f\u00e1cilmente Nginx.","title":"M\u00f3dulos en Nginx"},{"location":"P6.3/#modulo-autenticacion-ldap-en-nginx","text":"La soluci\u00f3n aprovecha el m\u00f3dulo ngx_http_auth_request_module de Nginx y NGINX, que reenv\u00eda las peticiones de autenticaci\u00f3n a un servicio externo. En la implementaci\u00f3n de referencia, ese servicio es un demonio que llamamos ldap-auth. Est\u00e1 escrito en Python y se comunica con un servidor de autenticaci\u00f3n del Protocolo Ligero de Acceso a Directorios (LDAP) - OpenLDAP por defecto, pero hemos probado el demonio ldap-auth tambi\u00e9n con configuraciones por defecto de Microsoft\u00ae Windows\u00ae Server Active Directory (tanto la versi\u00f3n 2003 como la 2012). Para realizar la autenticaci\u00f3n, el m\u00f3dulo http_auth_request realiza una subconsulta HTTP al demonio ldap-auth, que act\u00faa como intermediario e interpreta la subconsulta para el servidor LDAP - utiliza HTTP para la comunicaci\u00f3n con Nginx y la API apropiada para la comunicaci\u00f3n con el servidor LDAP. Para realizar la autenticaci\u00f3n, el m\u00f3dulo http_auth_request realiza una subconsulta HTTP al demonio ldap-auth, que act\u00faa como intermediario e interpreta la subconsulta para el servidor LDAP - utiliza HTTP para la comunicaci\u00f3n con Nginx y la API apropiada para la comunicaci\u00f3n con el servidor LDAP. A continuaci\u00f3n se describe paso a paso el proceso de autenticaci\u00f3n en la implementaci\u00f3n de referencia. Los detalles se determinan por los ajustes en el archivo de configuraci\u00f3n nginx-ldap-auth.conf; ver Configuraci\u00f3n de la implementaci\u00f3n de referencia m\u00e1s abajo. El diagrama de flujo debajo de los pasos resume el proceso. Un cliente env\u00eda una solicitud HTTP para un recurso protegido alojado en un servidor para el que Nginx est\u00e1 actuando como proxy inverso. Nginx (concretamente, el m\u00f3dulo http_auth_request ) reenv\u00eda la solicitud al demonio ldap-auth , que responde con el c\u00f3digo HTTP 401 porque no se han proporcionado credenciales. Nginx reenv\u00eda la solicitud a http://backend/login , que corresponde al demonio del backend. Escribe el URI de la solicitud original en la cabecera X-Target de la solicitud reenviada. El demonio del backend env\u00eda al cliente un formulario de inicio de sesi\u00f3n (el formulario est\u00e1 definido en el c\u00f3digo Python del demonio). Tal y como se configura en la directiva error_page, NGINX establece el c\u00f3digo HTTP del formulario de inicio de sesi\u00f3n en 200. El usuario rellena los campos Nombre de usuario y Contrase\u00f1a en el formulario y hace clic en el bot\u00f3n Login. Seg\u00fan el c\u00f3digo del formulario, el cliente genera una petici\u00f3n HTTP POST dirigida a /login , que Nginx reenv\u00eda al demonio del backend. El demonio del backend construye una cadena con el formato nombre de usuario:contrase\u00f1a , aplica la codificaci\u00f3n Base64, genera una cookie llamada nginxauth con su valor establecido a la cadena codificada, y env\u00eda la cookie al cliente. Establece el flag httponly para evitar el uso de JavaScript para leer o manipular la cookie (protegiendo contra la vulnerabilidad cross-site scripting [XSS]). El cliente retransmite su solicitud original (del paso 1), esta vez incluyendo la cookie en el campo Cookie de la cabecera HTTP. Nginx reenv\u00eda la solicitud al demonio ldap-auth (como en el paso 2). El demonio ldap-auth decodifica la cookie y env\u00eda el nombre de usuario y la contrase\u00f1a al servidor LDAP en una solicitud de autenticaci\u00f3n. La siguiente acci\u00f3n depende de si el servidor LDAP autentifica con \u00e9xito al usuario: Si la autenticaci\u00f3n tiene \u00e9xito, el demonio ldap-auth env\u00eda el c\u00f3digo HTTP 200 a Nginx. Nginx solicita el recurso al demonio del backend y \u00e9ste devuelve el c\u00f3digo del sitio web. El archivo nginx-ldap-auth.conf incluye directivas para el almacenamiento en cach\u00e9 de los resultados del intento de autenticaci\u00f3n. Si la autenticaci\u00f3n falla, el demonio ldap-auth env\u00eda el c\u00f3digo HTTP 401 a Nginx. Nginx reenv\u00eda la solicitud al demonio del backend de nuevo (como en el paso 3), y el proceso se repite.","title":"M\u00f3dulo autenticaci\u00f3n LDAP en Nginx"},{"location":"P6.3/#despliegue-con-docker-de-nginx-demonio-de-autenticacion-ldap-openldap","text":"Para esta pr\u00e1ctica nos crearemos un directorio que contendr\u00e1 nuestro index.html, con un texto muy simple: $ mkdir app $ cat << EOF > app/index.html <html> <body> <h1>\u00a1Hola Mundo!</h1> </body> </html> EOF As\u00ed como otro directorio, con el contenido de la configuraci\u00f3n pertinente de Nginx: $ mkdir conf $ cat << EOF > conf/ldap_nginx.conf server { listen 8080 ; location = / { auth_request /auth-proxy ; } location = /auth-proxy { internal ; proxy_pass http://nginx-ldap:8888 ; # URL y puerto para conectarse al servidor LDAP proxy_set_header X-Ldap-URL \"ldap://openldap:1389\" ; # Base DN proxy_set_header X-Ldap-BaseDN \"dc=example,dc=org\" ; # Bind DN proxy_set_header X-Ldap-BindDN \"cn=admin,dc=example,dc=org\" ; # Bind password proxy_set_header X-Ldap-BindPass \"adminpassword\" ; } } En esta configuraci\u00f3n le decimos a Nginx: Que escuche en el puerto 8080 las peticiones HTTP Que cuando se acceda al sitio web, se solicite autorizaci\u00f3n en el directorio del sitio web llamado /auth-proxy Se crea un nuevo location para ese directorio /auth-proxy y que es donde se realizar\u00e1 la configuraci\u00f3n de c\u00f3mo conectarnos a nuestro openldap, de acuerdo con la documentaci\u00f3n oficial de Nginx a prop\u00f3sito de su m\u00f3dulo de autenticaci\u00f3n: Se indica la URL de nuestro openldap (es el nombre del contenedor que hemos levantado, ya que Docker tiene un DNS propio entre sus contenedores) El DN (Nombre distinguido) base sobre el que se realizar\u00e1n las b\u00fasquedas en openldap El usuario y contrase\u00f1a con el que nos conectaremos al openldap para realizar las consultas Y ahora, procedemos con el siguiente docker-compose.yml : version : '2' services : nginx-ldap : # (1) image : bitnami/nginx-ldap-auth-daemon # (2) ports : # (3) - 8888:8888 nginx : # (4) image : bitnami/nginx ports : - 8080:8080 volumes : # (5) - ./app:/app - ./conf/ldap_nginx.conf:/opt/bitnami/nginx/conf/server_blocks/ldap_nginx.conf openldap : # (6) image : bitnami/openldap ports : - '1389:1389' environment : # (7) - LDAP_ADMIN_USERNAME=admin - LDAP_ADMIN_PASSWORD=adminpassword - LDAP_USERS=customuser - LDAP_PASSWORDS=custompassword Nombre del contenedor Imagen que descargaremos del Dockerhub y a partir de la cual crearemos nuestro contenedor Puerto/s que se publicar\u00e1n para el contenedor Nombre del contenedor Vol\u00famenes o directorios compartidos entre nuestra m\u00e1quina y el contenedor Nombre del contenedor Variables de entorno utilizadas para la configuraci\u00f3n de este contenedor. Incluye credenciales del administrador de openldap, as\u00ed como un usuario que se crear\u00e1 en dicho openldap (pod\u00e9is cambiar usuario y contrase\u00f1a si quer\u00e9is, pero luego deb\u00e9is recordarlas) Tras esto s\u00f3lo queda ejecutar compose: docker-compose up Y comprobar que no se producen errores.","title":"Despliegue con Docker de NGINX + demonio de autenticaci\u00f3n LDAP + OpenLDAP"},{"location":"P6.3/#despliegue-con-docker-de-php-apache-con-autenticacion-ldap","text":"Creamos un directorio que se llame Practica6.3 En primer lugar, como es obvio, dentro del directorio creado debemos crear el index.php de nuestra aplicaci\u00f3n: <?php echo 'Well, hello LDAP authenticated user!' ; Dentro de nuestro directorio de trabajo, creado anteriormente, crearemos otro directorio llamado Docker y dentro de \u00e9l, un Dockerfile ( ./Docker/Dockerfile ) Nota En Linux, cuando queremos hacer referencia al directorio actual, lo hacemos con un punto . Si dentro de nuestro directorio actual tenemos una carpeta llamada prueba , podemos hacer referencia a ella como ./prueba , ya que el . hace referencia precisamente al directorio donde nos encontramos. Para completar este Dockerfile con las opciones/directivas adecuadas, leed los comentarios y pod\u00e9is apoyaros en la teor\u00eda o en este cheatsheet : # ./Docker/Dockerfile --> directorio donde se encuentra este archivo # Imagen base sobre la que vamos a trabajar ____ php:7-apache # Activamos el m\u00f3dulo LDAP de Apache ejecutand el siguiente comando ____ a2enmod authnz_ldap # A\u00f1adimos las reglas/configuraci\u00f3n de LDAP al directorio conf-enabled de Apache # (crearemos este archivo en el siguiente paso) ____ Docker/ldap-demo.conf /etc/apache2/conf-enabled/ # A\u00f1adimos ayuda de depuraci\u00f3n (debugging) en la configuraci\u00f3n de apache # En caso de necesitarlo, lo descomentamos para ejecutar el siguiente comando # ____ echo \"LogLevel debug\" >> apache2.conf # Establecemos el directorio de trabajo adecuado ____ /var/www/html/demo # Configuramos Apache para usar la configuraci\u00f3n ldap definida arriba, la copiamos de nuestro ordenador al contenedor ____ Docker/.htaccess ./.htaccess # Copiamos los archivos del proyecto que necesitamos, al contenedor ____ index.php ./ Tarea Completa el Dockerfile. Ahora crearemos el archivo ./Docker/ldap-demo.conf , que es la configuraci\u00f3n LDAP. Aqu\u00ed establecemos los criterios de conexi\u00f3n con el contenedor de Openldap, password y URL. Las directivas PassEnv al principio del archivo nos permiten omitir nuestras credenciales y pasarlas luego como variables de entorno al correr la imagen del contenedor: # ./Docker/ldap-demo.conf PassEnv LDAP_BIND_ON PassEnv LDAP_PASSWORD PassEnv LDAP_URL <AuthnProviderAlias ldap demo> AuthLDAPBindDN ${ LDAP_BIND_ON } AuthLDAPBindPassword ${ LDAP_PASSWORD } AuthLDAPURL ${ LDAP_URL } </AuthnProviderAlias> Creamos el archivo .htaccess: # ./.htaccess AuthBasicProvider demo AuthType Basic AuthName \"Protected Area\" Require valid-user Dentro de nuestro directorio de trabajo, construimos la imagen con el siguiente comando: docker build \\ -t docker-ldap \\ -f ./Docker/dockerfile \\ . Corremos el contenedor indicando las credenciales de nuestra cuenta LDAP mediante variables de entorno con la flag -e . Para este caso, vamos a probar un servidor LDAP externo, csimulando que tuvi\u00e9ramos que integrar nuestro despliegue con un servidor ya existente en la empresa. Utilizaremos un servidor p\u00fablico en Internet dedicado a pruebas: https://www.forumsys.com/2022/05/10/online-ldap-test-server/ docker run \\ -p 3000 :80 \\ --name ldap_demo \\ -e LDAP_BIND_ON = 'cn=read-only-admin,dc=example,dc=com' \\ -e LDAP_PASSWORD = 'password' \\ -e LDAP_URL = 'LDAP://ldap.forumsys.com/dc=example,dc=com' \\ docker-ldap No nos queda m\u00e1s que visitar http://IP-M\u00e1q-Debian:3000/demo . Si todo ha ido bien, nos solicitar\u00e1 nuestras credenciales para loguearnos contra el servidor openldap. Tarea Documenta todo el proceso de la pr\u00e1ctica, siguiendo los pasos y con las capturas y explicaciones pertinentes.","title":"Despliegue con Docker de PHP + Apache con autenticaci\u00f3n LDAP"},{"location":"P6.3/#referencias","text":"Para qu\u00e9 sirve el protocolo LDAP y c\u00f3mo funciona OpenLDAP conceptos te\u00f3ricos Using Nginx and NGINX to Authenticate Application Users with LDAP Simple Docker/Apache/PHP Authentication with LDAP bitnami/nginx-ldap-auth-daemon nginxinc/nginx-ldap-auth Dockerfile cheatsheet Difference between RUN and CMD in a Dockerfile","title":"Referencias"},{"location":"ServAplic/","text":"Servidores de aplicaciones Introducci\u00f3n Un servidor de aplicaciones es un marco mixto de software que permite tanto la creaci\u00f3n de aplicaciones web como un entorno de servidor para ejecutarlas. A menudo puede ser una pila compleja de diferentes elementos computacionales que ejecutan tareas espec\u00edficas que necesitan trabajar como uno solo para alimentar m\u00faltiples nubes y software y aplicaciones basadas en la web. Situado entre el servidor web y el nivel de backend del servidor de bases de datos, el servidor de aplicaciones es esencialmente un intermediario para el servidor de bases de datos y los usuarios de las aplicaciones empresariales o de consumo que soporta mediante el uso de varios protocolos e interfaces de programaci\u00f3n de aplicaciones (API). Es habitual que se utilice junto con un servidor web o que contenga un servidor web, por lo que ambos pueden converger y denominarse servidor de aplicaciones web. Tambi\u00e9n es lo suficientemente vers\u00e1til como para ser utilizado con otros servidores de aplicaciones simult\u00e1neamente. Los servidores de aplicaciones tambi\u00e9n pueden contener sus propias interfaces gr\u00e1ficas de usuario para su gesti\u00f3n a trav\u00e9s de PC, pero tambi\u00e9n pueden ocuparse de sus propios recursos, as\u00ed como del procesamiento de transacciones, la mensajer\u00eda, la agrupaci\u00f3n de recursos y conexiones, y la realizaci\u00f3n de tareas de seguridad. Servidor de aplicaciones Las aplicaciones vienen en todas las formas, tama\u00f1os y casos de uso. En un mundo en el que dependemos de una serie de procesos empresariales cr\u00edticos, los servidores de aplicaciones son los ordenadores de gran potencia que proporcionan recursos de aplicaciones a los usuarios y clientes web. Los servidores de aplicaciones, como ya hemos dicho, se sit\u00faan f\u00edsica o virtualmente entre los servidores de bases de datos que almacenan los datos de las aplicaciones y los servidores web que se comunican con los clientes. Los servidores de aplicaciones y el middleware af\u00edn son los sistemas operativos que soportan el desarrollo y la entrega de una aplicaci\u00f3n. Ya sea una aplicaci\u00f3n de escritorio, m\u00f3vil o web, los servidores de aplicaciones desempe\u00f1an un papel fundamental en la conexi\u00f3n de un mundo de dispositivos. Terminolog\u00eda de los servidores de aplicaciones T\u00e9rmino Descripci\u00f3n Servidor web Responsable de almacenar, procesar y entregar los datos de E/S de las p\u00e1ginas web Cliente web Punto final que intenta acceder a los recursos de la web o de la aplicaci\u00f3n HTTPS Protocolo de comunicaci\u00f3n seguro entre el servidor web y los clientes web JSON Lenguaje para el intercambio entre los servidores web y de aplicaciones L\u00f3gica de negocio Reglas para el almacenamiento de datos y la transferencia de recursos de la aplicaci\u00f3n Aplicaci\u00f3n Un programa de software o un sitio web unido a una base de datos El papel del servidor de aplicaciones en la arquitectura de servicios Cuando los usuarios de las aplicaciones, ya sea usuarios f\u00edsicos o los clientes web, solicitan acceso a una aplicaci\u00f3n, el servidor de aplicaciones suele hacer el trabajo pesado en el backend para almacenar y procesar las solicitudes din\u00e1micas de las aplicaciones. \u00bfPor qu\u00e9 necesitamos servidores de aplicaciones? Miles de millones de clientes web hacen peticiones HTTP cada d\u00eda, esperando un acceso instant\u00e1neo a la aplicaci\u00f3n en cuesti\u00f3n. Headspace durante la rutina de la ma\u00f1ana, Google Docs para el informe extenso, Twitter durante la pausa para el caf\u00e9, no importa la aplicaci\u00f3n en uso, est\u00e1 siendo consultada en un servidor de aplicaciones y devuelta a trav\u00e9s de un servidor web. Los servidores web se encargan de servir a los clientes web peticiones HTTP con respuestas HTTP. A diferencia de los servidores de aplicaciones, el dise\u00f1o del servidor web es lo suficientemente ligero como para procesar las solicitudes de datos est\u00e1ticos de varias aplicaciones (o sitios web), manteniendo la seguridad. Las peticiones din\u00e1micas, a menudo en forma de aplicaciones, requieren asistencia adicional. Los servidores de aplicaciones optimizan el tr\u00e1fico y a\u00f1aden seguridad Para conseguir una agilidad \u00f3ptima del servidor web, no sirve gestionar tanto las peticiones HTTP de los clientes web como pasar o almacenar recursos de m\u00faltiples sitios web. Los servidores de aplicaciones llenan este vac\u00edo con un dise\u00f1o de alta potencia construido para manejar las solicitudes de contenido web din\u00e1mico. Los servidores de aplicaciones tambi\u00e9n proporcionan redundancia de programas y una capa adicional de seguridad. Una vez desplegado entre una base de datos y un servidor web, el trabajo de preservar y duplicar la arquitectura de la aplicaci\u00f3n a trav\u00e9s de la red es m\u00e1s factible. El paso adicional entre las potenciales comunicaciones web maliciosas y las joyas de la corona en el servidor de base de datos a\u00f1ade una capa de seguridad adicional. Dado que los servidores de aplicaciones pueden procesar solicitudes de l\u00f3gica empresarial, un intento de inyecci\u00f3n SQL es tambi\u00e9n mucho m\u00e1s dif\u00edcil. Las organizaciones pueden proteger a\u00fan m\u00e1s sus datos con un servidor proxy inverso colocado delante de sus bases de datos. Los servidores proxy y las VPN pueden hacer maravillas para anonimizar y encriptar la comunicaci\u00f3n para proteger a los usuarios y los datos de la empresa. \u00bfC\u00f3mo funcionan los servidores de aplicaciones? Pongamos como ejemplo un servidor de aplicaciones Java. \u00bfQu\u00e9 son los servlets? Un servlet es un programa Java que se ejecuta en un servidor Web y construye o sirve p\u00e1ginas web. De esta forma se pueden construir p\u00e1ginas din\u00e1micas, basadas en diferentes fuentes variables: datos proporcionados por el usuario, fuentes de informaci\u00f3n variable (p\u00e1ginas de noticias, por ejemplo), o programas que extraigan informaci\u00f3n de bases de datos. Comparado con un CGI, un servlet es m\u00e1s sencillo de utilizar, m\u00e1s eficiente (se arranca un hilo por cada petici\u00f3n y no un proceso entero), m\u00e1s potente y portable. Con los servlets podremos, entre otras cosas, procesar, sincronizar y coordinar m\u00faltiples peticiones de clientes, reenviar peticiones a otros servlets o a otros servidores u otros. Como la mayor\u00eda de los servidores de hoy en d\u00eda, los servidores de aplicaciones contienen caracter\u00edsticas de seguridad, transacciones, servicios, clustering, diagn\u00f3sticos y bases de datos. En lo que se diferencian los servidores de aplicaciones es en su capacidad para procesar peticiones de servlets (programas Java) desde un servidor web. En la imagen anterior, se muestra el flujo general de los servidores de aplicaciones web: El cliente abre un navegador y solicita acceso a un sitio web El servidor web recibe la petici\u00f3n HTTP y responde con la p\u00e1gina web deseada El servidor web gestiona las peticiones de datos est\u00e1ticos, pero el cliente quiere utilizar una herramienta interactiva Al tratarse de una petici\u00f3n de datos din\u00e1micos, el servidor web transfiere la petici\u00f3n a un servidor de aplicaciones El servidor de aplicaciones recibe la petici\u00f3n HTTP y la convierte en una petici\u00f3n de servlet El servlet llega al servidor de la base de datos, y el servidor de aplicaciones recibe una respuesta del servlet El servidor de aplicaciones traduce la respuesta del servlet al formato HTTP para el acceso del cliente Al recibir una solicitud de servlet de un servidor web, el servidor de aplicaciones procesa la solicitud y responde al servidor web mediante la respuesta de servlet. Dado que los servidores de aplicaciones trabajan principalmente con peticiones de l\u00f3gica de negocio, el servidor web traduce la respuesta del servlet y pasa una respuesta HTTP accesible para el usuario. Servidor de aplicaciones Servidor web Dise\u00f1ado para Sirve peticiones HTTP y de otra l\u00f3gica de negocio Sirve peticiones HTTP Almacena y proporciona L\u00f3gica de negocio Contenido web est\u00e1tico La utilizaci\u00f3n de los recursos es Pesada Ligera Soporta Transacciones distribuidas y Enterprise JavaBeans (EJB) Servlets, Java Server Pages (JSP) y JSON Servidores de aplicaciones en la d\u00e9cada de 2020 El mercado de los servidores de aplicaciones espera crecer a una CAGR del 13,2%, pasando de cerca de 17.000 millones de d\u00f3lares en 2020 a 41.000 millones en 2026. El crecimiento continuo no es una sorpresa, ya que la conectividad a Internet y la dependencia de las aplicaciones crece. La migraci\u00f3n a las plataformas y servicios en la nube y el auge de los dispositivos IoT son dos impulsores clave en el mercado de infraestructura de aplicaciones y middleware moderno. A esto hay que a\u00f1adir un movimiento hacia las pol\u00edticas BYOD (Bring Your Own Device) y una fuerza de trabajo remota que depende de una mayor conectividad y eficiencia operativa. Servidores de aplicaciones: El mejor amigo de un servidor web Los servidores de aplicaciones son fundamentales para las exigencias actuales de interconexi\u00f3n. Las empresas, en \u00faltima instancia, est\u00e1n al servicio de los intereses de los clientes por lo que sin una conexi\u00f3n escalable y estable a los recursos de las aplicaciones, los clientes modernos huir\u00e1n sin mirar atr\u00e1s. Los servidores de aplicaciones asumen el papel de conector y mejor amigo de los servidores web. Cuando los servidores web tienen una petici\u00f3n del cliente que es demasiado para soportar, los servidores de aplicaciones hacen posible mantener la comunicaci\u00f3n sin problemas con el contenido web din\u00e1mico. \u00bfQu\u00e9 es el despliegue de aplicaciones web? El despliegue en el desarrollo de software y web significa pasar los cambios o actualizaciones de un entorno de funcionamiento a otro. Al configurar un sitio web, siempre se tendr\u00e1 el sitio web en vivo, que se llama el entorno en vivo o entorno de producci\u00f3n. Si se quiere tener la capacidad de hacer cambios sin afectar a un sitio web en producci\u00f3n, se puede (y se debe) a\u00f1adir entornos adicionales. Estos entornos se llaman entornos de desarrollo o entornos de despliegue. Los entornos de desarrollo adicionales suelen ser un entorno local, un entorno de desarrollo y un entorno de preparaci\u00f3n o preproducci\u00f3n. El n\u00famero de entornos que se necesitan depende de cada caso y de la complejidad del proyecto en el que se est\u00e9 trabajando. Aunque los modelos de despliegue pueden variar, el m\u00e1s com\u00fan es el cl\u00e1sico modelo de despliegue \"de izquierda a derecha\" cuando se trabaja con m\u00faltiples entornos de despliegue. En este modelo, los cambios se realizan en entornos locales, de desarrollo o de preparaci\u00f3n (dependiendo de la configuraci\u00f3n) y se van pasando de izquierda a derecha a trav\u00e9s de los diferentes entornos, terminando en el de producci\u00f3n. Una vez completado este proceso de despliegue, los nuevos cambios ser\u00e1n visibles en el entorno activo. En la imagen anterior se muestra una forma muy simplificada y cl\u00e1sica de manejar los despliegues cuando se trabaja con sitios web en un CMS. No necesariamente se necesitan todos los entornos anteriores, pero el proceso sigue siendo el mismo. Al utilizar m\u00faltiples entornos se obtiene una lista de ventajas - la principal es que se pueden hacer cambios sin que afecten a su sitio web en vivo. Una vez que los cambios se hacen, se prueban y est\u00e1n listos para ser pasados a producci\u00f3n, el proceso de despliegue se encarga del resto. \u00bfDe qu\u00e9 pasos consta el proceso despliegue? El flujo del proceso de despliegue consta de 5 pasos: Planificaci\u00f3n, desarrollo, pruebas, despliegue y supervisi\u00f3n. A continuaci\u00f3n nos adentraremos en cada uno de los 5 pasos, pero antes una nota r\u00e1pida. El flujo del proceso de despliegue que aparece a continuaci\u00f3n cubre los aspectos fundamentales, que se dividen en 5 pasos. Esto no significa que sea la \u00fanica manera de hacerlo - podr\u00eda haber un proceso mejor para cada caso. Es una simplificaci\u00f3n para que cubra las partes m\u00e1s importantes. Recordar tener un plan de despliegue de software Para asegurarse de que el proceso de despliegue se desarrolle con la mayor fluidez posible, lo mejor es tener un plan de despliegue que se siga en todo momento. Al tener un plan nos aseguramos de que todo se haga de la misma manera cada vez que se realicen cambios. Esto es especialmente \u00fatil cuando varios usuarios trabajan en el mismo proyecto. Un plan de despliegue debe incluir reglas sobre cu\u00e1ndo desplegar desde los entornos locales a los sitios de desarrollo o de puesta en escena, as\u00ed como horarios para cuando los nuevos cambios pueden ir a un entorno en vivo. Al tener un plan establecido, se reduce el riesgo de conflictos entre los diferentes cambios y se asegura que el proceso de despliegue sea lo m\u00e1s f\u00e1cil y fluido posible. Si se est\u00e1 trabajando en un proyecto de c\u00f3digo abierto, tambi\u00e9n da la oportunidad de hacer Release Candidates y dejar que la comunidad lo pruebe para detectar cualquier error que se pueda haber pasado por alto. Adem\u00e1s de un plan general, tambi\u00e9n es importante planificar cada uno de los cambios que se vaya a realizar. Este proceso ser\u00e1 muy r\u00e1pido para los cambios menores, pero deber\u00eda ser mucho m\u00e1s extenso para los grandes cambios. Si se planifica con mucha antelaci\u00f3n, se estar\u00e1 mucho m\u00e1s preparado para tener un proceso de despliegue sin problemas. El desarrollo propiamente dicho Una vez que se tenga el plan en marcha, es el momento de realizar el desarrollo real. Para garantizar que cualquier desarrollo pueda realizarse simult\u00e1neamente y sin romper nada, es importante trabajar \u00fanicamente en entornos locales o de desarrollo. Una vez que el proceso de desarrollo est\u00e1 hecho, es el momento de empezar a probar y desplegar los cambios a trav\u00e9s de la configuraci\u00f3n de su entorno. Probar los cambios Probar los cambios es crucial para garantizar que no haya errores en el entorno de producci\u00f3n final. Pero las pruebas no pueden completarse sin desplegar los cambios en nuevos entornos. Una vez que se haya comprobado que todos los cambios funcionan en el entorno local o de desarrollo, es el momento de desplegar los cambios en el siguiente entorno. Esto debe hacerse hasta el entorno de preproducci\u00f3n, donde se deben realizar las pruebas finales de control de calidad. Si todo est\u00e1 correctamente probado y funciona en un entorno parecido al entorno real, es el momento de desplegarlo en vivo. Si se descubren errores por el camino en cualquier entorno, es importante tener un plan para manejarlos. Por lo general, cualquier cambio que no pase las pruebas en el entorno de ensayo debe ser enviado de nuevo a la fase de desarrollo y -una vez corregido- volver a trabajar en los entornos. Desplegar los cambios en el entorno real Una vez que se han realizado todas las pruebas en los entornos anteriores y se han corregido los errores, es el momento de desplegar los cambios en el entorno real. Esto deber\u00eda ser algo bastante seguro, pero todos los que han trabajado en el desarrollo de software saben que algo puede salir mal. As\u00ed que, aunque es f\u00e1cil detenerse aqu\u00ed, es importante incluir el \u00faltimo paso del proceso: la monitorizaci\u00f3n. Supervisar los cambios Una vez que los nuevos cambios est\u00e9n en marcha y los usuarios reales utilicen activamente el sitio web o la aplicaci\u00f3n, es importante supervisar que todo funcione seg\u00fan lo previsto. Independientemente de la planificaci\u00f3n realizada, existe la posibilidad de que los usuarios se encuentren con problemas o realicen acciones que usted no hab\u00eda previsto durante la planificaci\u00f3n y el desarrollo. Un buen consejo para la monitorizaci\u00f3n es planificar los lanzamientos para los momentos en los que la menor cantidad de usuarios lo noten y en los que se tengan recursos de desarrollo listos en caso de que haya que arreglar algo. De este modo, el n\u00famero de usuarios afectados por cualquier error ser\u00e1 m\u00ednimo y se tendr\u00e1 gente preparada para arreglarlo o revertir los cambios si es necesario. Si se han de revertir los cambios, es importante mantener la calma y tener un proceso para manejarlo con la misma minuciosidad con la que se manejan los despliegues. Diferentes tipos de despliegue Cuando se trata del tipo de despliegue, a menudo se divide en dos partes. Por lo general, se dividir\u00e1 entre metadatos y contenido, ya que estos tienen diferentes impactos en un nuevo entorno y deben ser manejados de manera diferente. Despliegue de metadatos Los metadatos incluyen los cambios en el c\u00f3digo, las plantillas, las hojas de estilo, los archivos, etc. Estos cambios a menudo requerir\u00e1n una comprobaci\u00f3n de validaci\u00f3n entre entornos para ver si tiene alg\u00fan conflicto imprevisto que deba resolverse. Muchas herramientas de despliegue incluyen comprobaciones de coherencia y ayudan a guiarte en caso de conflictos. Despliegue de contenidos El contenido, como el texto, las im\u00e1genes y los v\u00eddeos, se maneja de forma diferente durante el despliegue, ya que es menos complicado moverlo entre entornos que los metadatos. Por esa raz\u00f3n, a menudo ver\u00e1s que las herramientas de despliegue hacen que el despliegue de contenido sea accesible para los editores de contenido y no s\u00f3lo para los desarrolladores. De esta manera, un editor de contenidos no depende de un desarrollador cuando se trata de enviar nuevos contenidos a un entorno activo. Mejores pr\u00e1cticas de despliegue Cuando se trabaja con entornos de despliegue, es importante, como se ha mencionado anteriormente, tener un plan y un proceso claro para ello en el equipo. Para ampliar ese proceso hemos reunido algunas mejores pr\u00e1cticas que son buenas para implementar como parte de su proceso. Se ha de tener en cuenta que las siguientes pr\u00e1cticas recomendadas se refieren principalmente al desarrollo de software y de la web. Si se est\u00e1n llevando a cabo otros tipos de desarrollo puede haber otras cosas a considerar en el flujo de trabajo de despliegue. Utilizar Git Esto puede parecer obvio, pero tener un sistema de control de versiones es inestimable para cualquier flujo de trabajo de despliegue. Sin \u00e9l, es probable que se produzcan errores si se trabaja en equipo. Incluso si eres el \u00fanico desarrollador que trabaja en un proyecto, es muy recomendable utilizar Git en caso de que necesites volver a versiones anteriores o si alguien nuevo se une a tu equipo. Sin Git ser\u00e1 dif\u00edcil asegurar la consistencia en el flujo de trabajo de despliegue y puede llevar a que se cometan m\u00e1s errores por desplegar c\u00f3digo inacabado o por no tener a todos los miembros del equipo trabajando en la misma versi\u00f3n del c\u00f3digo. Trabajar en ramas Como regla general, tu equipo deber\u00eda trabajar en ramas. Hacerlo as\u00ed permitir\u00e1 trabajar en varias cosas al mismo tiempo sin que se afecten entre s\u00ed. Un ejemplo es cuando se encuentra un error que debe ser corregido. Si un desarrollador est\u00e1 utilizando una rama para trabajar en una nueva caracter\u00edstica, puede hacer r\u00e1pidamente una nueva rama del entorno de desarrollo para trabajar en el error. De este modo, habr\u00e1 dos ramas diferentes que no chocar\u00e1n ni crear\u00e1n posibles conflictos de fusi\u00f3n m\u00e1s adelante. Trabajar con ramas tambi\u00e9n ayuda al equipo con las preguntas y respuestas a la hora de desplegar en un entorno de preproducci\u00f3n. Tener los cambios en ramas separadas y fusionarlas dar\u00e1 a los testers una mejor visi\u00f3n de lo que se empuj\u00f3 (se hizo push) y lo que deben probar. Utilizar un entorno local como entorno de desarrollo Aunque es posible trabajar directamente en un entorno de desarrollo, en la mayor\u00eda de los casos se ahorrar\u00e1 mucho tiempo trabajando localmente. Al instalar el sitio web o el software de forma local, se podr\u00e1 trabajar de forma m\u00e1s eficiente y acelerar las pruebas y la verificaci\u00f3n del c\u00f3digo. En primer lugar, no tienes hay que confirmar, empujar y desplegar constantemente un cambio antes de poder verificar si funciona. Y cuando algo no funciona (esto nos pasa a todos) tendr\u00e1s que revertirlo, empujarlo de nuevo y volver a desplegarlo. En lugar de eso, puedes simplemente ejecutarlo todo localmente y, una vez que funcione como es debido, puedes empujarlo directamente al entorno de preparaci\u00f3n para una prueba m\u00e1s rigurosa. Revisar las diferencias antes de desplegarlo en el entorno real Una vez que el equipo de pruebas se haya asegurado de que todo funciona en el entorno de pruebas, es el momento de desplegar el c\u00f3digo en el entorno real. Pero antes de hacer el despliegue final, es importante hacer una revisi\u00f3n final de las diferencias entre el entorno actual en producci\u00f3n y el entorno de desarrollo del que se parte. Incluso despu\u00e9s de las pruebas exhaustivas y la garant\u00eda de calidad, las cosas pueden ir mal tan pronto como se llega al entorno real. Y una vez que eso sucede, a menudo puede ser muy estresante implementar correcciones r\u00e1pidas o hacer una reversi\u00f3n completa de la versi\u00f3n. Por lo general, se querr\u00e1 evitar esto a toda costa, por lo que es muy recomendable hacer una revisi\u00f3n final del c\u00f3digo antes de pulsar el bot\u00f3n de despliegue. Considerar tener grupos de usuarios con diferentes permisos Mientras que cualquier desarrollador debe ser capaz de empujar los cambios a los entornos de test, puede ser una buena idea para restringir qui\u00e9n puede desplegarlos en vivo. Para los equipos m\u00e1s peque\u00f1os, esto puede no tener mucho sentido, ya que puede crear un cuello de botella para implantar nuevos cambios. Pero si se trata de un equipo m\u00e1s grande con un nivel de experiencia muy variado entre los miembros del equipo, puede ser una gran idea dejar que s\u00f3lo los desarrolladores senior desplieguen en el entorno de producci\u00f3n. Esto asegura efectivamente un mayor nivel de control sobre el flujo de releases y tambi\u00e9n significa que al menos un par de ojos senior han visto lo que est\u00e1 pasando en el entorno real. Si lo que se tiene es un enfoque muy iterativo con lanzamientos r\u00e1pidos como el utilizado en la metodolog\u00eda CD (Continous Delivery), esto podr\u00eda ralentizarlo todo demasiado . Aun as\u00ed, dado que los cambios que se empujan son normalmente m\u00e1s peque\u00f1os con este enfoque, probablemente no se sufrir\u00e1n grandes retrasos. Y si significa detectar algunos errores m\u00e1s, el tiempo que se ahorra al no tener que corregir errores compensar\u00e1 el tiempo invertido. Hablando de romper cosas... Mantener la calma, incluso si algo se rompe Acabas de desplegar en tu entorno de producci\u00f3n y ahora tu sitio web est\u00e1 roto. Menuda liada, \u00bfahora qu\u00e9 se hace? Desgraciadamente, estas cosas ocurren - no importa lo cuidadoso que se sea. Pero en lugar de entrar en p\u00e1nico y aplicar hotfixes o retroceder inmediatamente, es importante mantener la calma y asegurarse de que lo que est\u00e1 haciendo no va a romper las cosas a\u00fan m\u00e1s. En primer lugar, se deber\u00eda comprobar si es posible realizar una reversi\u00f3n o rollback y si realmente se arreglar\u00eda algo. En algunas situaciones, es posible que se hayan hecho cambios que son irreversibles y un rollback s\u00f3lo causar\u00eda problemas a\u00fan mayores. Tambi\u00e9n hay que comprobar si lo que se ha roto es una caracter\u00edstica existente o nueva. De nuevo, si la cosa que se rompi\u00f3 no era parte de la nueva versi\u00f3n, probablemente no servir\u00e1 de nada hacer un rollback . As\u00ed que en lugar de entrar en p\u00e1nico, se debe tener un plan preparado y respirar hondo antes de ponerse a trabajar en la b\u00fasqueda de una soluci\u00f3n. Puede parecer sencillo, pero puede ayudar a salir de una mala situaci\u00f3n mucho m\u00e1s r\u00e1pido que si lanz\u00e1ndose directamente. \u00bfA qu\u00e9 hora del d\u00eda se deben desplegar los cambios? En caso de que algo se rompa al desplegar en el entorno de producci\u00f3n, es importante encontrar el mejor momento para hacerlo. Y aunque este momento var\u00eda mucho de un proyecto a otro, hay dos preguntas que pueden hacerse para determinar cu\u00e1ndo desplegar los cambios: \u00bfCu\u00e1ndo tiene la menor cantidad de usuarios activos? \u00bfCu\u00e1ndo tiene a alguien preparado para supervisar y solucionar los problemas despu\u00e9s del despliegue? \u00bfCu\u00e1ndo tiene el menor n\u00famero de usuarios activos? Por lo general, lo que se quiere es que el menor n\u00famero posible de personas se vea afectado por sus nuevos cambios. Por lo tanto, como regla general, debe buscar cualquier momento del d\u00eda en el que el menor n\u00famero de usuarios est\u00e9 utilizando activamente el sitio web o software. En el caso de los sitios web, esto puede hacerse consultando las herramientas de an\u00e1lisis de datos que se tengan en marcha, por ejemplo, Google Analytics. All\u00ed se podr\u00e1n crear informes personalizados que muestren a qu\u00e9 hora del d\u00edase tiene menos tr\u00e1fico, as\u00ed como identificar las horas punta en las que definitivamente no se deber\u00eda hacer ning\u00fan cambio. Adem\u00e1s de mirar la hora del d\u00eda, tambi\u00e9n puede valer la pena mirar c\u00f3mo se reparte la actividad de los usuarios entre los d\u00edas de la semana. Este an\u00e1lisis es muy bueno, pero a menudo acabar\u00e1 con la misma respuesta: Deber\u00edan publicarse los cambios durante la noche. Y aunque esto podr\u00eda parecer una gran idea si s\u00f3lo nos fij\u00e1ramos en esta cuesti\u00f3n, es importante que tambi\u00e9n tengamos en cuenta la siguiente. \u00bfHay alguien despierto y preparado para solucionar posibles problemas en ese momento? Si la respuesta es no, entonces desplegar los cambios en mitad de la noche podr\u00eda no ser la mejor idea. En su lugar, se deber\u00edan identificar las franjas horarias en las que puedas encontrar el mejor equilibrio entre el n\u00famero de usuarios activos y los desarrolladores dispuestos a solucionar los problemas. Esto variar\u00e1 mucho dependiendo del proyecto y del equipo, pero en general, se deber\u00edan encontrar algunas opciones. Y si ya se tiene un horario fijo de despliegue, incluso puede convencerse al equipo de que est\u00e9 listo a horas extra\u00f1as del d\u00eda. Es mucho m\u00e1s f\u00e1cil convencer a alguien de que venga unas horas antes si sabe que s\u00f3lo ocurre una vez cada ciclo o sprint. Es por este motivo que en muchas empresas se trabaja con guardias rotativas para ofrecer una disponibilidad total. Info Aunque no hay un momento perfecto para el despliegue, definitivamente hay momentos que son mejores que otros. \u00bfCu\u00e1les son las ventajas del despliegue y de los entornos m\u00faltiples? Reducci\u00f3n del riesgo de romper un sitio web en producci\u00f3n Una de las principales razones para utilizar m\u00faltiples entornos y confiar en el despliegue es reducir el riesgo de que los cambios tengan un impacto negativo en un sitio web en vivo. Mientras que los cambios menores se pueden hacer f\u00e1cilmente directamente en un sitio web en vivo, los cambios m\u00e1s grandes se pueden hacer en entornos separados sin el riesgo de romper nada en el entorno en vivo. Tener varios usuarios trabajando en el mismo sitio web tambi\u00e9n garantiza que nadie se arriesgue a romper algo debido a los cambios de otro usuario. Ahorro de tiempo Sin la preocupaci\u00f3n de romper algo en un sitio web en vivo, se pueden realizar los cambios en el orden que se prefiera. Esto significa que se puede optimizar el flujo de trabajo para realizar los cambios sin tener en cuenta el aspecto o el funcionamiento del sitio web mientras se lleva a cabo. Si se trabaja en un entorno local tambi\u00e9n existe la ventaja de que los cambios se procesan m\u00e1s r\u00e1pido y no hay dependencias de ning\u00fan problema de conectividad. A la hora de desplegar los cambios, tambi\u00e9n se ahorrar\u00e1 tiempo, ya que se podr\u00e1n realizar todos los cambios al mismo tiempo en lugar de tener que hacerlo en varios pasos m\u00e1s peque\u00f1os. El contenido sensible al tiempo es m\u00e1s f\u00e1cil de gestionar Si se est\u00e1n llevando a cabo campa\u00f1as que son sensibles al tiempo y que s\u00f3lo pueden ponerse en marcha a partir de un determinado d\u00eda u hora, entonces la ejecuci\u00f3n de m\u00faltiples entornos y el uso del despliegue pueden ahorrar una gran cantidad de estr\u00e9s. Al crear todo el contenido en un entorno de puesta en escena/preprod (o similar) puedes terminar tu campa\u00f1a sin preocuparte de que sea visible para tus usuarios. Y cuando llegue el momento de lanzarla, podr\u00e1 hacerla visible en muy poco tiempo despleg\u00e1ndola en su entorno real. Y si la herramienta de despliegue incluye roles de usuario con configuraci\u00f3n de permisos, es posible que un editor de contenidos haga todo esto -incluyendo el despliegue de los cambios- sin involucrar a un desarrollador en el proceso. Despliegue de aplicaciones Java Introducci\u00f3n En el lado del servidor, tenemos que conseguir que nuestro servidor HTTP sea capaz de ejecutar programas de aplicaci\u00f3n que recojan los par\u00e1metros de peticiones del cliente, los procesen y devuelvan al servidor un documento que \u00e9ste pasar\u00e1 a su vez al cliente. As\u00ed, para el cliente el servidor no habr\u00e1 hecho nada distinto a lo estipulado en el protocolo HTTP, pero el servidor podr\u00e1 valerse de herramientas externas para procesar y servir la petici\u00f3n solicitada, pudiendo as\u00ed no limitarse a servir p\u00e1ginas est\u00e1ticas, sino utilizar otras aplicaciones (servlets, JSP...) para servir documentos con contenido din\u00e1mico. Los programas de aplicaci\u00f3n son t\u00edpicamente programas que realizan consultas a bases de datos, procesan la informaci\u00f3n resultante y devuelven la salida al servidor, entre otras tareas. Vamos a centrarnos en las aplicaciones web JavaEE, en las que los componentes din\u00e1micos que recibir\u00e1n las peticiones HTTP en el servidor ser\u00e1n los servlets y JSPs. Estos componentes podr\u00e1n analizar esta petici\u00f3n y utilizar otros componentes Java para realizar las acciones necesarias (beans, EJBs, etc). Estructura de una aplicaci\u00f3n Java Una aplicaci\u00f3n web JavaEE que utilice servlets o p\u00e1ginas JSP debe tener una estructura de ficheros y directorios determinada: En el directorio ra\u00edz de la aplicaci\u00f3n se colocan las p\u00e1ginas HTML o JSP (podemos dividirlas tambi\u00e9n en directorios si queremos) Colgando del directorio inicial de la aplicaci\u00f3n, se tiene un directorio WEB-INF, que contiene la informaci\u00f3n Web relevante para la aplicaci\u00f3n. El resto de elementos de la aplicaci\u00f3n (im\u00e1genes, etc), podemos estructurarlos como nos convenga. Esta estructura estar\u00e1 contenida dentro de alg\u00fan directorio, que ser\u00e1 el directorio correspondiente a la aplicaci\u00f3n Web, y que podremos, si lo hacemos convenientemente, copiar en el servidor que nos convenga. Es decir, cualquier servidor Web JavaEE soporta esta estructura en una aplicaci\u00f3n Web, s\u00f3lo tendremos que copiarla en el directorio adecuado de cada servidor. Cada aplicaci\u00f3n web JavaEE es un contexto, una unidad que comprende un conjunto de recursos, clases Java y su configuraci\u00f3n. Cuando hablemos de contexto, nos estaremos refiriendo a la aplicaci\u00f3n web en conjunto. Empaquetamiento Una forma de distribuir aplicaciones Web es empaquetar toda la aplicaci\u00f3n (a partir de su directorio inicial) dentro de un fichero WAR (de forma parecida a como se hace con un TAR o un JAR), y distribuir dicho fichero. Podemos crear un fichero WAR de la misma forma que creamos un JAR, utilizando la herramienta JAR. Estos ficheros WAR son un est\u00e1ndar de JavaEE, por lo que podremos utilizarlos en los diferentes servidores de aplicaciones JavaEE existentes. Despliegue de archivos WAR Los archivos WAR, son un tipo especial de JAR utilizado para distribuir los artefactos o contenido de las aplicaciones Web en tecnolog\u00eda JEE: p\u00e1ginas Web HTML o JSP,clases Java, servlets Java, archivos XML, librer\u00edas de etiquetas (tag libraries) y otros recursos. El empaquetamiento en archivos WAR es algo est\u00e1ndar, pero no as\u00ed el proceso de despliegue, que es dependiente del servidor. No obstante, la mayor\u00eda de servidores JavaEE funcionan en este aspecto de modo similar: permiten desplegar las aplicaciones desde una consola de administraci\u00f3n y tambi\u00e9n \"dejando caer\" el fichero en determinado directorio. Maven Maven es una herramienta open-source, que se cre\u00f3 en 2001 con el objetivo de simplificar los procesos de build (compilar y generar ejecutables a partir del c\u00f3digo fuente). Antes de existir Maven, si quer\u00edamos compilar y generar ejecutables de un proyecto, ten\u00edamos que analizar qu\u00e9 partes de c\u00f3digo se deb\u00edan compilar, qu\u00e9 librer\u00edas utilizaba el c\u00f3digo, d\u00f3nde incluirlas, qu\u00e9 dependencias de compilaci\u00f3n hab\u00eda en el proyecto\u2026 En el mejor de los casos, se empleaban unos pocos minutos para saber c\u00f3mo hacer una build del proyecto. En el peor de los casos, el proceso de build era tan complejo que un desarrollador pod\u00eda tardar horas en saber c\u00f3mo compilar y generar los ejecutables a partir del c\u00f3digo. Ahora, la build de cualquier proyecto Maven, independientemente de sus m\u00f3dulos, dependencias o librer\u00edas, consiste simplemente en ejecutar el comando mvn install . Por otra parte, antes de Maven, cada vez que sal\u00eda una nueva versi\u00f3n de un analizador est\u00e1tico de c\u00f3digo, de un framework de pruebas unitarias (como JUnit) o cualquier librer\u00eda, hab\u00eda que parar todo el desarrollo para reajustar el proceso de build a las nuevas necesidades. Y\u2026 \u00bfc\u00f3mo se ejecutaban las pruebas? \u00bfC\u00f3mo se generaban informes? Sin Maven, en cada proyecto esto se hac\u00eda de distinta manera. Lo cierto es que Maven es mucho m\u00e1s que una herramienta que hace builds del c\u00f3digo. Podr\u00edamos decir, que Maven es una herramienta capaz de gestionar un proyecto software completo , desde la etapa en la que se comprueba que el c\u00f3digo es correcto, hasta que se despliega la aplicaci\u00f3n, pasando por la ejecuci\u00f3n de pruebas y generaci\u00f3n de informes y documentaci\u00f3n. Para ello, en Maven se definen tres ciclos de build del software con una serie de etapas diferenciadas. Por ejemplo el ciclo por defecto tiene las etapas de: Validaci\u00f3n (validate): Validar que el proyecto es correcto. Compilaci\u00f3n (compile). Test (test): Probar el c\u00f3digo fuente usando un framework de pruebas unitarias. Empaquetar (package): Empaquetar el c\u00f3digo compilado y transformarlo en alg\u00fan formato tipo .jar o .war. Pruebas de integraci\u00f3n (integration-test): Procesar y desplegar el c\u00f3digo en alg\u00fan entorno donde se puedan ejecutar las pruebas de integraci\u00f3n. Verificar que el c\u00f3digo empaquetado es v\u00e1lido y cumple los criterios de calidad (verify). Instalar el c\u00f3digo empaquetado en el repositorio local de Maven, para usarlo como dependencia de otros proyectos (install). Desplegar el c\u00f3digo a un entorno (deploy). Para poder llevar a cabo alguna de estas fases en nuestro c\u00f3digo, tan solo tendremos que ejecutar mvn y el nombre de la fase (la palabra que puse entre par\u00e9ntesis). Adem\u00e1s van en cadena, es decir, si empaquetamos el c\u00f3digo (package), Maven ejecutar\u00e1 desde la fase de validaci\u00f3n (validate) a empaquetaci\u00f3n (package). As\u00ed de simple. Por otra parte, con Maven la gesti\u00f3n de dependencias entre m\u00f3dulos y distintas versiones de librer\u00edas se hace muy sencilla. En este caso, solo tenemos que indicar los m\u00f3dulos que componen el proyecto, o qu\u00e9 librer\u00edas utiliza el software que estamos desarrollando en un fichero de configuraci\u00f3n de Maven del proyecto llamado POM (Project Object Module). Adem\u00e1s, en el caso de las librer\u00edas, no tienes ni tan siquiera que descargarlas a mano. Maven posee un repositorio remoto (Maven central) donde se encuentran la mayor\u00eda de librer\u00edas que se utilizan en los desarrollos de software, y que la propia herramienta se descarga cuando sea necesario. Digamos que Maven aporta una sem\u00e1ntica com\u00fan al proceso de build y desarrollo del software. Incluso, establece una estructura com\u00fan de directorios para todos los proyectos. Por ejemplo el c\u00f3digo estar\u00e1 en ${ra\u00edz del proyecto}/src/main/java , los recursos en ${ra\u00edz del proyecto }/src/main/resources . Los tests est\u00e1n en ${ra\u00edz del proyecto }/src/test . Despliegue de aplicaciones Node.js con Express \u00bfQu\u00e9 es Node.js? Node JS es un entorno de ejecuci\u00f3n de JavaScript r\u00e1pido que utilizamos para construir aplicaciones del lado del servidor, pero por s\u00ed mismono sabe c\u00f3mo servir archivos, manejar peticiones ni m\u00e9todos HTTP, as\u00ed que aqu\u00ed es donde entra en juego Express JS. Node.js no es un lenguaje de programaci\u00f3n. M\u00e1s bien, es un entorno de ejecuci\u00f3n que se utiliza para ejecutar JavaScript fuera del navegador. Node.js tampoco es un framework (una plataforma para desarrollar aplicaciones de software). El tiempo de ejecuci\u00f3n de Node.js se construye sobre un lenguaje de programaci\u00f3n -en este caso, JavaScript- y ayuda a la ejecuci\u00f3n de los propios frameworks. En resumen, Node.js no es un lenguaje de programaci\u00f3n ni un marco de trabajo; es un entorno para ellos. \u00bfQu\u00e9 es Express? Express JS es un framework de Node.js dise\u00f1ado para construir aplicaciones web de API's y aplicaciones m\u00f3viles multiplataforma de forma r\u00e1pida y hacer que Node.js sea f\u00e1cil. \u00bfQu\u00e9 es npm? NPM responde a las siglas de Node Package Manager o manejador de paquetes de node, es la herramienta por defecto de JavaScript para la tarea de compartir e instalar paquetes. Tal como reza su documentaci\u00f3n, npm se compone de al menos dos partes principales. Un repositorio online para publicar paquetes de software libre para ser utilizados en proyectos Node.js Una herramienta para la terminal (command line utility) para interactuar con dicho repositorio que te ayuda a la instalaci\u00f3n de utilidades, manejo de dependencias y la publicaci\u00f3n de paquetes. As\u00ed pues, NPM es un gestor de paquetes para Javascript. Es una especie de Maven para paquetes Javascript, es decir, sirve para instalar y gestionar versiones de paquetes y librer\u00edas js. NPM lleva mucho tiempo siendo el referente en cuanto a gestores de paquetes javascript, pero desde hace un tiempo le ha salido un competidor: Yarn. Los de yarn aseguran que su gestor de librer\u00edas js es mucho m\u00e1s r\u00e1pido y potente, pero de momento el uso de NPM es mayoritario. package.json Cada proyecto en JavaScript puede enfocarse como un paquete npm con su propia informaci\u00f3n de paquete y su archivo package.json para describir el proyecto. package.json se generar\u00e1 cuando se ejecute npm init para inicializar un proyecto JavaScript/Node.js, con los siguientes metadatos b\u00e1sicos proporcionados por los desarrolladores: name: el nombre de la librer\u00eda/proyecto JavaScript version: la versi\u00f3n del proyecto. description: la descripci\u00f3n del proyecto license: la licencia del proyecto NPM scripts package.json tambi\u00e9n soporta la propiedad scripts que puede definirse para ejecutar herramientas de l\u00ednea de comandos que se instalan en el contexto local del proyecto. Por ejemplo, la porci\u00f3n de scripts de un proyecto npm puede tener un aspecto similar a este: { \"scripts\" : { \"build\" : \"tsc\" , \"format\" : \"prettier --write **/*.ts\" , \"format-check\" : \"prettier --check **/*.ts\" , \"lint\" : \"eslint src/**/*.ts\" , \"pack\" : \"ncc build\" , \"test\" : \"jest\" , \"all\" : \"npm run build && npm run format && npm run lint && npm run pack && npm test\" } } Con eslint, prettier, ncc, jest no necesariamente instalados como ejecutables globales sino como locales de tu proyecto dentro de node_modules/.bin/ . CI/CD (Continous Integration/Continous Deployment-Delivery) La CI/CD es un m\u00e9todo para distribuir las aplicaciones a los clientes con frecuencia mediante el uso de la automatizaci\u00f3n en las etapas del desarrollo de aplicaciones. Los principales conceptos que se le atribuyen son la integraci\u00f3n, la distribuci\u00f3n y la implementaci\u00f3n continuas. Se trata de una soluci\u00f3n para los problemas que puede generar la integraci\u00f3n del c\u00f3digo nuevo para los equipos de desarrollo y de operaciones (tambi\u00e9n conocida como \"el infierno de la integraci\u00f3n\"). En concreto, el proceso de integraci\u00f3n y distribuci\u00f3n continuas incorpora la automatizaci\u00f3n y la supervisi\u00f3n permanentes en todo el ciclo de vida de las aplicaciones, desde las etapas de integraci\u00f3n y prueba hasta las de distribuci\u00f3n e implementaci\u00f3n. Este conjunto de pr\u00e1cticas se conoce como \"canales de CI/CD\" y cuenta con el respaldo de los equipos de desarrollo y de operaciones que trabajan en conjunto de manera \u00e1gil, con un enfoque de DevOps o de ingenier\u00eda de confiabilidad del sitio (SRE). \u00bfCu\u00e1l es la diferencia entre la integraci\u00f3n, la distribuci\u00f3n y la implementaci\u00f3n continuas? Estas siglas tienen diferentes significados. \"CI\" siempre se refiere a la integraci\u00f3n continua, que es un proceso de automatizaci\u00f3n para los desarrolladores. El \u00e9xito de la CI implica que se dise\u00f1en, prueben y combinen los cambios nuevos en el c\u00f3digo de una aplicaci\u00f3n con regularidad en un repositorio compartido. Supone una soluci\u00f3n al problema de que se desarrollen demasiadas divisiones de una aplicaci\u00f3n al mismo tiempo, que luego podr\u00edan entrar en conflicto entre s\u00ed. La sigla \"CD\" se refiere a la distribuci\u00f3n o la implementaci\u00f3n continuas, y se trata de conceptos relacionados que suelen usarse indistintamente. Ambos se refieren a la automatizaci\u00f3n de las etapas posteriores del proceso, pero a veces se usan por separado para explicar hasta d\u00f3nde llega la automatizaci\u00f3n. Por lo general, la distribuci\u00f3n continua se refiere a que los cambios que implementa un desarrollador en una aplicaci\u00f3n se someten a pruebas autom\u00e1ticas de errores y se cargan en un repositorio (como GitHub o un registro de contenedores), para que luego el equipo de operaciones pueda implementarlos en un entorno de producci\u00f3n en vivo. Es una soluci\u00f3n al problema de la falta de supervisi\u00f3n y comunicaci\u00f3n entre los equipos comerciales y de desarrollo, as\u00ed que su prop\u00f3sito es garantizar que la implementaci\u00f3n del c\u00f3digo nuevo se lleve a cabo con el m\u00ednimo esfuerzo. La implementaci\u00f3n continua (la otra definici\u00f3n de \"CD\") hace referencia al lanzamiento autom\u00e1tico de los cambios que implementa el desarrollador desde el repositorio hasta la producci\u00f3n, para ponerlos a disposici\u00f3n de los clientes. As\u00ed ya no se sobrecarga a los equipos de operaciones con procesos manuales que retrasan la distribuci\u00f3n de las aplicaciones. Con este tipo de implementaci\u00f3n, se aprovechan los beneficios de la distribuci\u00f3n continua y se automatiza la siguiente etapa del proceso. La CI/CD puede incluir solamente la integraci\u00f3n y la distribuci\u00f3n continuas, o las tres pr\u00e1cticas vinculadas, con la implementaci\u00f3n continua. Para complicar un poco m\u00e1s las cosas, a veces se utiliza el t\u00e9rmino \"distribuci\u00f3n continua\" para abarcar tambi\u00e9n los procesos de la implementaci\u00f3n continua. En realidad, no vale la pena profundizar en la sem\u00e1ntica. Solo debe recordar que la integraci\u00f3n y la distribuci\u00f3n continuas son un proceso que suele percibirse como una canalizaci\u00f3n e implica incorporar un alto nivel de automatizaci\u00f3n permanente y supervisi\u00f3n constante al desarrollo de las aplicaciones. El significado de los t\u00e9rminos var\u00eda en cada caso y depende de la cantidad de automatizaci\u00f3n que se haya incorporado a la canalizaci\u00f3n de integraci\u00f3n y distribuci\u00f3n continuas. Muchas empresas comienzan con la incorporaci\u00f3n de la CI, y luego van automatizando la distribuci\u00f3n y la implementaci\u00f3n, por ejemplo, con las aplicaciones desarrolladas directamente en la nube. Nuestros especialistas pueden ayudar a que su empresa desarrolle las pr\u00e1cticas, las herramientas y la cultura necesarias para modernizar las aplicaciones actuales y dise\u00f1ar otras nuevas con mayor eficiencia. Integraci\u00f3n continua El objetivo del dise\u00f1o de las aplicaciones modernas es que los desarrolladores puedan trabajar de forma simult\u00e1nea en distintas funciones de la misma aplicaci\u00f3n. Sin embargo, si una empresa fusiona todo el c\u00f3digo fuente diversificado en un solo d\u00eda (conocido como el \"d\u00eda de la fusi\u00f3n\"), las tareas pueden tornarse tediosas, manuales y muy lentas. Esto se debe a que si un desarrollador que trabaja de forma aislada implementa un cambio en una aplicaci\u00f3n, existe la posibilidad de que entre en conflicto con las modificaciones que otros desarrolladores implementaron al mismo tiempo. El problema puede agravarse a\u00fan m\u00e1s si cada desarrollador personaliza su propio entorno de desarrollo integrado (IDE) local, en lugar de que todo el equipo adopte un IDE basado en la nube. La integraci\u00f3n continua (CI) permite que los desarrolladores incorporen los cambios del c\u00f3digo a un repositorio compartido con mayor frecuencia, o incluso a diario. Una vez que se incorporan las modificaciones del desarrollador, se validan con la compilaci\u00f3n autom\u00e1tica de la aplicaci\u00f3n y la ejecuci\u00f3n de distintas pruebas automatizadas (generalmente, de unidad e integraci\u00f3n), para garantizar que los cambios no hayan introducido una falla. Esto significa que se debe probar todo, desde las clases y el funcionamiento hasta los distintos m\u00f3dulos que conforman toda la aplicaci\u00f3n. Si una prueba autom\u00e1tica detecta un conflicto entre el c\u00f3digo nuevo y el actual, la CI facilita la resoluci\u00f3n de esos errores con rapidez. Distribuci\u00f3n continua Despu\u00e9s de la automatizaci\u00f3n de las compilaciones y las pruebas de unidad e integraci\u00f3n de la CI, la distribuci\u00f3n continua automatiza el traslado del c\u00f3digo validado hacia un repositorio. Por eso, para que la distribuci\u00f3n continua sea eficaz, es importante que la CI ya est\u00e9 incorporada al proceso de desarrollo. El objetivo de la distribuci\u00f3n continua es tener una base de c\u00f3digo que pueda implementarse en el entorno de producci\u00f3n en cualquier momento. Cada etapa (desde la incorporaci\u00f3n de los cambios al c\u00f3digo hasta la distribuci\u00f3n de las compilaciones listas para la producci\u00f3n) implica la automatizaci\u00f3n de las pruebas y del lanzamiento del c\u00f3digo. Al final de este proceso, el equipo de operaciones puede implementar una aplicaci\u00f3n para la producci\u00f3n de forma r\u00e1pida y sencilla. Descubra las otras implementaciones que puede automatizar Implementaci\u00f3n continua La \u00faltima etapa del canal consolidado de CI/CD es la implementaci\u00f3n continua, que automatiza el lanzamiento de una aplicaci\u00f3n a la producci\u00f3n, ya que es una extensi\u00f3n de la distribuci\u00f3n continua, la cual automatiza el traslado de una compilaci\u00f3n lista para la producci\u00f3n a un repositorio del c\u00f3digo. Debido a que no hay ninguna entrada manual en la etapa anterior a la producci\u00f3n, la implementaci\u00f3n continua depende en gran medida del correcto dise\u00f1o de la automatizaci\u00f3n de las pruebas. En la pr\u00e1ctica, los cambios que implementan los desarrolladores en la aplicaci\u00f3n en la nube podr\u00edan ponerse en marcha unos cuantos minutos despu\u00e9s de su creaci\u00f3n (siempre que hayan pasado las pruebas automatizadas). Esto facilita mucho m\u00e1s la recepci\u00f3n e incorporaci\u00f3n permanente de los comentarios de los usuarios. En conjunto, todas estas pr\u00e1cticas de CI/CD permiten que se implementen las aplicaciones con menos riesgos, ya que es m\u00e1s f\u00e1cil incorporar los cambios en las aplicaciones de a poco, en lugar de hacerlo todo de una sola vez. Sin embargo, tambi\u00e9n deben realizarse muchas inversiones iniciales, ya que se deben dise\u00f1ar las pruebas automatizadas para que se adapten a las distintas etapas de prueba y lanzamiento en el canal de la CI/CD. Conclusi\u00f3n Se ha explicado en este tema cu\u00e1les son las caracter\u00edsticas, usos y diferencias entre los servidores web y los servidores de aplicaciones. Tambi\u00e9n hemos explicado detalladamente en qu\u00e9 consiste un proceso de despliegue cl\u00e1sico de una aplicaci\u00f3n web, cu\u00e1les son sus fases y caracter\u00edsticas. Para reforzar este proceso, hemos listado una serie de buenas pr\u00e1cticas a la hora de llevarlo a cabo. Por \u00faltimo, hemos presentado las nuevas tendencias en el mundo del despliegue, como son las t\u00e9cnicas de CI/CD, que abordaremos de forma m\u00e1s profunda en el Tema 7. Referencias What is an application server? (I) What is an application server? (II) What is deployment in software and web development Simple y r\u00e1pido. Entiende qu\u00e9 es Maven en menos de 10 min. Maven in 5 Minutes T\u00edtulo de experto universitario en desarrollo de aplicaciones y servicios con JavaEE Qu\u00e9 es Node.js y por qu\u00e9 deber\u00eda usarlo \u00bfQu\u00e9 son la integraci\u00f3n y la distribuci\u00f3n continuas (CI/CD)?","title":"Tema 3 - Servidores de aplicaciones"},{"location":"ServAplic/#servidores-de-aplicaciones","text":"","title":"Servidores de aplicaciones"},{"location":"ServAplic/#introduccion","text":"Un servidor de aplicaciones es un marco mixto de software que permite tanto la creaci\u00f3n de aplicaciones web como un entorno de servidor para ejecutarlas. A menudo puede ser una pila compleja de diferentes elementos computacionales que ejecutan tareas espec\u00edficas que necesitan trabajar como uno solo para alimentar m\u00faltiples nubes y software y aplicaciones basadas en la web. Situado entre el servidor web y el nivel de backend del servidor de bases de datos, el servidor de aplicaciones es esencialmente un intermediario para el servidor de bases de datos y los usuarios de las aplicaciones empresariales o de consumo que soporta mediante el uso de varios protocolos e interfaces de programaci\u00f3n de aplicaciones (API). Es habitual que se utilice junto con un servidor web o que contenga un servidor web, por lo que ambos pueden converger y denominarse servidor de aplicaciones web. Tambi\u00e9n es lo suficientemente vers\u00e1til como para ser utilizado con otros servidores de aplicaciones simult\u00e1neamente. Los servidores de aplicaciones tambi\u00e9n pueden contener sus propias interfaces gr\u00e1ficas de usuario para su gesti\u00f3n a trav\u00e9s de PC, pero tambi\u00e9n pueden ocuparse de sus propios recursos, as\u00ed como del procesamiento de transacciones, la mensajer\u00eda, la agrupaci\u00f3n de recursos y conexiones, y la realizaci\u00f3n de tareas de seguridad.","title":"Introducci\u00f3n"},{"location":"ServAplic/#servidor-de-aplicaciones","text":"Las aplicaciones vienen en todas las formas, tama\u00f1os y casos de uso. En un mundo en el que dependemos de una serie de procesos empresariales cr\u00edticos, los servidores de aplicaciones son los ordenadores de gran potencia que proporcionan recursos de aplicaciones a los usuarios y clientes web. Los servidores de aplicaciones, como ya hemos dicho, se sit\u00faan f\u00edsica o virtualmente entre los servidores de bases de datos que almacenan los datos de las aplicaciones y los servidores web que se comunican con los clientes. Los servidores de aplicaciones y el middleware af\u00edn son los sistemas operativos que soportan el desarrollo y la entrega de una aplicaci\u00f3n. Ya sea una aplicaci\u00f3n de escritorio, m\u00f3vil o web, los servidores de aplicaciones desempe\u00f1an un papel fundamental en la conexi\u00f3n de un mundo de dispositivos.","title":"Servidor de aplicaciones"},{"location":"ServAplic/#terminologia-de-los-servidores-de-aplicaciones","text":"T\u00e9rmino Descripci\u00f3n Servidor web Responsable de almacenar, procesar y entregar los datos de E/S de las p\u00e1ginas web Cliente web Punto final que intenta acceder a los recursos de la web o de la aplicaci\u00f3n HTTPS Protocolo de comunicaci\u00f3n seguro entre el servidor web y los clientes web JSON Lenguaje para el intercambio entre los servidores web y de aplicaciones L\u00f3gica de negocio Reglas para el almacenamiento de datos y la transferencia de recursos de la aplicaci\u00f3n Aplicaci\u00f3n Un programa de software o un sitio web unido a una base de datos","title":"Terminolog\u00eda de los servidores de aplicaciones"},{"location":"ServAplic/#el-papel-del-servidor-de-aplicaciones-en-la-arquitectura-de-servicios","text":"Cuando los usuarios de las aplicaciones, ya sea usuarios f\u00edsicos o los clientes web, solicitan acceso a una aplicaci\u00f3n, el servidor de aplicaciones suele hacer el trabajo pesado en el backend para almacenar y procesar las solicitudes din\u00e1micas de las aplicaciones.","title":"El papel del servidor de aplicaciones en la arquitectura de servicios"},{"location":"ServAplic/#por-que-necesitamos-servidores-de-aplicaciones","text":"Miles de millones de clientes web hacen peticiones HTTP cada d\u00eda, esperando un acceso instant\u00e1neo a la aplicaci\u00f3n en cuesti\u00f3n. Headspace durante la rutina de la ma\u00f1ana, Google Docs para el informe extenso, Twitter durante la pausa para el caf\u00e9, no importa la aplicaci\u00f3n en uso, est\u00e1 siendo consultada en un servidor de aplicaciones y devuelta a trav\u00e9s de un servidor web. Los servidores web se encargan de servir a los clientes web peticiones HTTP con respuestas HTTP. A diferencia de los servidores de aplicaciones, el dise\u00f1o del servidor web es lo suficientemente ligero como para procesar las solicitudes de datos est\u00e1ticos de varias aplicaciones (o sitios web), manteniendo la seguridad. Las peticiones din\u00e1micas, a menudo en forma de aplicaciones, requieren asistencia adicional.","title":"\u00bfPor qu\u00e9 necesitamos servidores de aplicaciones?"},{"location":"ServAplic/#los-servidores-de-aplicaciones-optimizan-el-trafico-y-anaden-seguridad","text":"Para conseguir una agilidad \u00f3ptima del servidor web, no sirve gestionar tanto las peticiones HTTP de los clientes web como pasar o almacenar recursos de m\u00faltiples sitios web. Los servidores de aplicaciones llenan este vac\u00edo con un dise\u00f1o de alta potencia construido para manejar las solicitudes de contenido web din\u00e1mico. Los servidores de aplicaciones tambi\u00e9n proporcionan redundancia de programas y una capa adicional de seguridad. Una vez desplegado entre una base de datos y un servidor web, el trabajo de preservar y duplicar la arquitectura de la aplicaci\u00f3n a trav\u00e9s de la red es m\u00e1s factible. El paso adicional entre las potenciales comunicaciones web maliciosas y las joyas de la corona en el servidor de base de datos a\u00f1ade una capa de seguridad adicional. Dado que los servidores de aplicaciones pueden procesar solicitudes de l\u00f3gica empresarial, un intento de inyecci\u00f3n SQL es tambi\u00e9n mucho m\u00e1s dif\u00edcil. Las organizaciones pueden proteger a\u00fan m\u00e1s sus datos con un servidor proxy inverso colocado delante de sus bases de datos. Los servidores proxy y las VPN pueden hacer maravillas para anonimizar y encriptar la comunicaci\u00f3n para proteger a los usuarios y los datos de la empresa.","title":"Los servidores de aplicaciones optimizan el tr\u00e1fico y a\u00f1aden seguridad"},{"location":"ServAplic/#como-funcionan-los-servidores-de-aplicaciones","text":"Pongamos como ejemplo un servidor de aplicaciones Java. \u00bfQu\u00e9 son los servlets? Un servlet es un programa Java que se ejecuta en un servidor Web y construye o sirve p\u00e1ginas web. De esta forma se pueden construir p\u00e1ginas din\u00e1micas, basadas en diferentes fuentes variables: datos proporcionados por el usuario, fuentes de informaci\u00f3n variable (p\u00e1ginas de noticias, por ejemplo), o programas que extraigan informaci\u00f3n de bases de datos. Comparado con un CGI, un servlet es m\u00e1s sencillo de utilizar, m\u00e1s eficiente (se arranca un hilo por cada petici\u00f3n y no un proceso entero), m\u00e1s potente y portable. Con los servlets podremos, entre otras cosas, procesar, sincronizar y coordinar m\u00faltiples peticiones de clientes, reenviar peticiones a otros servlets o a otros servidores u otros. Como la mayor\u00eda de los servidores de hoy en d\u00eda, los servidores de aplicaciones contienen caracter\u00edsticas de seguridad, transacciones, servicios, clustering, diagn\u00f3sticos y bases de datos. En lo que se diferencian los servidores de aplicaciones es en su capacidad para procesar peticiones de servlets (programas Java) desde un servidor web. En la imagen anterior, se muestra el flujo general de los servidores de aplicaciones web: El cliente abre un navegador y solicita acceso a un sitio web El servidor web recibe la petici\u00f3n HTTP y responde con la p\u00e1gina web deseada El servidor web gestiona las peticiones de datos est\u00e1ticos, pero el cliente quiere utilizar una herramienta interactiva Al tratarse de una petici\u00f3n de datos din\u00e1micos, el servidor web transfiere la petici\u00f3n a un servidor de aplicaciones El servidor de aplicaciones recibe la petici\u00f3n HTTP y la convierte en una petici\u00f3n de servlet El servlet llega al servidor de la base de datos, y el servidor de aplicaciones recibe una respuesta del servlet El servidor de aplicaciones traduce la respuesta del servlet al formato HTTP para el acceso del cliente Al recibir una solicitud de servlet de un servidor web, el servidor de aplicaciones procesa la solicitud y responde al servidor web mediante la respuesta de servlet. Dado que los servidores de aplicaciones trabajan principalmente con peticiones de l\u00f3gica de negocio, el servidor web traduce la respuesta del servlet y pasa una respuesta HTTP accesible para el usuario. Servidor de aplicaciones Servidor web Dise\u00f1ado para Sirve peticiones HTTP y de otra l\u00f3gica de negocio Sirve peticiones HTTP Almacena y proporciona L\u00f3gica de negocio Contenido web est\u00e1tico La utilizaci\u00f3n de los recursos es Pesada Ligera Soporta Transacciones distribuidas y Enterprise JavaBeans (EJB) Servlets, Java Server Pages (JSP) y JSON","title":"\u00bfC\u00f3mo funcionan los servidores de aplicaciones?"},{"location":"ServAplic/#servidores-de-aplicaciones-en-la-decada-de-2020","text":"El mercado de los servidores de aplicaciones espera crecer a una CAGR del 13,2%, pasando de cerca de 17.000 millones de d\u00f3lares en 2020 a 41.000 millones en 2026. El crecimiento continuo no es una sorpresa, ya que la conectividad a Internet y la dependencia de las aplicaciones crece. La migraci\u00f3n a las plataformas y servicios en la nube y el auge de los dispositivos IoT son dos impulsores clave en el mercado de infraestructura de aplicaciones y middleware moderno. A esto hay que a\u00f1adir un movimiento hacia las pol\u00edticas BYOD (Bring Your Own Device) y una fuerza de trabajo remota que depende de una mayor conectividad y eficiencia operativa.","title":"Servidores de aplicaciones en la d\u00e9cada de 2020"},{"location":"ServAplic/#servidores-de-aplicaciones-el-mejor-amigo-de-un-servidor-web","text":"Los servidores de aplicaciones son fundamentales para las exigencias actuales de interconexi\u00f3n. Las empresas, en \u00faltima instancia, est\u00e1n al servicio de los intereses de los clientes por lo que sin una conexi\u00f3n escalable y estable a los recursos de las aplicaciones, los clientes modernos huir\u00e1n sin mirar atr\u00e1s. Los servidores de aplicaciones asumen el papel de conector y mejor amigo de los servidores web. Cuando los servidores web tienen una petici\u00f3n del cliente que es demasiado para soportar, los servidores de aplicaciones hacen posible mantener la comunicaci\u00f3n sin problemas con el contenido web din\u00e1mico.","title":"Servidores de aplicaciones: El mejor amigo de un servidor web"},{"location":"ServAplic/#que-es-el-despliegue-de-aplicaciones-web","text":"El despliegue en el desarrollo de software y web significa pasar los cambios o actualizaciones de un entorno de funcionamiento a otro. Al configurar un sitio web, siempre se tendr\u00e1 el sitio web en vivo, que se llama el entorno en vivo o entorno de producci\u00f3n. Si se quiere tener la capacidad de hacer cambios sin afectar a un sitio web en producci\u00f3n, se puede (y se debe) a\u00f1adir entornos adicionales. Estos entornos se llaman entornos de desarrollo o entornos de despliegue. Los entornos de desarrollo adicionales suelen ser un entorno local, un entorno de desarrollo y un entorno de preparaci\u00f3n o preproducci\u00f3n. El n\u00famero de entornos que se necesitan depende de cada caso y de la complejidad del proyecto en el que se est\u00e9 trabajando. Aunque los modelos de despliegue pueden variar, el m\u00e1s com\u00fan es el cl\u00e1sico modelo de despliegue \"de izquierda a derecha\" cuando se trabaja con m\u00faltiples entornos de despliegue. En este modelo, los cambios se realizan en entornos locales, de desarrollo o de preparaci\u00f3n (dependiendo de la configuraci\u00f3n) y se van pasando de izquierda a derecha a trav\u00e9s de los diferentes entornos, terminando en el de producci\u00f3n. Una vez completado este proceso de despliegue, los nuevos cambios ser\u00e1n visibles en el entorno activo. En la imagen anterior se muestra una forma muy simplificada y cl\u00e1sica de manejar los despliegues cuando se trabaja con sitios web en un CMS. No necesariamente se necesitan todos los entornos anteriores, pero el proceso sigue siendo el mismo. Al utilizar m\u00faltiples entornos se obtiene una lista de ventajas - la principal es que se pueden hacer cambios sin que afecten a su sitio web en vivo. Una vez que los cambios se hacen, se prueban y est\u00e1n listos para ser pasados a producci\u00f3n, el proceso de despliegue se encarga del resto.","title":"\u00bfQu\u00e9 es el despliegue de aplicaciones web?"},{"location":"ServAplic/#de-que-pasos-consta-el-proceso-despliegue","text":"El flujo del proceso de despliegue consta de 5 pasos: Planificaci\u00f3n, desarrollo, pruebas, despliegue y supervisi\u00f3n. A continuaci\u00f3n nos adentraremos en cada uno de los 5 pasos, pero antes una nota r\u00e1pida. El flujo del proceso de despliegue que aparece a continuaci\u00f3n cubre los aspectos fundamentales, que se dividen en 5 pasos. Esto no significa que sea la \u00fanica manera de hacerlo - podr\u00eda haber un proceso mejor para cada caso. Es una simplificaci\u00f3n para que cubra las partes m\u00e1s importantes. Recordar tener un plan de despliegue de software Para asegurarse de que el proceso de despliegue se desarrolle con la mayor fluidez posible, lo mejor es tener un plan de despliegue que se siga en todo momento. Al tener un plan nos aseguramos de que todo se haga de la misma manera cada vez que se realicen cambios. Esto es especialmente \u00fatil cuando varios usuarios trabajan en el mismo proyecto. Un plan de despliegue debe incluir reglas sobre cu\u00e1ndo desplegar desde los entornos locales a los sitios de desarrollo o de puesta en escena, as\u00ed como horarios para cuando los nuevos cambios pueden ir a un entorno en vivo. Al tener un plan establecido, se reduce el riesgo de conflictos entre los diferentes cambios y se asegura que el proceso de despliegue sea lo m\u00e1s f\u00e1cil y fluido posible. Si se est\u00e1 trabajando en un proyecto de c\u00f3digo abierto, tambi\u00e9n da la oportunidad de hacer Release Candidates y dejar que la comunidad lo pruebe para detectar cualquier error que se pueda haber pasado por alto. Adem\u00e1s de un plan general, tambi\u00e9n es importante planificar cada uno de los cambios que se vaya a realizar. Este proceso ser\u00e1 muy r\u00e1pido para los cambios menores, pero deber\u00eda ser mucho m\u00e1s extenso para los grandes cambios. Si se planifica con mucha antelaci\u00f3n, se estar\u00e1 mucho m\u00e1s preparado para tener un proceso de despliegue sin problemas. El desarrollo propiamente dicho Una vez que se tenga el plan en marcha, es el momento de realizar el desarrollo real. Para garantizar que cualquier desarrollo pueda realizarse simult\u00e1neamente y sin romper nada, es importante trabajar \u00fanicamente en entornos locales o de desarrollo. Una vez que el proceso de desarrollo est\u00e1 hecho, es el momento de empezar a probar y desplegar los cambios a trav\u00e9s de la configuraci\u00f3n de su entorno. Probar los cambios Probar los cambios es crucial para garantizar que no haya errores en el entorno de producci\u00f3n final. Pero las pruebas no pueden completarse sin desplegar los cambios en nuevos entornos. Una vez que se haya comprobado que todos los cambios funcionan en el entorno local o de desarrollo, es el momento de desplegar los cambios en el siguiente entorno. Esto debe hacerse hasta el entorno de preproducci\u00f3n, donde se deben realizar las pruebas finales de control de calidad. Si todo est\u00e1 correctamente probado y funciona en un entorno parecido al entorno real, es el momento de desplegarlo en vivo. Si se descubren errores por el camino en cualquier entorno, es importante tener un plan para manejarlos. Por lo general, cualquier cambio que no pase las pruebas en el entorno de ensayo debe ser enviado de nuevo a la fase de desarrollo y -una vez corregido- volver a trabajar en los entornos. Desplegar los cambios en el entorno real Una vez que se han realizado todas las pruebas en los entornos anteriores y se han corregido los errores, es el momento de desplegar los cambios en el entorno real. Esto deber\u00eda ser algo bastante seguro, pero todos los que han trabajado en el desarrollo de software saben que algo puede salir mal. As\u00ed que, aunque es f\u00e1cil detenerse aqu\u00ed, es importante incluir el \u00faltimo paso del proceso: la monitorizaci\u00f3n. Supervisar los cambios Una vez que los nuevos cambios est\u00e9n en marcha y los usuarios reales utilicen activamente el sitio web o la aplicaci\u00f3n, es importante supervisar que todo funcione seg\u00fan lo previsto. Independientemente de la planificaci\u00f3n realizada, existe la posibilidad de que los usuarios se encuentren con problemas o realicen acciones que usted no hab\u00eda previsto durante la planificaci\u00f3n y el desarrollo. Un buen consejo para la monitorizaci\u00f3n es planificar los lanzamientos para los momentos en los que la menor cantidad de usuarios lo noten y en los que se tengan recursos de desarrollo listos en caso de que haya que arreglar algo. De este modo, el n\u00famero de usuarios afectados por cualquier error ser\u00e1 m\u00ednimo y se tendr\u00e1 gente preparada para arreglarlo o revertir los cambios si es necesario. Si se han de revertir los cambios, es importante mantener la calma y tener un proceso para manejarlo con la misma minuciosidad con la que se manejan los despliegues.","title":"\u00bfDe qu\u00e9 pasos consta el proceso despliegue?"},{"location":"ServAplic/#diferentes-tipos-de-despliegue","text":"Cuando se trata del tipo de despliegue, a menudo se divide en dos partes. Por lo general, se dividir\u00e1 entre metadatos y contenido, ya que estos tienen diferentes impactos en un nuevo entorno y deben ser manejados de manera diferente.","title":"Diferentes tipos de despliegue"},{"location":"ServAplic/#despliegue-de-metadatos","text":"Los metadatos incluyen los cambios en el c\u00f3digo, las plantillas, las hojas de estilo, los archivos, etc. Estos cambios a menudo requerir\u00e1n una comprobaci\u00f3n de validaci\u00f3n entre entornos para ver si tiene alg\u00fan conflicto imprevisto que deba resolverse. Muchas herramientas de despliegue incluyen comprobaciones de coherencia y ayudan a guiarte en caso de conflictos.","title":"Despliegue de metadatos"},{"location":"ServAplic/#despliegue-de-contenidos","text":"El contenido, como el texto, las im\u00e1genes y los v\u00eddeos, se maneja de forma diferente durante el despliegue, ya que es menos complicado moverlo entre entornos que los metadatos. Por esa raz\u00f3n, a menudo ver\u00e1s que las herramientas de despliegue hacen que el despliegue de contenido sea accesible para los editores de contenido y no s\u00f3lo para los desarrolladores. De esta manera, un editor de contenidos no depende de un desarrollador cuando se trata de enviar nuevos contenidos a un entorno activo.","title":"Despliegue de contenidos"},{"location":"ServAplic/#mejores-practicas-de-despliegue","text":"Cuando se trabaja con entornos de despliegue, es importante, como se ha mencionado anteriormente, tener un plan y un proceso claro para ello en el equipo. Para ampliar ese proceso hemos reunido algunas mejores pr\u00e1cticas que son buenas para implementar como parte de su proceso. Se ha de tener en cuenta que las siguientes pr\u00e1cticas recomendadas se refieren principalmente al desarrollo de software y de la web. Si se est\u00e1n llevando a cabo otros tipos de desarrollo puede haber otras cosas a considerar en el flujo de trabajo de despliegue.","title":"Mejores pr\u00e1cticas de despliegue"},{"location":"ServAplic/#utilizar-git","text":"Esto puede parecer obvio, pero tener un sistema de control de versiones es inestimable para cualquier flujo de trabajo de despliegue. Sin \u00e9l, es probable que se produzcan errores si se trabaja en equipo. Incluso si eres el \u00fanico desarrollador que trabaja en un proyecto, es muy recomendable utilizar Git en caso de que necesites volver a versiones anteriores o si alguien nuevo se une a tu equipo. Sin Git ser\u00e1 dif\u00edcil asegurar la consistencia en el flujo de trabajo de despliegue y puede llevar a que se cometan m\u00e1s errores por desplegar c\u00f3digo inacabado o por no tener a todos los miembros del equipo trabajando en la misma versi\u00f3n del c\u00f3digo.","title":"Utilizar Git"},{"location":"ServAplic/#trabajar-en-ramas","text":"Como regla general, tu equipo deber\u00eda trabajar en ramas. Hacerlo as\u00ed permitir\u00e1 trabajar en varias cosas al mismo tiempo sin que se afecten entre s\u00ed. Un ejemplo es cuando se encuentra un error que debe ser corregido. Si un desarrollador est\u00e1 utilizando una rama para trabajar en una nueva caracter\u00edstica, puede hacer r\u00e1pidamente una nueva rama del entorno de desarrollo para trabajar en el error. De este modo, habr\u00e1 dos ramas diferentes que no chocar\u00e1n ni crear\u00e1n posibles conflictos de fusi\u00f3n m\u00e1s adelante. Trabajar con ramas tambi\u00e9n ayuda al equipo con las preguntas y respuestas a la hora de desplegar en un entorno de preproducci\u00f3n. Tener los cambios en ramas separadas y fusionarlas dar\u00e1 a los testers una mejor visi\u00f3n de lo que se empuj\u00f3 (se hizo push) y lo que deben probar.","title":"Trabajar en ramas"},{"location":"ServAplic/#utilizar-un-entorno-local-como-entorno-de-desarrollo","text":"Aunque es posible trabajar directamente en un entorno de desarrollo, en la mayor\u00eda de los casos se ahorrar\u00e1 mucho tiempo trabajando localmente. Al instalar el sitio web o el software de forma local, se podr\u00e1 trabajar de forma m\u00e1s eficiente y acelerar las pruebas y la verificaci\u00f3n del c\u00f3digo. En primer lugar, no tienes hay que confirmar, empujar y desplegar constantemente un cambio antes de poder verificar si funciona. Y cuando algo no funciona (esto nos pasa a todos) tendr\u00e1s que revertirlo, empujarlo de nuevo y volver a desplegarlo. En lugar de eso, puedes simplemente ejecutarlo todo localmente y, una vez que funcione como es debido, puedes empujarlo directamente al entorno de preparaci\u00f3n para una prueba m\u00e1s rigurosa.","title":"Utilizar un entorno local como entorno de desarrollo"},{"location":"ServAplic/#revisar-las-diferencias-antes-de-desplegarlo-en-el-entorno-real","text":"Una vez que el equipo de pruebas se haya asegurado de que todo funciona en el entorno de pruebas, es el momento de desplegar el c\u00f3digo en el entorno real. Pero antes de hacer el despliegue final, es importante hacer una revisi\u00f3n final de las diferencias entre el entorno actual en producci\u00f3n y el entorno de desarrollo del que se parte. Incluso despu\u00e9s de las pruebas exhaustivas y la garant\u00eda de calidad, las cosas pueden ir mal tan pronto como se llega al entorno real. Y una vez que eso sucede, a menudo puede ser muy estresante implementar correcciones r\u00e1pidas o hacer una reversi\u00f3n completa de la versi\u00f3n. Por lo general, se querr\u00e1 evitar esto a toda costa, por lo que es muy recomendable hacer una revisi\u00f3n final del c\u00f3digo antes de pulsar el bot\u00f3n de despliegue.","title":"Revisar las diferencias antes de desplegarlo en el entorno real"},{"location":"ServAplic/#considerar-tener-grupos-de-usuarios-con-diferentes-permisos","text":"Mientras que cualquier desarrollador debe ser capaz de empujar los cambios a los entornos de test, puede ser una buena idea para restringir qui\u00e9n puede desplegarlos en vivo. Para los equipos m\u00e1s peque\u00f1os, esto puede no tener mucho sentido, ya que puede crear un cuello de botella para implantar nuevos cambios. Pero si se trata de un equipo m\u00e1s grande con un nivel de experiencia muy variado entre los miembros del equipo, puede ser una gran idea dejar que s\u00f3lo los desarrolladores senior desplieguen en el entorno de producci\u00f3n. Esto asegura efectivamente un mayor nivel de control sobre el flujo de releases y tambi\u00e9n significa que al menos un par de ojos senior han visto lo que est\u00e1 pasando en el entorno real. Si lo que se tiene es un enfoque muy iterativo con lanzamientos r\u00e1pidos como el utilizado en la metodolog\u00eda CD (Continous Delivery), esto podr\u00eda ralentizarlo todo demasiado . Aun as\u00ed, dado que los cambios que se empujan son normalmente m\u00e1s peque\u00f1os con este enfoque, probablemente no se sufrir\u00e1n grandes retrasos. Y si significa detectar algunos errores m\u00e1s, el tiempo que se ahorra al no tener que corregir errores compensar\u00e1 el tiempo invertido. Hablando de romper cosas...","title":"Considerar tener grupos de usuarios con diferentes permisos"},{"location":"ServAplic/#mantener-la-calma-incluso-si-algo-se-rompe","text":"Acabas de desplegar en tu entorno de producci\u00f3n y ahora tu sitio web est\u00e1 roto. Menuda liada, \u00bfahora qu\u00e9 se hace? Desgraciadamente, estas cosas ocurren - no importa lo cuidadoso que se sea. Pero en lugar de entrar en p\u00e1nico y aplicar hotfixes o retroceder inmediatamente, es importante mantener la calma y asegurarse de que lo que est\u00e1 haciendo no va a romper las cosas a\u00fan m\u00e1s. En primer lugar, se deber\u00eda comprobar si es posible realizar una reversi\u00f3n o rollback y si realmente se arreglar\u00eda algo. En algunas situaciones, es posible que se hayan hecho cambios que son irreversibles y un rollback s\u00f3lo causar\u00eda problemas a\u00fan mayores. Tambi\u00e9n hay que comprobar si lo que se ha roto es una caracter\u00edstica existente o nueva. De nuevo, si la cosa que se rompi\u00f3 no era parte de la nueva versi\u00f3n, probablemente no servir\u00e1 de nada hacer un rollback . As\u00ed que en lugar de entrar en p\u00e1nico, se debe tener un plan preparado y respirar hondo antes de ponerse a trabajar en la b\u00fasqueda de una soluci\u00f3n. Puede parecer sencillo, pero puede ayudar a salir de una mala situaci\u00f3n mucho m\u00e1s r\u00e1pido que si lanz\u00e1ndose directamente.","title":"Mantener la calma, incluso si algo se rompe"},{"location":"ServAplic/#a-que-hora-del-dia-se-deben-desplegar-los-cambios","text":"En caso de que algo se rompa al desplegar en el entorno de producci\u00f3n, es importante encontrar el mejor momento para hacerlo. Y aunque este momento var\u00eda mucho de un proyecto a otro, hay dos preguntas que pueden hacerse para determinar cu\u00e1ndo desplegar los cambios: \u00bfCu\u00e1ndo tiene la menor cantidad de usuarios activos? \u00bfCu\u00e1ndo tiene a alguien preparado para supervisar y solucionar los problemas despu\u00e9s del despliegue?","title":"\u00bfA qu\u00e9 hora del d\u00eda se deben desplegar los cambios?"},{"location":"ServAplic/#cuando-tiene-el-menor-numero-de-usuarios-activos","text":"Por lo general, lo que se quiere es que el menor n\u00famero posible de personas se vea afectado por sus nuevos cambios. Por lo tanto, como regla general, debe buscar cualquier momento del d\u00eda en el que el menor n\u00famero de usuarios est\u00e9 utilizando activamente el sitio web o software. En el caso de los sitios web, esto puede hacerse consultando las herramientas de an\u00e1lisis de datos que se tengan en marcha, por ejemplo, Google Analytics. All\u00ed se podr\u00e1n crear informes personalizados que muestren a qu\u00e9 hora del d\u00edase tiene menos tr\u00e1fico, as\u00ed como identificar las horas punta en las que definitivamente no se deber\u00eda hacer ning\u00fan cambio. Adem\u00e1s de mirar la hora del d\u00eda, tambi\u00e9n puede valer la pena mirar c\u00f3mo se reparte la actividad de los usuarios entre los d\u00edas de la semana. Este an\u00e1lisis es muy bueno, pero a menudo acabar\u00e1 con la misma respuesta: Deber\u00edan publicarse los cambios durante la noche. Y aunque esto podr\u00eda parecer una gran idea si s\u00f3lo nos fij\u00e1ramos en esta cuesti\u00f3n, es importante que tambi\u00e9n tengamos en cuenta la siguiente.","title":"\u00bfCu\u00e1ndo tiene el menor n\u00famero de usuarios activos?"},{"location":"ServAplic/#hay-alguien-despierto-y-preparado-para-solucionar-posibles-problemas-en-ese-momento","text":"Si la respuesta es no, entonces desplegar los cambios en mitad de la noche podr\u00eda no ser la mejor idea. En su lugar, se deber\u00edan identificar las franjas horarias en las que puedas encontrar el mejor equilibrio entre el n\u00famero de usuarios activos y los desarrolladores dispuestos a solucionar los problemas. Esto variar\u00e1 mucho dependiendo del proyecto y del equipo, pero en general, se deber\u00edan encontrar algunas opciones. Y si ya se tiene un horario fijo de despliegue, incluso puede convencerse al equipo de que est\u00e9 listo a horas extra\u00f1as del d\u00eda. Es mucho m\u00e1s f\u00e1cil convencer a alguien de que venga unas horas antes si sabe que s\u00f3lo ocurre una vez cada ciclo o sprint. Es por este motivo que en muchas empresas se trabaja con guardias rotativas para ofrecer una disponibilidad total. Info Aunque no hay un momento perfecto para el despliegue, definitivamente hay momentos que son mejores que otros.","title":"\u00bfHay alguien despierto y preparado para solucionar posibles problemas en ese momento?"},{"location":"ServAplic/#cuales-son-las-ventajas-del-despliegue-y-de-los-entornos-multiples","text":"","title":"\u00bfCu\u00e1les son las ventajas del despliegue y de los entornos m\u00faltiples?"},{"location":"ServAplic/#reduccion-del-riesgo-de-romper-un-sitio-web-en-produccion","text":"Una de las principales razones para utilizar m\u00faltiples entornos y confiar en el despliegue es reducir el riesgo de que los cambios tengan un impacto negativo en un sitio web en vivo. Mientras que los cambios menores se pueden hacer f\u00e1cilmente directamente en un sitio web en vivo, los cambios m\u00e1s grandes se pueden hacer en entornos separados sin el riesgo de romper nada en el entorno en vivo. Tener varios usuarios trabajando en el mismo sitio web tambi\u00e9n garantiza que nadie se arriesgue a romper algo debido a los cambios de otro usuario.","title":"Reducci\u00f3n del riesgo de romper un sitio web en producci\u00f3n"},{"location":"ServAplic/#ahorro-de-tiempo","text":"Sin la preocupaci\u00f3n de romper algo en un sitio web en vivo, se pueden realizar los cambios en el orden que se prefiera. Esto significa que se puede optimizar el flujo de trabajo para realizar los cambios sin tener en cuenta el aspecto o el funcionamiento del sitio web mientras se lleva a cabo. Si se trabaja en un entorno local tambi\u00e9n existe la ventaja de que los cambios se procesan m\u00e1s r\u00e1pido y no hay dependencias de ning\u00fan problema de conectividad. A la hora de desplegar los cambios, tambi\u00e9n se ahorrar\u00e1 tiempo, ya que se podr\u00e1n realizar todos los cambios al mismo tiempo en lugar de tener que hacerlo en varios pasos m\u00e1s peque\u00f1os.","title":"Ahorro de tiempo"},{"location":"ServAplic/#el-contenido-sensible-al-tiempo-es-mas-facil-de-gestionar","text":"Si se est\u00e1n llevando a cabo campa\u00f1as que son sensibles al tiempo y que s\u00f3lo pueden ponerse en marcha a partir de un determinado d\u00eda u hora, entonces la ejecuci\u00f3n de m\u00faltiples entornos y el uso del despliegue pueden ahorrar una gran cantidad de estr\u00e9s. Al crear todo el contenido en un entorno de puesta en escena/preprod (o similar) puedes terminar tu campa\u00f1a sin preocuparte de que sea visible para tus usuarios. Y cuando llegue el momento de lanzarla, podr\u00e1 hacerla visible en muy poco tiempo despleg\u00e1ndola en su entorno real. Y si la herramienta de despliegue incluye roles de usuario con configuraci\u00f3n de permisos, es posible que un editor de contenidos haga todo esto -incluyendo el despliegue de los cambios- sin involucrar a un desarrollador en el proceso.","title":"El contenido sensible al tiempo es m\u00e1s f\u00e1cil de gestionar"},{"location":"ServAplic/#despliegue-de-aplicaciones-java","text":"","title":"Despliegue de aplicaciones Java"},{"location":"ServAplic/#introduccion_1","text":"En el lado del servidor, tenemos que conseguir que nuestro servidor HTTP sea capaz de ejecutar programas de aplicaci\u00f3n que recojan los par\u00e1metros de peticiones del cliente, los procesen y devuelvan al servidor un documento que \u00e9ste pasar\u00e1 a su vez al cliente. As\u00ed, para el cliente el servidor no habr\u00e1 hecho nada distinto a lo estipulado en el protocolo HTTP, pero el servidor podr\u00e1 valerse de herramientas externas para procesar y servir la petici\u00f3n solicitada, pudiendo as\u00ed no limitarse a servir p\u00e1ginas est\u00e1ticas, sino utilizar otras aplicaciones (servlets, JSP...) para servir documentos con contenido din\u00e1mico. Los programas de aplicaci\u00f3n son t\u00edpicamente programas que realizan consultas a bases de datos, procesan la informaci\u00f3n resultante y devuelven la salida al servidor, entre otras tareas. Vamos a centrarnos en las aplicaciones web JavaEE, en las que los componentes din\u00e1micos que recibir\u00e1n las peticiones HTTP en el servidor ser\u00e1n los servlets y JSPs. Estos componentes podr\u00e1n analizar esta petici\u00f3n y utilizar otros componentes Java para realizar las acciones necesarias (beans, EJBs, etc).","title":"Introducci\u00f3n"},{"location":"ServAplic/#estructura-de-una-aplicacion-java","text":"Una aplicaci\u00f3n web JavaEE que utilice servlets o p\u00e1ginas JSP debe tener una estructura de ficheros y directorios determinada: En el directorio ra\u00edz de la aplicaci\u00f3n se colocan las p\u00e1ginas HTML o JSP (podemos dividirlas tambi\u00e9n en directorios si queremos) Colgando del directorio inicial de la aplicaci\u00f3n, se tiene un directorio WEB-INF, que contiene la informaci\u00f3n Web relevante para la aplicaci\u00f3n. El resto de elementos de la aplicaci\u00f3n (im\u00e1genes, etc), podemos estructurarlos como nos convenga. Esta estructura estar\u00e1 contenida dentro de alg\u00fan directorio, que ser\u00e1 el directorio correspondiente a la aplicaci\u00f3n Web, y que podremos, si lo hacemos convenientemente, copiar en el servidor que nos convenga. Es decir, cualquier servidor Web JavaEE soporta esta estructura en una aplicaci\u00f3n Web, s\u00f3lo tendremos que copiarla en el directorio adecuado de cada servidor. Cada aplicaci\u00f3n web JavaEE es un contexto, una unidad que comprende un conjunto de recursos, clases Java y su configuraci\u00f3n. Cuando hablemos de contexto, nos estaremos refiriendo a la aplicaci\u00f3n web en conjunto.","title":"Estructura de una aplicaci\u00f3n Java"},{"location":"ServAplic/#empaquetamiento","text":"Una forma de distribuir aplicaciones Web es empaquetar toda la aplicaci\u00f3n (a partir de su directorio inicial) dentro de un fichero WAR (de forma parecida a como se hace con un TAR o un JAR), y distribuir dicho fichero. Podemos crear un fichero WAR de la misma forma que creamos un JAR, utilizando la herramienta JAR. Estos ficheros WAR son un est\u00e1ndar de JavaEE, por lo que podremos utilizarlos en los diferentes servidores de aplicaciones JavaEE existentes.","title":"Empaquetamiento"},{"location":"ServAplic/#despliegue-de-archivos-war","text":"Los archivos WAR, son un tipo especial de JAR utilizado para distribuir los artefactos o contenido de las aplicaciones Web en tecnolog\u00eda JEE: p\u00e1ginas Web HTML o JSP,clases Java, servlets Java, archivos XML, librer\u00edas de etiquetas (tag libraries) y otros recursos. El empaquetamiento en archivos WAR es algo est\u00e1ndar, pero no as\u00ed el proceso de despliegue, que es dependiente del servidor. No obstante, la mayor\u00eda de servidores JavaEE funcionan en este aspecto de modo similar: permiten desplegar las aplicaciones desde una consola de administraci\u00f3n y tambi\u00e9n \"dejando caer\" el fichero en determinado directorio.","title":"Despliegue de archivos WAR"},{"location":"ServAplic/#maven","text":"Maven es una herramienta open-source, que se cre\u00f3 en 2001 con el objetivo de simplificar los procesos de build (compilar y generar ejecutables a partir del c\u00f3digo fuente). Antes de existir Maven, si quer\u00edamos compilar y generar ejecutables de un proyecto, ten\u00edamos que analizar qu\u00e9 partes de c\u00f3digo se deb\u00edan compilar, qu\u00e9 librer\u00edas utilizaba el c\u00f3digo, d\u00f3nde incluirlas, qu\u00e9 dependencias de compilaci\u00f3n hab\u00eda en el proyecto\u2026 En el mejor de los casos, se empleaban unos pocos minutos para saber c\u00f3mo hacer una build del proyecto. En el peor de los casos, el proceso de build era tan complejo que un desarrollador pod\u00eda tardar horas en saber c\u00f3mo compilar y generar los ejecutables a partir del c\u00f3digo. Ahora, la build de cualquier proyecto Maven, independientemente de sus m\u00f3dulos, dependencias o librer\u00edas, consiste simplemente en ejecutar el comando mvn install . Por otra parte, antes de Maven, cada vez que sal\u00eda una nueva versi\u00f3n de un analizador est\u00e1tico de c\u00f3digo, de un framework de pruebas unitarias (como JUnit) o cualquier librer\u00eda, hab\u00eda que parar todo el desarrollo para reajustar el proceso de build a las nuevas necesidades. Y\u2026 \u00bfc\u00f3mo se ejecutaban las pruebas? \u00bfC\u00f3mo se generaban informes? Sin Maven, en cada proyecto esto se hac\u00eda de distinta manera. Lo cierto es que Maven es mucho m\u00e1s que una herramienta que hace builds del c\u00f3digo. Podr\u00edamos decir, que Maven es una herramienta capaz de gestionar un proyecto software completo , desde la etapa en la que se comprueba que el c\u00f3digo es correcto, hasta que se despliega la aplicaci\u00f3n, pasando por la ejecuci\u00f3n de pruebas y generaci\u00f3n de informes y documentaci\u00f3n. Para ello, en Maven se definen tres ciclos de build del software con una serie de etapas diferenciadas. Por ejemplo el ciclo por defecto tiene las etapas de: Validaci\u00f3n (validate): Validar que el proyecto es correcto. Compilaci\u00f3n (compile). Test (test): Probar el c\u00f3digo fuente usando un framework de pruebas unitarias. Empaquetar (package): Empaquetar el c\u00f3digo compilado y transformarlo en alg\u00fan formato tipo .jar o .war. Pruebas de integraci\u00f3n (integration-test): Procesar y desplegar el c\u00f3digo en alg\u00fan entorno donde se puedan ejecutar las pruebas de integraci\u00f3n. Verificar que el c\u00f3digo empaquetado es v\u00e1lido y cumple los criterios de calidad (verify). Instalar el c\u00f3digo empaquetado en el repositorio local de Maven, para usarlo como dependencia de otros proyectos (install). Desplegar el c\u00f3digo a un entorno (deploy). Para poder llevar a cabo alguna de estas fases en nuestro c\u00f3digo, tan solo tendremos que ejecutar mvn y el nombre de la fase (la palabra que puse entre par\u00e9ntesis). Adem\u00e1s van en cadena, es decir, si empaquetamos el c\u00f3digo (package), Maven ejecutar\u00e1 desde la fase de validaci\u00f3n (validate) a empaquetaci\u00f3n (package). As\u00ed de simple. Por otra parte, con Maven la gesti\u00f3n de dependencias entre m\u00f3dulos y distintas versiones de librer\u00edas se hace muy sencilla. En este caso, solo tenemos que indicar los m\u00f3dulos que componen el proyecto, o qu\u00e9 librer\u00edas utiliza el software que estamos desarrollando en un fichero de configuraci\u00f3n de Maven del proyecto llamado POM (Project Object Module). Adem\u00e1s, en el caso de las librer\u00edas, no tienes ni tan siquiera que descargarlas a mano. Maven posee un repositorio remoto (Maven central) donde se encuentran la mayor\u00eda de librer\u00edas que se utilizan en los desarrollos de software, y que la propia herramienta se descarga cuando sea necesario. Digamos que Maven aporta una sem\u00e1ntica com\u00fan al proceso de build y desarrollo del software. Incluso, establece una estructura com\u00fan de directorios para todos los proyectos. Por ejemplo el c\u00f3digo estar\u00e1 en ${ra\u00edz del proyecto}/src/main/java , los recursos en ${ra\u00edz del proyecto }/src/main/resources . Los tests est\u00e1n en ${ra\u00edz del proyecto }/src/test .","title":"Maven"},{"location":"ServAplic/#despliegue-de-aplicaciones-nodejs-con-express","text":"","title":"Despliegue de aplicaciones Node.js con Express"},{"location":"ServAplic/#que-es-nodejs","text":"Node JS es un entorno de ejecuci\u00f3n de JavaScript r\u00e1pido que utilizamos para construir aplicaciones del lado del servidor, pero por s\u00ed mismono sabe c\u00f3mo servir archivos, manejar peticiones ni m\u00e9todos HTTP, as\u00ed que aqu\u00ed es donde entra en juego Express JS. Node.js no es un lenguaje de programaci\u00f3n. M\u00e1s bien, es un entorno de ejecuci\u00f3n que se utiliza para ejecutar JavaScript fuera del navegador. Node.js tampoco es un framework (una plataforma para desarrollar aplicaciones de software). El tiempo de ejecuci\u00f3n de Node.js se construye sobre un lenguaje de programaci\u00f3n -en este caso, JavaScript- y ayuda a la ejecuci\u00f3n de los propios frameworks. En resumen, Node.js no es un lenguaje de programaci\u00f3n ni un marco de trabajo; es un entorno para ellos.","title":"\u00bfQu\u00e9 es Node.js?"},{"location":"ServAplic/#que-es-express","text":"Express JS es un framework de Node.js dise\u00f1ado para construir aplicaciones web de API's y aplicaciones m\u00f3viles multiplataforma de forma r\u00e1pida y hacer que Node.js sea f\u00e1cil.","title":"\u00bfQu\u00e9 es Express?"},{"location":"ServAplic/#que-es-npm","text":"NPM responde a las siglas de Node Package Manager o manejador de paquetes de node, es la herramienta por defecto de JavaScript para la tarea de compartir e instalar paquetes. Tal como reza su documentaci\u00f3n, npm se compone de al menos dos partes principales. Un repositorio online para publicar paquetes de software libre para ser utilizados en proyectos Node.js Una herramienta para la terminal (command line utility) para interactuar con dicho repositorio que te ayuda a la instalaci\u00f3n de utilidades, manejo de dependencias y la publicaci\u00f3n de paquetes. As\u00ed pues, NPM es un gestor de paquetes para Javascript. Es una especie de Maven para paquetes Javascript, es decir, sirve para instalar y gestionar versiones de paquetes y librer\u00edas js. NPM lleva mucho tiempo siendo el referente en cuanto a gestores de paquetes javascript, pero desde hace un tiempo le ha salido un competidor: Yarn. Los de yarn aseguran que su gestor de librer\u00edas js es mucho m\u00e1s r\u00e1pido y potente, pero de momento el uso de NPM es mayoritario.","title":"\u00bfQu\u00e9 es npm?"},{"location":"ServAplic/#packagejson","text":"Cada proyecto en JavaScript puede enfocarse como un paquete npm con su propia informaci\u00f3n de paquete y su archivo package.json para describir el proyecto. package.json se generar\u00e1 cuando se ejecute npm init para inicializar un proyecto JavaScript/Node.js, con los siguientes metadatos b\u00e1sicos proporcionados por los desarrolladores: name: el nombre de la librer\u00eda/proyecto JavaScript version: la versi\u00f3n del proyecto. description: la descripci\u00f3n del proyecto license: la licencia del proyecto","title":"package.json"},{"location":"ServAplic/#npm-scripts","text":"package.json tambi\u00e9n soporta la propiedad scripts que puede definirse para ejecutar herramientas de l\u00ednea de comandos que se instalan en el contexto local del proyecto. Por ejemplo, la porci\u00f3n de scripts de un proyecto npm puede tener un aspecto similar a este: { \"scripts\" : { \"build\" : \"tsc\" , \"format\" : \"prettier --write **/*.ts\" , \"format-check\" : \"prettier --check **/*.ts\" , \"lint\" : \"eslint src/**/*.ts\" , \"pack\" : \"ncc build\" , \"test\" : \"jest\" , \"all\" : \"npm run build && npm run format && npm run lint && npm run pack && npm test\" } } Con eslint, prettier, ncc, jest no necesariamente instalados como ejecutables globales sino como locales de tu proyecto dentro de node_modules/.bin/ .","title":"NPM scripts"},{"location":"ServAplic/#cicd-continous-integrationcontinous-deployment-delivery","text":"La CI/CD es un m\u00e9todo para distribuir las aplicaciones a los clientes con frecuencia mediante el uso de la automatizaci\u00f3n en las etapas del desarrollo de aplicaciones. Los principales conceptos que se le atribuyen son la integraci\u00f3n, la distribuci\u00f3n y la implementaci\u00f3n continuas. Se trata de una soluci\u00f3n para los problemas que puede generar la integraci\u00f3n del c\u00f3digo nuevo para los equipos de desarrollo y de operaciones (tambi\u00e9n conocida como \"el infierno de la integraci\u00f3n\"). En concreto, el proceso de integraci\u00f3n y distribuci\u00f3n continuas incorpora la automatizaci\u00f3n y la supervisi\u00f3n permanentes en todo el ciclo de vida de las aplicaciones, desde las etapas de integraci\u00f3n y prueba hasta las de distribuci\u00f3n e implementaci\u00f3n. Este conjunto de pr\u00e1cticas se conoce como \"canales de CI/CD\" y cuenta con el respaldo de los equipos de desarrollo y de operaciones que trabajan en conjunto de manera \u00e1gil, con un enfoque de DevOps o de ingenier\u00eda de confiabilidad del sitio (SRE).","title":"CI/CD (Continous Integration/Continous Deployment-Delivery)"},{"location":"ServAplic/#cual-es-la-diferencia-entre-la-integracion-la-distribucion-y-la-implementacion-continuas","text":"Estas siglas tienen diferentes significados. \"CI\" siempre se refiere a la integraci\u00f3n continua, que es un proceso de automatizaci\u00f3n para los desarrolladores. El \u00e9xito de la CI implica que se dise\u00f1en, prueben y combinen los cambios nuevos en el c\u00f3digo de una aplicaci\u00f3n con regularidad en un repositorio compartido. Supone una soluci\u00f3n al problema de que se desarrollen demasiadas divisiones de una aplicaci\u00f3n al mismo tiempo, que luego podr\u00edan entrar en conflicto entre s\u00ed. La sigla \"CD\" se refiere a la distribuci\u00f3n o la implementaci\u00f3n continuas, y se trata de conceptos relacionados que suelen usarse indistintamente. Ambos se refieren a la automatizaci\u00f3n de las etapas posteriores del proceso, pero a veces se usan por separado para explicar hasta d\u00f3nde llega la automatizaci\u00f3n. Por lo general, la distribuci\u00f3n continua se refiere a que los cambios que implementa un desarrollador en una aplicaci\u00f3n se someten a pruebas autom\u00e1ticas de errores y se cargan en un repositorio (como GitHub o un registro de contenedores), para que luego el equipo de operaciones pueda implementarlos en un entorno de producci\u00f3n en vivo. Es una soluci\u00f3n al problema de la falta de supervisi\u00f3n y comunicaci\u00f3n entre los equipos comerciales y de desarrollo, as\u00ed que su prop\u00f3sito es garantizar que la implementaci\u00f3n del c\u00f3digo nuevo se lleve a cabo con el m\u00ednimo esfuerzo. La implementaci\u00f3n continua (la otra definici\u00f3n de \"CD\") hace referencia al lanzamiento autom\u00e1tico de los cambios que implementa el desarrollador desde el repositorio hasta la producci\u00f3n, para ponerlos a disposici\u00f3n de los clientes. As\u00ed ya no se sobrecarga a los equipos de operaciones con procesos manuales que retrasan la distribuci\u00f3n de las aplicaciones. Con este tipo de implementaci\u00f3n, se aprovechan los beneficios de la distribuci\u00f3n continua y se automatiza la siguiente etapa del proceso. La CI/CD puede incluir solamente la integraci\u00f3n y la distribuci\u00f3n continuas, o las tres pr\u00e1cticas vinculadas, con la implementaci\u00f3n continua. Para complicar un poco m\u00e1s las cosas, a veces se utiliza el t\u00e9rmino \"distribuci\u00f3n continua\" para abarcar tambi\u00e9n los procesos de la implementaci\u00f3n continua. En realidad, no vale la pena profundizar en la sem\u00e1ntica. Solo debe recordar que la integraci\u00f3n y la distribuci\u00f3n continuas son un proceso que suele percibirse como una canalizaci\u00f3n e implica incorporar un alto nivel de automatizaci\u00f3n permanente y supervisi\u00f3n constante al desarrollo de las aplicaciones. El significado de los t\u00e9rminos var\u00eda en cada caso y depende de la cantidad de automatizaci\u00f3n que se haya incorporado a la canalizaci\u00f3n de integraci\u00f3n y distribuci\u00f3n continuas. Muchas empresas comienzan con la incorporaci\u00f3n de la CI, y luego van automatizando la distribuci\u00f3n y la implementaci\u00f3n, por ejemplo, con las aplicaciones desarrolladas directamente en la nube. Nuestros especialistas pueden ayudar a que su empresa desarrolle las pr\u00e1cticas, las herramientas y la cultura necesarias para modernizar las aplicaciones actuales y dise\u00f1ar otras nuevas con mayor eficiencia.","title":"\u00bfCu\u00e1l es la diferencia entre la integraci\u00f3n, la distribuci\u00f3n y la implementaci\u00f3n continuas?"},{"location":"ServAplic/#integracion-continua","text":"El objetivo del dise\u00f1o de las aplicaciones modernas es que los desarrolladores puedan trabajar de forma simult\u00e1nea en distintas funciones de la misma aplicaci\u00f3n. Sin embargo, si una empresa fusiona todo el c\u00f3digo fuente diversificado en un solo d\u00eda (conocido como el \"d\u00eda de la fusi\u00f3n\"), las tareas pueden tornarse tediosas, manuales y muy lentas. Esto se debe a que si un desarrollador que trabaja de forma aislada implementa un cambio en una aplicaci\u00f3n, existe la posibilidad de que entre en conflicto con las modificaciones que otros desarrolladores implementaron al mismo tiempo. El problema puede agravarse a\u00fan m\u00e1s si cada desarrollador personaliza su propio entorno de desarrollo integrado (IDE) local, en lugar de que todo el equipo adopte un IDE basado en la nube. La integraci\u00f3n continua (CI) permite que los desarrolladores incorporen los cambios del c\u00f3digo a un repositorio compartido con mayor frecuencia, o incluso a diario. Una vez que se incorporan las modificaciones del desarrollador, se validan con la compilaci\u00f3n autom\u00e1tica de la aplicaci\u00f3n y la ejecuci\u00f3n de distintas pruebas automatizadas (generalmente, de unidad e integraci\u00f3n), para garantizar que los cambios no hayan introducido una falla. Esto significa que se debe probar todo, desde las clases y el funcionamiento hasta los distintos m\u00f3dulos que conforman toda la aplicaci\u00f3n. Si una prueba autom\u00e1tica detecta un conflicto entre el c\u00f3digo nuevo y el actual, la CI facilita la resoluci\u00f3n de esos errores con rapidez.","title":"Integraci\u00f3n continua"},{"location":"ServAplic/#distribucion-continua","text":"Despu\u00e9s de la automatizaci\u00f3n de las compilaciones y las pruebas de unidad e integraci\u00f3n de la CI, la distribuci\u00f3n continua automatiza el traslado del c\u00f3digo validado hacia un repositorio. Por eso, para que la distribuci\u00f3n continua sea eficaz, es importante que la CI ya est\u00e9 incorporada al proceso de desarrollo. El objetivo de la distribuci\u00f3n continua es tener una base de c\u00f3digo que pueda implementarse en el entorno de producci\u00f3n en cualquier momento. Cada etapa (desde la incorporaci\u00f3n de los cambios al c\u00f3digo hasta la distribuci\u00f3n de las compilaciones listas para la producci\u00f3n) implica la automatizaci\u00f3n de las pruebas y del lanzamiento del c\u00f3digo. Al final de este proceso, el equipo de operaciones puede implementar una aplicaci\u00f3n para la producci\u00f3n de forma r\u00e1pida y sencilla. Descubra las otras implementaciones que puede automatizar","title":"Distribuci\u00f3n continua"},{"location":"ServAplic/#implementacion-continua","text":"La \u00faltima etapa del canal consolidado de CI/CD es la implementaci\u00f3n continua, que automatiza el lanzamiento de una aplicaci\u00f3n a la producci\u00f3n, ya que es una extensi\u00f3n de la distribuci\u00f3n continua, la cual automatiza el traslado de una compilaci\u00f3n lista para la producci\u00f3n a un repositorio del c\u00f3digo. Debido a que no hay ninguna entrada manual en la etapa anterior a la producci\u00f3n, la implementaci\u00f3n continua depende en gran medida del correcto dise\u00f1o de la automatizaci\u00f3n de las pruebas. En la pr\u00e1ctica, los cambios que implementan los desarrolladores en la aplicaci\u00f3n en la nube podr\u00edan ponerse en marcha unos cuantos minutos despu\u00e9s de su creaci\u00f3n (siempre que hayan pasado las pruebas automatizadas). Esto facilita mucho m\u00e1s la recepci\u00f3n e incorporaci\u00f3n permanente de los comentarios de los usuarios. En conjunto, todas estas pr\u00e1cticas de CI/CD permiten que se implementen las aplicaciones con menos riesgos, ya que es m\u00e1s f\u00e1cil incorporar los cambios en las aplicaciones de a poco, en lugar de hacerlo todo de una sola vez. Sin embargo, tambi\u00e9n deben realizarse muchas inversiones iniciales, ya que se deben dise\u00f1ar las pruebas automatizadas para que se adapten a las distintas etapas de prueba y lanzamiento en el canal de la CI/CD.","title":"Implementaci\u00f3n continua"},{"location":"ServAplic/#conclusion","text":"Se ha explicado en este tema cu\u00e1les son las caracter\u00edsticas, usos y diferencias entre los servidores web y los servidores de aplicaciones. Tambi\u00e9n hemos explicado detalladamente en qu\u00e9 consiste un proceso de despliegue cl\u00e1sico de una aplicaci\u00f3n web, cu\u00e1les son sus fases y caracter\u00edsticas. Para reforzar este proceso, hemos listado una serie de buenas pr\u00e1cticas a la hora de llevarlo a cabo. Por \u00faltimo, hemos presentado las nuevas tendencias en el mundo del despliegue, como son las t\u00e9cnicas de CI/CD, que abordaremos de forma m\u00e1s profunda en el Tema 7.","title":"Conclusi\u00f3n"},{"location":"ServAplic/#referencias","text":"What is an application server? (I) What is an application server? (II) What is deployment in software and web development Simple y r\u00e1pido. Entiende qu\u00e9 es Maven en menos de 10 min. Maven in 5 Minutes T\u00edtulo de experto universitario en desarrollo de aplicaciones y servicios con JavaEE Qu\u00e9 es Node.js y por qu\u00e9 deber\u00eda usarlo \u00bfQu\u00e9 son la integraci\u00f3n y la distribuci\u00f3n continuas (CI/CD)?","title":"Referencias"},{"location":"cicd/","text":"CI/CD (Integraci\u00f3n y despliegue continuo) Informaci\u00f3n Estos apuntes est\u00e1n basados en gran medida en unos apuntes de Domingo Gallardo y se distribuyen porque su licencia as\u00ed lo permite. Continuous Delivery (CD) Una idea fundamental de las metodolog\u00edas \u00e1giles es entregar valor frecuentemente para obtener una pronta retroalimentaci\u00f3n del cliente. Para ello es necesario tener muy engrasados los procesos de despliegue y puesta en producci\u00f3n del software. Una de las formas que facilita la optimizaci\u00f3n de la puesta en producci\u00f3n de software es la pr\u00e1ctica de XP ( eXtreme Programming ) de Integraci\u00f3n continua ( Continuous Integration ). En esta pr\u00e1ctica los miembros del equipo integran sus commits diariamente en el proyecto y en cada integraci\u00f3n se lanzan tests automatizados que verifican que los cambios no introducen errores. Adem\u00e1s de esta pr\u00e1ctica, debemos tener tambi\u00e9n automatizados todos los procesos de compilaci\u00f3n ( build ) y despliegue ( deployment ) de la aplicaci\u00f3n en los distintos entornos de prueba. Esto es lo que se denomina Despliegue continuo ( Continuous Deployment ). En esta l\u00ednea, se han popularizado herramientas como Docker o Kubernetes que facilitan el despliegue del software y su automatizaci\u00f3n y cada vez se demandan m\u00e1s profesionales (denominados DevOps ) con capacidad de gestionar estos despliegues automatizados. Y en los \u00faltimos a\u00f1os se ha dado un paso m\u00e1s all\u00e1 y se ha comenzado a hablar de Entrega continua ( Continous Delivery en ingl\u00e9s) con la idea de promover software que est\u00e9 listo en cualquier momento para salir a producci\u00f3n. En este tema veremos todos estos conceptos, con la idea de tomar un primer contacto con todos ellos. Necesitar\u00edamos un curso (o m\u00e1s) para verlos en profundidad. Intentaremos al menos conocer los conceptos b\u00e1sicos para poder seguir profundizando en alguno de ellos en el futuro. El problema de la puesta en producci\u00f3n En las empresas tradicionales no \u00e1giles el proceso de subir a producci\u00f3n una nueva versi\u00f3n es un proceso muy complicado y estresante. Se hace pocas veces, cuatro o cinco veces al a\u00f1o, durante el fin de semana cuando todos los servicios est\u00e1n parados. El proceso de genera muchos trastornos y dolores de cabeza. El equipo de operaciones tiene que estar pendiente del m\u00f3vil para detectar posibles problemas y ca\u00eddas del sistema. Una vez puesto el software en producci\u00f3n el equipo de desarrollo se dedicar\u00e1 continuamente a corregir bugs y solucionar problemas detectados por los usuarios. Esto no es \u00e1gil. Esto no permite conseguir lo que hemos comentado muchas veces de un ciclo corto de retroalimentaci\u00f3n para que el cliente pueda probar r\u00e1pidamente las nuevas caracter\u00edsticas y se pueda comprobar su valor. Recordemos que en ambientes inciertos y no predecibles es fundamental poder validar con el cliente las nuevas funcionalidades introducidas, para adaptarse y corregir posibles errores. La realizaci\u00f3n de entregas frecuentes tambi\u00e9n permite minimizar el riesgo. Todo el tiempo que estamos desarrollando algo sin ponerlo en producci\u00f3n es un riesgo acumulado. Hasta que no est\u00e1 en producci\u00f3n y ha sido aceptado por el cliente no sabemos si lo que estamos desarrollando va a ser validado o no. Cuanto menos tardemos en validarlo, menor ser\u00e1 el riesgo. La siguiente figura est\u00e1 sacada de la charla de Eduardo Ferro ( @eferro ) Continuous Delivery: Germinando una cultura \u00e1gil moderna . En la figura de la izquierda se entrega mucho valor de golpe y el riesgo que se ha ido acumulando es mucho mayor que en la figura de la derecha, en la que se entregan peque\u00f1os incrementos de valor que nos permiten tener una retroalimentaci\u00f3n m\u00e1s r\u00e1pida y adaptar mucho mejor el producto a las necesidades de los clientes. El proceso de puesta en producci\u00f3n del software depende mucho del tipo de software. En un extremo, por ejemplo, una p\u00e1gina web se puede cambiar modificando directamente el fichero HTML en la propia m\u00e1quina en la que se est\u00e1 ejecutando el servidor web. No hace falta ni recompilar, ni reiniciar el servidor. En el otro extremo, un software de control de una placa de un satelite espacial puede estar embebido en el propio firmware de la placa y para realizar un cambio puede ser necesario hasta volver a grabar y producir la placa. En general, la mayor\u00eda de sistemas software se encuentran entre ambos extremos. Es importante analizar con detalle cu\u00e1l es el proceso de despliegue de nuestro software, cu\u00e1nto tarda en subir a producci\u00f3n un cambio de una l\u00ednea de c\u00f3digo y cu\u00e1les son los cuellos de botella en el proceso. La denominada ultima milla consiste en los pasos necesarios para la puesta en producci\u00f3n de nuestro sistema. De nada nos sirve tener un equipo \u00e1gil que hace iteraciones y reuniones con el cliente si despu\u00e9s tenemos un equipo de QA ( Quality Assurance ) con un 90% de pruebas manuales y otro de operaciones que tiene que configurar manualmente cualquier nuevo despliegue a producci\u00f3n y al que le cuesta dos d\u00edas revertir un despliegue fallido. Debemos analizar cu\u00e1l es nuestro proceso de release y hacer lo posible por mejorarlo. Encontrar los cuellos de botella, reducir los tiempos, automatizar todo lo que podamos. De forma que pasemos de un release por trimestre a un release mensual. Y despu\u00e9s a un release cada dos semanas. Y despu\u00e9s a un release semanal. Y despu\u00e9s a un posible release con cada posible cada cambio. Al final, como dice Eduardo Ferro en la charla mencionada anteriormente, el tiempo de subir un commit a producci\u00f3n debe ser de menos de 15 minutos y debemos de poder automatizar el proceso de puesta en producci\u00f3n hasta el extremo que lo podamos hacer a discreci\u00f3n, cuando queramos, \u00fanicamente pulsando un bot\u00f3n. Un elemento central de todo el proceso de despliegue es la configuraci\u00f3n de un pipeline de despliegue lo m\u00e1s automatizada posible. el pipeline representa todos los pasos necesarios que llevan el c\u00f3digo fuente hasta producci\u00f3n. Lo veremos en detalle m\u00e1s adelante, pero es interesante adelantarla aqu\u00ed. En la imagen se puede ver: Compilaci\u00f3n de todas las dependencias en binarios. En el caso de una aplicaci\u00f3n Java podr\u00edamos tener dependencias externas (que no har\u00eda falta compilar, s\u00f3lo descargarse) y dependencias de librer\u00edas internas que s\u00ed que estamos modificando y que deber\u00edamos recompilar. Empaquetamiento , construcci\u00f3n de un \u00fanico binario a partir de todos los binarios existentes. En el caso de una aplicaci\u00f3n Java, la fase de package (por ejemplo, realizada con Maven) generar\u00eda un fichero WAR que podr\u00edamos distribuir. Tambi\u00e9n, si utilizamos Docker, en esta fase generaremos una m\u00e1quina Docker que podremos distribuir. Despliegue en distintos entornos de prueba y lanzamiento de pruebas en los distintos entornos. Cada entorno tiene su propia configuraci\u00f3n, definida por variables de entorno o par\u00e1metros de los comandos de puesta en marcha. Despliegue en entorno de staging (r\u00e9plica muy similar al entorno de producci\u00f3n). Despliegue en entorno de producci\u00f3n . En el enfoque de entrega continua el proceso anterior est\u00e1 completamente automatizado y la puesta en producci\u00f3n se puede modular y realizar en el momento que nos interese pulsando \u00fanicamente un bot\u00f3n en cualquier momento. Recordemos que la forma m\u00e1s tradicional de enfrentar el problema del lanzamiento es separar una rama de release de la rama de desarrollo. Al separar la rama de release podemos seguir introduciendo cambios en la rama de desarrollo sin afectar para nada al release. En la rama de release se realiza todo el pipeline de despliegue y se prueba en todos los entornos. Se introducen correcciones de peque\u00f1os bugs encontrados y se tambi\u00e9n se puede incluir alg\u00fan commit escogido de la rama de desarrollo haciendo un cherry-pick . Finalmente, la \u00faltima versi\u00f3n comprobada se pasa a producci\u00f3n y mezcla con la rama de releases y con la de desarrollo. En el enfoque de lanzamiento continuo no existen ramas de release, sino que en cualquier commit de la rama principal es candidato a ser puesto en producci\u00f3n. Integraci\u00f3n continua La Integraci\u00f3n continua es una pr\u00e1ctica en la que los miembros del equipo integran su trabajo frecuentemente en el proyecto. Se trata de una pr\u00e1ctica de XP (eXtreme Programming) en la que se recomienda que cada miembro integre sus cambios diariamente. Esto lleva a m\u00faltiples integraciones cada d\u00eda. Cada integraci\u00f3n es verificada por una compilaci\u00f3n autom\u00e1tica ( automated build ) en la que se lanzan todos los tests y se detectan errores lo m\u00e1s r\u00e1pidamente posible. Esta pr\u00e1ctica obliga a que todos los cambios realizados por los desarrolladores sean puestos en com\u00fan continuamente, lo que promueve la compartici\u00f3n de conocimiento entre todos los miembros del equipo. Cuando una persona va a integrar sus cambios primero debe comprobar que \u00e9stos son compatibles con los cambios que ha habido en el proyecto. Como se integra diariamente, \u00e9stos no ser\u00e1n demasiados y si hay alg\u00fan error ser\u00e1 f\u00e1cil de solucionar. Sin embargo, si se desarrolla una versi\u00f3n separada que tarda mucho en integrarse ser\u00e1 muy posible que cuando se realice la integraci\u00f3n surjan muchos problemas de m\u00e1s dif\u00edcil soluci\u00f3n. Trunk based vs. feature branches Uno de los debates frecuentes relacionados con los flujos de trabajo de Git es si es m\u00e1s conveniente un flujo de trabajo trunk based (basado en la rama principal) o uno con feature branches (ramas de caracter\u00edsticas). La imagen anterior est\u00e1 tambi\u00e9n sacada de la charla de Eduardo Ferro. En ella se muestran los dos flujos de trabajo y se muestran parejas de desarrolladores porque est\u00e1n aplicando tambi\u00e9n pair programming . En el flujo de desarrollo trunk based todos los desarrolladores publican sus commits continuamente (al menos una vez al d\u00eda) sobre la rama principal del proyecto. Esto obliga a mantenerse continuamente al d\u00eda sobre los cambios que otros est\u00e1n introduciendo y a tener cuidado de que nuestros cambios vayan en la misma direcci\u00f3n. El desarrollador actualiza su repositorio local y comienza a programar un peque\u00f1o incremento (c\u00f3digo y tests). Cuando termina lanza todos los tests para asegurarse de que no se ha roto nada. Antes de publicar los cambios, vuelve a actualizar el repositorio local con los nuevos cambios que se han a\u00f1adido a la rama principal y vuelve a lanzar los tests. Si todo funciona bien, publica los cambios en el repositorio compartido. Entre las ventajas de esta t\u00e9cnica se encuentran: La integraci\u00f3n de un nuevo commit es f\u00e1cil porque la rama principal ha cambiado poco desde el commit anterior que integramos. No ha habido demasiado tiempo para que el proyecto diverja mucho. La transparencia en los cambios hace que se detecten antes los error El conocimiento del equipo evoluciona conjuntamente. Todo el mundo tiene informaci\u00f3n actualizada a diario de los cambios que se van introduciendo en el proyecto. Obliga a dividir los cambios grandes en cambios peque\u00f1os que se van integrando poco a poco. Esto obliga a hacer un mayor esfuerzo de dise\u00f1o y utilizar mejores arquitecturas de software. Entre los inconvenientes podemos destacar: Interrupciones m\u00e1s frecuentes en el flujo de trabajo del equipo debido a problemas introducidos por malos commits. No se pueden hacer pull requests en los que se haga una revisi\u00f3n de c\u00f3digo. Necesidad m\u00e1s frecuente de reverts que corrigen equivocaciones. Obliga al equipo a una gran disciplina y a una gran madurez. No se deben buscar culpables por los errores introducidos. Los errores nos hacen aprender. El flujo de desarrollo de ramas de caracter\u00edsticas se basa en separar ramas de caracter\u00edsticas de la rama principal. En cada rama de caracter\u00edstica se desarrolla una caracter\u00edstica y se integra en la rama principal cuando est\u00e9 terminada. Esta integraci\u00f3n se puede hacer usando un pull request. Ventajas: Se integran en la rama principal cambios completos. Durante el desarrollo de la caracter\u00edstica puedes aislarte del resto del desarrollo del proyecto y centrarte \u00fanicamente en la caracter\u00edstica que est\u00e1s desarrollando. Los fallos son locales a la rama. Un fallo no afecta al resto del equipo. Puedes tomarte un tiempo en arreglar el fallo sin que el resto del equipo se quede bloqueado. Posibilidad de usar pull requests y realizar revisiones de c\u00f3digo. Inconvenientes: Si las ramas tienen una duraci\u00f3n muy larga el proyecto puede haberse modificado mucho cuando vayamos a hacer la integraci\u00f3n, haci\u00e9ndola bastante complicada. El conocimiento compartido sobre el c\u00f3digo del proyecto es mucho menor y se limita a los posibles conflictos que podemos tener en la rama que hemos desarrollado. El primero que integra su rama no tiene problemas, los problemas los tienen las siguientes integraciones. Esto crea un efecto perverso en el que intentamos ser los primeros posiblemente a costa de menos calidad en el c\u00f3digo. Posiblemente, la mejor opci\u00f3n sea comenzar con ramas de caracter\u00edsticas e ir haci\u00e9ndolas cada vez m\u00e1s cortas, de forma que se integren cada dos o tres d\u00edas como m\u00e1ximo. Al igual que en el enfoque de trunk based podr\u00edan no ser caracter\u00edsticas completas, sino peque\u00f1os incrementos. Por ejemplo, una rama podr\u00eda contener la parte de backend de la caracter\u00edstica y despu\u00e9s har\u00edamos la de frontend. Y cuando el equipo se acostumbre a hacer ramas cada vez m\u00e1s peque\u00f1as, podr\u00edamos plantearnos la opci\u00f3n de pasar a un modelo basado en trunk. Herramientas de integraci\u00f3n continua Una de las caracter\u00edsticas fundamentales de la integraci\u00f3n continua es que cada vez que se integra un commit en la rama principal se debe realizar una construcci\u00f3n autom\u00e1tica del proyecto, lanz\u00e1ndose todos los tests en el entorno de integraci\u00f3n continua y construy\u00e9ndose el binario candidato a desplegar en producci\u00f3n. La forma de realizar esto es mediante las denominadas herramientas de integraci\u00f3n continua. Podemos elegir como herramientas de integraci\u00f3n continua una herramienta que instalamos en nuestros propios servidores de integraci\u00f3n ( CI server ) o construcci\u00f3n continua ( continuous build server ) como Jenkins o tambi\u00e9n en un servicio en la nube como GitHub Actions . Cualquiera de estas herramientas permiten automatizar el lanzamiento de tests y la compilaci\u00f3n autom\u00e1tica de la aplicaci\u00f3n y la generaci\u00f3n de una aplicaci\u00f3n distribuible. Esta aplicaci\u00f3n puede ser un binario, un JAR o WAR, una m\u00e1quina Docker, etc. que puede ser desplegada en distintos entornos, incluido el de producci\u00f3n. El servicio de integraci\u00f3n continua genera tambi\u00e9n notificaciones autom\u00e1ticas a todos los miembros del equipo indicando el estado de la compilaci\u00f3n. Tambi\u00e9n suele proporcionar un panel de control con la indicaci\u00f3n del estado de cada build. Generaci\u00f3n del ejecutable El resultado de la compilaci\u00f3n autom\u00e1tica realizada por el servidor de integraci\u00f3n continua debe ser un artefacto desplegable en los distintos entornos en los que vamos a probar la aplicaci\u00f3n. La aplicaci\u00f3n s\u00f3lo se debe compilar una \u00fanica vez y el resultado debe almacenarse en un sitio accesible por cualquier proceso y miembro del equipo. Es fundamental que las herramientas usadas para construir la aplicaci\u00f3n puedan ser usadas desde l\u00ednea de comando. De esta forma es mucho m\u00e1s sencillo adaptar y configurar distintos tipos de scripts de compilaci\u00f3n ( build scripts ). Entre las herramientas m\u00e1s usadas destacamos las siguientes: Make (C, Unix) Rake (Ruby) Maven (Java) Gradle (Java, Scala, etc.) sbt (Scala, Play Framework) Igual que hay distintas herramientas de compilaci\u00f3n para los diferentes lenguajes de programaci\u00f3n, existen diferentes formatos en los que se guardan los artefactos binarios resultantes de la compilaci\u00f3n. Por ejemplo, el binario resultante de una aplicaci\u00f3n C es un fichero compilado que se ejecutar\u00e1 en el sistema operativo para el que haya sido compilada la aplicaci\u00f3n, mientras que el resultante de una aplicaci\u00f3n Java es un fichero JAR que podremos desplegar en cualquier m\u00e1quina en la que tengamos instalado un JRE ( Java Runtime Environment ). Adem\u00e1s tenemos el problema a\u00f1adido de generaci\u00f3n de distintos binarios para diferentes sistemas operativos. Por ejemplo, si estamos desarrollando una aplicaci\u00f3n de escritorio que va a funcionar en Windows, Linux y Mac deberemos generar los binarios correspondientes a esas distintas plataformas y despu\u00e9s testearlos de forma autom\u00e1tica en distintos ordenadores cada uno con su sistema operativo espec\u00edfico. En la actualidad se est\u00e1 haciendo cada vez m\u00e1s popular la utilizaci\u00f3n de im\u00e1genes Docker como artefacto binario a distribuir y ejecutar. Entre las ventajas de este enfoque se encuentran el ser multiplaforma (para ejecutarlas basta con tener instalado el Docker Engine ) y que los contenedores (servicios en ejecuci\u00f3n) se pueden configurar y combinar o ejecutar en clusters usando herramientas como Kubernetes . Es una buena pr\u00e1ctica darle a cada artefacto binario compilado un nombre distinto en el que aparezca el n\u00famero de versi\u00f3n. En el caso de la integraci\u00f3n continua, normalmente se le da al binario un nombre en el que aparece la fecha e incluso la hora de la compilaci\u00f3n. De esta forma, las distintas compilaciones pueden ser identificadas de forma \u00fanica. En el caso en que nuestra aplicaci\u00f3n dependa de paquetes externos es conveniente descargarlos y almacenarlos en un sitio centralizado de forma que no tengan que descargarse de Internet cada vez que se realiza una nueva compilaci\u00f3n. Para ello es conveniente configurar correctamente las cach\u00e9s del sistema de build que estemos utilizando. La aplicaci\u00f3n desplegable debe consistir en un \u00fanico artefacto con el nombre correcto que contenga todo lo necesario para ejecutarse en distintos entornos de prueba y pueda ser puesto en producci\u00f3n. El artefacto debe almacenarse en un lugar centralizado, accesible desde los distintos entornos de forma autom\u00e1tica. Por ejemplo, podemos usar un servidor web local y dejar el fichero en una URL concreta. Principios y pr\u00e1cticas de integraci\u00f3n continua A continuaci\u00f3n presentamos en forma de \u00edtems un resumen de los elementos importantes de la integraci\u00f3n continua que hemos visto hasta ahora. Desarrollo de c\u00f3digo El sistema debe siempre poder ser construido (build) y probado con \u00e9xito. Todo el mundo hace merge de los cambios con frecuencia. Despu\u00e9s de cada commit, el sistema se integra inmediata y autom\u00e1ticamente. Se desarrolla el sistema en peque\u00f1os incrementos. Testing Los desarrolladores prueban su c\u00f3digo en sus espacios de trabajo privados. Despu\u00e9s mezclan los cambios en el repositorio. Servidor de integraci\u00f3n continua (CI server): Monitoriza el repositorio y comprueba los cambios cuando ocurren. Construye el sistema y ejecuta las pruebas unitarias y de integraci\u00f3n. Informa al equipo de la construcci\u00f3n con \u00e9xito o de los fallos. Errores en los build El equipo arregla el problema lo antes posible. Continuar para integrar y probar continuamente durante todo el proyecto. El \u00faltimo ejecutable compilado debe estar f\u00e1cilmente disponible El resultado de la compilaci\u00f3n debe ser un artefacto ejecutable disponible para desplegar en distintos entornos, incluso en producci\u00f3n. Configuraci\u00f3n del despliegue Para hacer integraci\u00f3n continua es necesario tener m\u00faltiples entornos. En el entorno de trabajo del desarrollador se pueden lanzar los tests m\u00e1s r\u00e1pidos y utilizar una configuraci\u00f3n en la que usemos una base de datos en memoria (como H2) que acelere la velocidad de los tests que utilicen base de datos. Se puede configurar un primer entorno de integraci\u00f3n continua para que se ejecuten en \u00e9l tests m\u00e1s lentos y con una configuraci\u00f3n m\u00e1s parecida a la del entorno de producci\u00f3n. Por ejemplo, se puede utilizar una configuraci\u00f3n en la que se utilice una base de datos similar a la que se usa en producci\u00f3n, poblada con datos de prueba. En este entorno se deben lanzar los tests r\u00e1pidos y tambi\u00e9n tests m\u00e1s lentos de integraci\u00f3n. Y despu\u00e9s tendremos otros entornos cada vez m\u00e1s similares al entorno de producci\u00f3n en los que tambi\u00e9n se realizar\u00e1n todos los tests autom\u00e1ticos y otros tests manuales necesarios para tener la confirmaci\u00f3n de que todo est\u00e1 funcionando correctamente. En todos estos entornos deberemos instalar la misma aplicaci\u00f3n compilada, y tendremos que modificar en cada caso su configuraci\u00f3n. Es importante tener la capacidad de automatizar tanto la gesti\u00f3n de entornos de despliegue (arrancar las bases de datos correctas, configurar puertos, etc.) como la instalaci\u00f3n y ejecuci\u00f3n de la aplicaci\u00f3n en el entorno. Antiguamente los distintos entornos eran m\u00e1quinas f\u00edsicas distintas, con configuraciones distintas previamente instaladas en cada una de ellas. Hoy en d\u00eda es mucho m\u00e1s com\u00fan utilizar entornos virtuales f\u00e1cilmente construibles a partir de scripts y c\u00f3digo usando herramientas de virtualizaci\u00f3n como Docker, Kubernetes, etc. Tambi\u00e9n es habitual la utilizaci\u00f3n de entornos en la nube (Heroku, GitHub Actions, Amazon Web Services, etc.) tanto para prueba como para producci\u00f3n. Entornos de despliegue Podemos diferenciar diferentes tipos de entornos (configuraci\u00f3n de servicios y servidores) en los que se despliega y prueba el build de la aplicaci\u00f3n. En general, ordenados de menor a mayor parecido a producci\u00f3n, podemos diferenciar. Local : ordenador del desarrollador. Se ejecutan tests unitarios de la caracter\u00edstica que se est\u00e1 desarrollando. Desarrollo/Trunk/Master : ordenador de integraci\u00f3n continua conectado a la rama de desarrollo en el que se ejecutan todos los tests unitarios continuamente. Integraci\u00f3n : Entorno en el que se sustituyen los mocks y bases de datos de memoria por servicios reales, aunque con copias parciales de los datos de producci\u00f3n. Test/QA : Entornos en los que se realizan pruebas funcionales, de interfaz de usuario, de performance o de seguridad, entre otros. Pueden ser manuales y/o automatizados. Stage/Preproducci\u00f3n : Entorno id\u00e9ntico al de producci\u00f3n en el que se hace la \u00faltima validaci\u00f3n de la nueva versi\u00f3n a desplegar a producci\u00f3n. Copia de la base de datos de producci\u00f3n y con servidores similares a los de producci\u00f3n, para poder comprobar rendimiento. Producci\u00f3n : Entorno que usan los clientes reales de la aplicaci\u00f3n. Configuraci\u00f3n de la aplicaci\u00f3n La aplicaci\u00f3n debe poder funcionar en distintos entornos sin tener que ser recompilada. Para ello es necesario poder configurar su funcionamiento definiendo par\u00e1metros que podamos modificar previamente a su ejecuci\u00f3n sin tener que recompilarla. Existen multitud de elementos que podemos necesitar configurar dependiendo del entorno en que queremos que funcione la aplicaci\u00f3n. Por ejemplo: URL de conexi\u00f3n a la base de datos Usuario y contrase\u00f1a de conexi\u00f3n a la base de datos Puerto en el que la aplicaci\u00f3n va a recibir las peticiones Direcciones de los servicios a los que debe conectarse (por ejemplo, servicio SMTP de correo electr\u00f3nico) En cada entorno en los que va a funcionar la aplicaci\u00f3n estos par\u00e1metros van a tener unos valores distintos que hay que pasarle a la aplicaci\u00f3n cuando se ponga en funcionamiento. Existen diversas formas de definir estas propiedades. Las m\u00e1s usuales son: Mediante ficheros de configuraci\u00f3n de la aplicaci\u00f3n. Mediante variables de entorno cuyos valores establecemos con scripts antes de lanzar la aplicaci\u00f3n. Mediante argumentos del comando que lanza la aplicaci\u00f3n Estas distintas configuraciones deben estar tambi\u00e9n guardadas en el control de versiones, igual que el c\u00f3digo de la aplicaci\u00f3n, para poder tambi\u00e9n controlar su evoluci\u00f3n y sus cambios. Configuraci\u00f3n de im\u00e1genes Docker Docker tambi\u00e9n tiene muchas estrategias que permiten configurar la ejecuci\u00f3n de una imagen. Una de las m\u00e1s utilizadas es hacer que la imagen utilice variables de entorno que pueden ser modificadas en al lanzar el contenedor. Para ello debemos definir en la imagen estas variables con el comando ENV , pudiendo dar valores por defecto. Por ejemplo, podemos definir el siguiente Dockerfile : FROM alpine ENV saludo=\"Hola, mundo!\" CMD echo $saludo Creamos la imagen: $ docker build -t saludo . Si lanzamos la imagen muestra el saludo por defecto: $ docker run saludo Hola, mundo! Y podemos configurar el saludo de varias formas. Por ejemplo, indicando el valor del par\u00e1metro al hacer run : $ docker run -e \"saludo=\u00bfQu\u00e9 tal est\u00e1s?\" saludo \u00bfQu\u00e9 tal est\u00e1s? O guardando el valor del par\u00e1metro en un fichero de propiedades que pasamos al ejecutar la imagen. Por ejemplo, en el fichero propiedades.txt escribimos lo siguiente: saludo=\u00bfQu\u00e9 passsa, colega? Y ejecutamos la imagen de la siguiente forma: $ docker run --env-file=propiedades.txt saludo \u00bfQu\u00e9 passsa, colega? Entrega continua El concepto de Entrega continua ( Continuous Delivery ) es una extensi\u00f3n de la Integraci\u00f3n continua que se populariz\u00f3 a ra\u00edz del libro que publicaron en 2010 Jez Humble y David Farley (ver las referencias). Es un concepto que parte de la integraci\u00f3n continua para llegar a una automatizaci\u00f3n completa de la puesta en producci\u00f3n. El objetivo es conseguir una puesta en producci\u00f3n (release) del software: Poco arriesgada Frecuente Barata R\u00e1pida Predecible Reproducible En palabras de Jez Humble, la entrega continua consiste en: \u201cReduce the cost, time, and risk of delivering incremental changes to users\u201d Jez Humble (2013), Charla Adopting Continuous Delivery Otra frase muy importante, que ya hemos comentado alguna vez: \u201cHow long would it take your organization to deploy a change that involved just one single line of code? Do you do this on a repeatable, reliable basis?\u201d Mary Poppendieck Algunas t\u00e9cnicas que se utilizan en la Entrega continua (muchas de ellas ya las hemos visto): Peque\u00f1os cambios que se despliegan continuamente Todos los builds son candidatos al release Todo en el control de versiones (se debe poder probar cualquier release) Pipelines de despliegue (deployment pipelines) Integraci\u00f3n continua: automatizaci\u00f3n de builds, tests, despliegues, entornos Pipeline de despliegue Tal y como hemos comentado cuando habl\u00e1bamos de integraci\u00f3n continua, un elemento central es de la automatizaci\u00f3n es el pipeline de despliegue. un pipeline de despliegue es una implementaci\u00f3n automatizada del proceso de construcci\u00f3n, despliegue, prueba y lanzamiento de nuestro sistema. La utilizaci\u00f3n de un pipeline de despliegue garantiza la visibilidad de todo el proceso, lo que garantiza un feedback temprano y un control continuo del mismo. Explicaci\u00f3n del libro de Jeff Humble: Cada cambio que se realiza sobre la configuraci\u00f3n de la aplicaci\u00f3n, su c\u00f3digo fuente o sus datos, lanza la creaci\u00f3n de una nueva instancia de el pipeline. Uno de los primeros pasos en el pipeline es crear los binarios y los instaladores. El resto de el pipeline ejecuta una serie de tests sobre los binarios para probar que pueden ser lanzados. Cada test que pasa el candidato a release nos da m\u00e1s confianza de que funcionar\u00e1 correctamente esta combinaci\u00f3n particular de c\u00f3digo binario, informaci\u00f3n de configuraci\u00f3n, entorno y datos. Si el candidato a release pasa todos los tests, puede ser lanzado. En el libro de Humble y Farley se muestra el siguiente esquema que representa sus distintos elementos. En la parte superior se muestra el sistema de control de versiones, en donde se almacena el c\u00f3digo del proyecto y los datos de las distintas configuraciones de los entornos y de la aplicaci\u00f3n. Las configuraciones de los entornos y de la aplicaci\u00f3n se deben guardar en el sistema de control de versiones para gestionar su evoluci\u00f3n y modificaci\u00f3n de la misma forma que gestionamos la evoluci\u00f3n del c\u00f3digo. En la parte inferior se muestra el repositorio de artefactos en donde se almacenan los binarios de la aplicaci\u00f3n. Puede ser, por ejemplo, Docker Hub en el caso de ser una aplicaci\u00f3n dockerizada. En la fase de commit el c\u00f3digo se compila y se lanzan los tests unitarios. Se generan los binarios que se almacenan en el repositorio de artefactos. En la fase de aceptaci\u00f3n se configuran y despliegan los binarios en un entorno similar al de producci\u00f3n. Se realizan test de aceptaci\u00f3n/integraci\u00f3n y se valida la aplicaci\u00f3n y se deja lista para ser publicada a producci\u00f3n por parte de Operaciones. En la fase de UAT (User Acceptance Testing) se realizan pruebas manuales en un entorno lo m\u00e1s parecido posible al de producci\u00f3n. En la fase capacidad se realizan tests de rendimiento. El binario se despliega en producci\u00f3n si todas las fases anteriores se pasan con \u00e9xito. En la siguiente figura se muestra un ejemplo de posible secuencia de despliegue: Son muy \u00fatiles los tableros de control de el pipeline de despliegue, como por ejemplo el que proporciona Jenkins. Peque\u00f1os cambios Tal y como hemos comentado en la introducci\u00f3n del tema, una pr\u00e1ctica fundamental de los equipos que utilizan las t\u00e9cnicas de entrega continua es subir a producci\u00f3n continuamente peque\u00f1os cambios con los que se van introduciendo poco a poco las nuevas funcionalidades. Por ejemplo, en la charla de Juan Ignacio S\u00e1nchez Continuous Integration at CartoDB ( v\u00eddeo ) se explica c\u00f3mo es el proceso de integraci\u00f3n continua en Carto , una importante empresa espa\u00f1ola de gesti\u00f3n de datos geogr\u00e1ficos. Algunas de las m\u00e9tricas que muestra en la charla reflejan claramente cu\u00e1l es el funcionamiento de la integraci\u00f3n continua en la empresa. Su producto opensource m\u00e1s importante es CartoDB (enlace a su repositorio GitHub ). En la \u00e9poca de la charla el producto integraba una media de 22 pull requests semanales y 15 despliegues en producci\u00f3n. Si todos los cambios son peque\u00f1os, \u00bfc\u00f3mo se introducen los cambios grandes en el proyecto?. Por ejemplo, nuevas caracter\u00edsticas complejas en las que se necesitan combinar distintas funcionalidades elementales. Es posible ir desarrollando, probando y colocando las piezas en el c\u00f3digo (sin mostrar en la interfaz de usuario) para que el sistema evolucione hacia un momento futuro en sea f\u00e1cil introducir una caracter\u00edstica totalmente nueva mediante un peque\u00f1o cambio. Como dice Kent Beck: Make the change easy, then make the easy change. Para ello podemos usar las siguientes estrategias: Codificaci\u00f3n y prueba de las peque\u00f1as funcionalidades por separado. Buen dise\u00f1o de c\u00f3digo, por ejemplo seleccionar una implementaci\u00f3n concreta utilizando interfaces y factor\u00edas, pero dejar la estructura lista para introducir futuros cambios. Peque\u00f1os cambios en las APIs compatibles con los tests de regresi\u00f3n. Uso de mocks. Interruptores de caracter\u00edsticas. Esta \u00faltima t\u00e9cnica es muy interesante. Consiste en definir interruptores o flags booleanos en el c\u00f3digo que hagan que ciertas caracter\u00edsticas se muestren o no en la aplicaci\u00f3n dependiendo de si los flags est\u00e1n o no activos. En este art\u00edculo se puede encontrar una explicaci\u00f3n en profundidad de m\u00faltiples t\u00e9cnicas usadas para implementar los interruptores de caracter\u00edsticas. Dependiendo de la t\u00e9cnica es posible hasta definir interruptores que se puedan modificar en tiempo de ejecuci\u00f3n e incluso que se puedan mostrar o no la funcionalidad a seg\u00fan qu\u00e9 usuarios implementando un sistema de canary release . Canary release La idea del canary release consiste en configurar un sistema de despliegue que permita mantener simult\u00e1neamente en producci\u00f3n dos versiones de la aplicaci\u00f3n. En el caso de una aplicaci\u00f3n web, podr\u00edamos configurar un proxy o router intermedio que se encargue de encauzar las peticiones de los usuarios a una versi\u00f3n de la aplicaci\u00f3n o a otra. Cuando se lanza una caracter\u00edstica nueva se puede configurar el proxy para que s\u00f3lo sea probada por una peque\u00f1a cantidad de usuarios y detectar posibles errores en este despliegue reducido. Cuando se haya comprobado con este peque\u00f1o grupo que todo funciona correctamente se modifica la configuraci\u00f3n del proxy para que todos accedan a la nueva versi\u00f3n. La configuraci\u00f3n del proxy puede llegar a ser bastante compleja, haciendo el filtro de usuarios en funci\u00f3n de par\u00e1metros que nos interesen (localizaci\u00f3n, tipo de usuario, etc.). Este sistema tambi\u00e9n puede utilizarse, junto con el de interruptores de caracter\u00edsticas, para realizar pruebas A/B de nuevas caracter\u00edsticas. DevOps Tradicionalmente el trabajo de los desarrolladores y el de los t\u00e9cnicos de operaciones (responsables de la puesta en producci\u00f3n del sistema, monitorizaci\u00f3n de servidores, etc. ) son contrapuestos. Dev elopers: Su trabajo es a\u00f1adir nuevas caracter\u00edsticas. Trabajan en entornos locales (\u201cen mim\u00e1quina funciona\u201d). Utilizan herramientas y lenguajes que permiten abstraer y automatizar. Op erations: Su trabajo es mantener el sitio web seguro, estable y r\u00e1pido. Detectar problemas, apagar fuegos. Los desarrolladores quieren introducir cambios r\u00e1pidamente en el sistema, mientras que a los t\u00e9cnicos de operaciones les gustar\u00eda mantener el sistema lo m\u00e1s estable posible. Los profesionales DevOps representan una nueva filosof\u00eda de trabajo, que combina elementos propios de los desarrolladores y de operaciones, incorporando todas las nuevas t\u00e9cnicas de las que hemos estado hablando en este tema y herramientas denominadas Infrastructure-as-code como Docker, Kubernetes, Ansible, Terraform,etc. en las que podemos definir la configuraci\u00f3n de entornos y servidores usando c\u00f3digo y ficheros almacenables en un sistema de control de versiones, en lugar de tener que configurar f\u00edsicamente el hardware. Principos y buenas pr\u00e1cticas En la charla mencionada anteriormente sobre pr\u00e1cticas de integraci\u00f3n continua en Carto, Juan Ignacio S\u00e1nchez lista 10 buenas pr\u00e1cticas que ellos est\u00e1n siguiendo. Muchas ya las hemos visto, pero es interesante repasarlas todas juntas, validadas por la experiencia de su \u00e9xito en una empresa puntera de desarrollo de software. Son las siguientes: Mantener un repositorio de c\u00f3digo Automatizar la compilaci\u00f3n Hacer la compilaci\u00f3n auto-testeable (mediante test autom\u00e1ticos) Todo el mundo realiza commits en la rama principal todos los d\u00edas Cada commit en la rama principal debe ser compilado Mantener la compilaci\u00f3n r\u00e1pida Testear en un clon del entorno de producci\u00f3n Hacer f\u00e1cil de obtener los \u00faltimos productos compilados Todo el mundo puede ver los resultados de las \u00faltimas compilaciones Automatizar el despliegue Consejos adicionales La integraci\u00f3n continua y el despliegue continuo son dos conceptos que pueden convertirse en aliados muy poderosos para nuestro equipo si ponemos los medios necesarios. Es obvio que nada de esto es posible sin una bater\u00eda de pruebas s\u00f3lidas, extensas y consistentes. Si no disponemos de pruebas unitarias de calidad y que cubran la totalidad de nuestro c\u00f3digo, lo m\u00e1s probable es que nuestro CI nos de muchos falsos positivos (fallos que no han sido detectados por las pruebas unitarias), que a su vez provocar\u00e1 que se desplieguen fallos en nuestros entornos de pre-producci\u00f3n y producci\u00f3n. Por ejemplo, si el proceso de integraci\u00f3n continua se ejecuta en nuestro servidor de Gitlab en vez de en el ordenador de cada desarrollador de nuestro equipo, estaremos facilitando la tarea de comprobaci\u00f3n automatizada de error. A\u00fan m\u00e1s importante, si dicho proceso se ejecuta en pocos minutos, estaremos creando un h\u00e1bito entre las personas del proyecto de enviar m\u00e1s a menudo sus cambios al repositorio, ya que hacerlo les permitir\u00e1 obtener una visi\u00f3n r\u00e1pida sobre la calidad de su trabajo. Esto a su vez har\u00e1 que la frecuencia de los cambios que se env\u00edan al repositorio sea m\u00e1s alta, mientras que el tama\u00f1o de cada cambio enviado sea m\u00e1s peque\u00f1o. Las tareas de revisi\u00f3n se har\u00e1n m\u00e1s amenas y f\u00e1ciles, y por lo tanto, mejora la velocidad con la que se integran dichos cambios en \u00abdevelop\u00bb. Nuestro equipo de control de calidad podr\u00e1 tener un flujo continuo de trabajo, lo cual reducir\u00e1 los tiempos \u00abmuertos\u00bb (personas \u00abbloqueadas\u00bb a la espera de que otras personas terminen su trabajo). Por \u00faltimo, cabe decir que no es obligatorio que el despliegue continuo tenga lugar en cuanto un cambio se mezcle en las ramas de \u00abdevelop\u00bb o \u00abmaster\u00bb. Existen muchos casos en los que el l\u00edder del equipo (o una persona con rol similar) deber\u00e1 tomar la decisi\u00f3n de dar luz verde al proceso de despliegue en base a m\u00e1s factores. Es decir, el proceso en s\u00ed puede estar automatizado (es lo mas recomendable), pero la acci\u00f3n que desencadena el despliegue puede ser humana. Referencias Charla de Eduardo Ferro (2020): Continuous Delivery: Germinando una cultura \u00e1gil moderna . Martin Fowler (2006): Continuous Integration Jez Humble y David Farley (2010): Continuous Delivery Integraci\u00f3n continua y despliegue continuo","title":"Tema 7 - CI/CD (Integraci\u00f3n y despliegue continuo)"},{"location":"cicd/#cicd-integracion-y-despliegue-continuo","text":"Informaci\u00f3n Estos apuntes est\u00e1n basados en gran medida en unos apuntes de Domingo Gallardo y se distribuyen porque su licencia as\u00ed lo permite.","title":"CI/CD (Integraci\u00f3n y despliegue continuo)"},{"location":"cicd/#continuous-delivery-cd","text":"Una idea fundamental de las metodolog\u00edas \u00e1giles es entregar valor frecuentemente para obtener una pronta retroalimentaci\u00f3n del cliente. Para ello es necesario tener muy engrasados los procesos de despliegue y puesta en producci\u00f3n del software. Una de las formas que facilita la optimizaci\u00f3n de la puesta en producci\u00f3n de software es la pr\u00e1ctica de XP ( eXtreme Programming ) de Integraci\u00f3n continua ( Continuous Integration ). En esta pr\u00e1ctica los miembros del equipo integran sus commits diariamente en el proyecto y en cada integraci\u00f3n se lanzan tests automatizados que verifican que los cambios no introducen errores. Adem\u00e1s de esta pr\u00e1ctica, debemos tener tambi\u00e9n automatizados todos los procesos de compilaci\u00f3n ( build ) y despliegue ( deployment ) de la aplicaci\u00f3n en los distintos entornos de prueba. Esto es lo que se denomina Despliegue continuo ( Continuous Deployment ). En esta l\u00ednea, se han popularizado herramientas como Docker o Kubernetes que facilitan el despliegue del software y su automatizaci\u00f3n y cada vez se demandan m\u00e1s profesionales (denominados DevOps ) con capacidad de gestionar estos despliegues automatizados. Y en los \u00faltimos a\u00f1os se ha dado un paso m\u00e1s all\u00e1 y se ha comenzado a hablar de Entrega continua ( Continous Delivery en ingl\u00e9s) con la idea de promover software que est\u00e9 listo en cualquier momento para salir a producci\u00f3n. En este tema veremos todos estos conceptos, con la idea de tomar un primer contacto con todos ellos. Necesitar\u00edamos un curso (o m\u00e1s) para verlos en profundidad. Intentaremos al menos conocer los conceptos b\u00e1sicos para poder seguir profundizando en alguno de ellos en el futuro.","title":"Continuous Delivery (CD)"},{"location":"cicd/#el-problema-de-la-puesta-en-produccion","text":"En las empresas tradicionales no \u00e1giles el proceso de subir a producci\u00f3n una nueva versi\u00f3n es un proceso muy complicado y estresante. Se hace pocas veces, cuatro o cinco veces al a\u00f1o, durante el fin de semana cuando todos los servicios est\u00e1n parados. El proceso de genera muchos trastornos y dolores de cabeza. El equipo de operaciones tiene que estar pendiente del m\u00f3vil para detectar posibles problemas y ca\u00eddas del sistema. Una vez puesto el software en producci\u00f3n el equipo de desarrollo se dedicar\u00e1 continuamente a corregir bugs y solucionar problemas detectados por los usuarios. Esto no es \u00e1gil. Esto no permite conseguir lo que hemos comentado muchas veces de un ciclo corto de retroalimentaci\u00f3n para que el cliente pueda probar r\u00e1pidamente las nuevas caracter\u00edsticas y se pueda comprobar su valor. Recordemos que en ambientes inciertos y no predecibles es fundamental poder validar con el cliente las nuevas funcionalidades introducidas, para adaptarse y corregir posibles errores. La realizaci\u00f3n de entregas frecuentes tambi\u00e9n permite minimizar el riesgo. Todo el tiempo que estamos desarrollando algo sin ponerlo en producci\u00f3n es un riesgo acumulado. Hasta que no est\u00e1 en producci\u00f3n y ha sido aceptado por el cliente no sabemos si lo que estamos desarrollando va a ser validado o no. Cuanto menos tardemos en validarlo, menor ser\u00e1 el riesgo. La siguiente figura est\u00e1 sacada de la charla de Eduardo Ferro ( @eferro ) Continuous Delivery: Germinando una cultura \u00e1gil moderna . En la figura de la izquierda se entrega mucho valor de golpe y el riesgo que se ha ido acumulando es mucho mayor que en la figura de la derecha, en la que se entregan peque\u00f1os incrementos de valor que nos permiten tener una retroalimentaci\u00f3n m\u00e1s r\u00e1pida y adaptar mucho mejor el producto a las necesidades de los clientes. El proceso de puesta en producci\u00f3n del software depende mucho del tipo de software. En un extremo, por ejemplo, una p\u00e1gina web se puede cambiar modificando directamente el fichero HTML en la propia m\u00e1quina en la que se est\u00e1 ejecutando el servidor web. No hace falta ni recompilar, ni reiniciar el servidor. En el otro extremo, un software de control de una placa de un satelite espacial puede estar embebido en el propio firmware de la placa y para realizar un cambio puede ser necesario hasta volver a grabar y producir la placa. En general, la mayor\u00eda de sistemas software se encuentran entre ambos extremos. Es importante analizar con detalle cu\u00e1l es el proceso de despliegue de nuestro software, cu\u00e1nto tarda en subir a producci\u00f3n un cambio de una l\u00ednea de c\u00f3digo y cu\u00e1les son los cuellos de botella en el proceso. La denominada ultima milla consiste en los pasos necesarios para la puesta en producci\u00f3n de nuestro sistema. De nada nos sirve tener un equipo \u00e1gil que hace iteraciones y reuniones con el cliente si despu\u00e9s tenemos un equipo de QA ( Quality Assurance ) con un 90% de pruebas manuales y otro de operaciones que tiene que configurar manualmente cualquier nuevo despliegue a producci\u00f3n y al que le cuesta dos d\u00edas revertir un despliegue fallido. Debemos analizar cu\u00e1l es nuestro proceso de release y hacer lo posible por mejorarlo. Encontrar los cuellos de botella, reducir los tiempos, automatizar todo lo que podamos. De forma que pasemos de un release por trimestre a un release mensual. Y despu\u00e9s a un release cada dos semanas. Y despu\u00e9s a un release semanal. Y despu\u00e9s a un posible release con cada posible cada cambio. Al final, como dice Eduardo Ferro en la charla mencionada anteriormente, el tiempo de subir un commit a producci\u00f3n debe ser de menos de 15 minutos y debemos de poder automatizar el proceso de puesta en producci\u00f3n hasta el extremo que lo podamos hacer a discreci\u00f3n, cuando queramos, \u00fanicamente pulsando un bot\u00f3n. Un elemento central de todo el proceso de despliegue es la configuraci\u00f3n de un pipeline de despliegue lo m\u00e1s automatizada posible. el pipeline representa todos los pasos necesarios que llevan el c\u00f3digo fuente hasta producci\u00f3n. Lo veremos en detalle m\u00e1s adelante, pero es interesante adelantarla aqu\u00ed. En la imagen se puede ver: Compilaci\u00f3n de todas las dependencias en binarios. En el caso de una aplicaci\u00f3n Java podr\u00edamos tener dependencias externas (que no har\u00eda falta compilar, s\u00f3lo descargarse) y dependencias de librer\u00edas internas que s\u00ed que estamos modificando y que deber\u00edamos recompilar. Empaquetamiento , construcci\u00f3n de un \u00fanico binario a partir de todos los binarios existentes. En el caso de una aplicaci\u00f3n Java, la fase de package (por ejemplo, realizada con Maven) generar\u00eda un fichero WAR que podr\u00edamos distribuir. Tambi\u00e9n, si utilizamos Docker, en esta fase generaremos una m\u00e1quina Docker que podremos distribuir. Despliegue en distintos entornos de prueba y lanzamiento de pruebas en los distintos entornos. Cada entorno tiene su propia configuraci\u00f3n, definida por variables de entorno o par\u00e1metros de los comandos de puesta en marcha. Despliegue en entorno de staging (r\u00e9plica muy similar al entorno de producci\u00f3n). Despliegue en entorno de producci\u00f3n . En el enfoque de entrega continua el proceso anterior est\u00e1 completamente automatizado y la puesta en producci\u00f3n se puede modular y realizar en el momento que nos interese pulsando \u00fanicamente un bot\u00f3n en cualquier momento. Recordemos que la forma m\u00e1s tradicional de enfrentar el problema del lanzamiento es separar una rama de release de la rama de desarrollo. Al separar la rama de release podemos seguir introduciendo cambios en la rama de desarrollo sin afectar para nada al release. En la rama de release se realiza todo el pipeline de despliegue y se prueba en todos los entornos. Se introducen correcciones de peque\u00f1os bugs encontrados y se tambi\u00e9n se puede incluir alg\u00fan commit escogido de la rama de desarrollo haciendo un cherry-pick . Finalmente, la \u00faltima versi\u00f3n comprobada se pasa a producci\u00f3n y mezcla con la rama de releases y con la de desarrollo. En el enfoque de lanzamiento continuo no existen ramas de release, sino que en cualquier commit de la rama principal es candidato a ser puesto en producci\u00f3n.","title":"El problema de la puesta en producci\u00f3n"},{"location":"cicd/#integracion-continua","text":"La Integraci\u00f3n continua es una pr\u00e1ctica en la que los miembros del equipo integran su trabajo frecuentemente en el proyecto. Se trata de una pr\u00e1ctica de XP (eXtreme Programming) en la que se recomienda que cada miembro integre sus cambios diariamente. Esto lleva a m\u00faltiples integraciones cada d\u00eda. Cada integraci\u00f3n es verificada por una compilaci\u00f3n autom\u00e1tica ( automated build ) en la que se lanzan todos los tests y se detectan errores lo m\u00e1s r\u00e1pidamente posible. Esta pr\u00e1ctica obliga a que todos los cambios realizados por los desarrolladores sean puestos en com\u00fan continuamente, lo que promueve la compartici\u00f3n de conocimiento entre todos los miembros del equipo. Cuando una persona va a integrar sus cambios primero debe comprobar que \u00e9stos son compatibles con los cambios que ha habido en el proyecto. Como se integra diariamente, \u00e9stos no ser\u00e1n demasiados y si hay alg\u00fan error ser\u00e1 f\u00e1cil de solucionar. Sin embargo, si se desarrolla una versi\u00f3n separada que tarda mucho en integrarse ser\u00e1 muy posible que cuando se realice la integraci\u00f3n surjan muchos problemas de m\u00e1s dif\u00edcil soluci\u00f3n.","title":"Integraci\u00f3n continua"},{"location":"cicd/#trunk-based-vs-feature-branches","text":"Uno de los debates frecuentes relacionados con los flujos de trabajo de Git es si es m\u00e1s conveniente un flujo de trabajo trunk based (basado en la rama principal) o uno con feature branches (ramas de caracter\u00edsticas). La imagen anterior est\u00e1 tambi\u00e9n sacada de la charla de Eduardo Ferro. En ella se muestran los dos flujos de trabajo y se muestran parejas de desarrolladores porque est\u00e1n aplicando tambi\u00e9n pair programming . En el flujo de desarrollo trunk based todos los desarrolladores publican sus commits continuamente (al menos una vez al d\u00eda) sobre la rama principal del proyecto. Esto obliga a mantenerse continuamente al d\u00eda sobre los cambios que otros est\u00e1n introduciendo y a tener cuidado de que nuestros cambios vayan en la misma direcci\u00f3n. El desarrollador actualiza su repositorio local y comienza a programar un peque\u00f1o incremento (c\u00f3digo y tests). Cuando termina lanza todos los tests para asegurarse de que no se ha roto nada. Antes de publicar los cambios, vuelve a actualizar el repositorio local con los nuevos cambios que se han a\u00f1adido a la rama principal y vuelve a lanzar los tests. Si todo funciona bien, publica los cambios en el repositorio compartido. Entre las ventajas de esta t\u00e9cnica se encuentran: La integraci\u00f3n de un nuevo commit es f\u00e1cil porque la rama principal ha cambiado poco desde el commit anterior que integramos. No ha habido demasiado tiempo para que el proyecto diverja mucho. La transparencia en los cambios hace que se detecten antes los error El conocimiento del equipo evoluciona conjuntamente. Todo el mundo tiene informaci\u00f3n actualizada a diario de los cambios que se van introduciendo en el proyecto. Obliga a dividir los cambios grandes en cambios peque\u00f1os que se van integrando poco a poco. Esto obliga a hacer un mayor esfuerzo de dise\u00f1o y utilizar mejores arquitecturas de software. Entre los inconvenientes podemos destacar: Interrupciones m\u00e1s frecuentes en el flujo de trabajo del equipo debido a problemas introducidos por malos commits. No se pueden hacer pull requests en los que se haga una revisi\u00f3n de c\u00f3digo. Necesidad m\u00e1s frecuente de reverts que corrigen equivocaciones. Obliga al equipo a una gran disciplina y a una gran madurez. No se deben buscar culpables por los errores introducidos. Los errores nos hacen aprender. El flujo de desarrollo de ramas de caracter\u00edsticas se basa en separar ramas de caracter\u00edsticas de la rama principal. En cada rama de caracter\u00edstica se desarrolla una caracter\u00edstica y se integra en la rama principal cuando est\u00e9 terminada. Esta integraci\u00f3n se puede hacer usando un pull request. Ventajas: Se integran en la rama principal cambios completos. Durante el desarrollo de la caracter\u00edstica puedes aislarte del resto del desarrollo del proyecto y centrarte \u00fanicamente en la caracter\u00edstica que est\u00e1s desarrollando. Los fallos son locales a la rama. Un fallo no afecta al resto del equipo. Puedes tomarte un tiempo en arreglar el fallo sin que el resto del equipo se quede bloqueado. Posibilidad de usar pull requests y realizar revisiones de c\u00f3digo. Inconvenientes: Si las ramas tienen una duraci\u00f3n muy larga el proyecto puede haberse modificado mucho cuando vayamos a hacer la integraci\u00f3n, haci\u00e9ndola bastante complicada. El conocimiento compartido sobre el c\u00f3digo del proyecto es mucho menor y se limita a los posibles conflictos que podemos tener en la rama que hemos desarrollado. El primero que integra su rama no tiene problemas, los problemas los tienen las siguientes integraciones. Esto crea un efecto perverso en el que intentamos ser los primeros posiblemente a costa de menos calidad en el c\u00f3digo. Posiblemente, la mejor opci\u00f3n sea comenzar con ramas de caracter\u00edsticas e ir haci\u00e9ndolas cada vez m\u00e1s cortas, de forma que se integren cada dos o tres d\u00edas como m\u00e1ximo. Al igual que en el enfoque de trunk based podr\u00edan no ser caracter\u00edsticas completas, sino peque\u00f1os incrementos. Por ejemplo, una rama podr\u00eda contener la parte de backend de la caracter\u00edstica y despu\u00e9s har\u00edamos la de frontend. Y cuando el equipo se acostumbre a hacer ramas cada vez m\u00e1s peque\u00f1as, podr\u00edamos plantearnos la opci\u00f3n de pasar a un modelo basado en trunk.","title":"Trunk based vs. feature branches"},{"location":"cicd/#herramientas-de-integracion-continua","text":"Una de las caracter\u00edsticas fundamentales de la integraci\u00f3n continua es que cada vez que se integra un commit en la rama principal se debe realizar una construcci\u00f3n autom\u00e1tica del proyecto, lanz\u00e1ndose todos los tests en el entorno de integraci\u00f3n continua y construy\u00e9ndose el binario candidato a desplegar en producci\u00f3n. La forma de realizar esto es mediante las denominadas herramientas de integraci\u00f3n continua. Podemos elegir como herramientas de integraci\u00f3n continua una herramienta que instalamos en nuestros propios servidores de integraci\u00f3n ( CI server ) o construcci\u00f3n continua ( continuous build server ) como Jenkins o tambi\u00e9n en un servicio en la nube como GitHub Actions . Cualquiera de estas herramientas permiten automatizar el lanzamiento de tests y la compilaci\u00f3n autom\u00e1tica de la aplicaci\u00f3n y la generaci\u00f3n de una aplicaci\u00f3n distribuible. Esta aplicaci\u00f3n puede ser un binario, un JAR o WAR, una m\u00e1quina Docker, etc. que puede ser desplegada en distintos entornos, incluido el de producci\u00f3n. El servicio de integraci\u00f3n continua genera tambi\u00e9n notificaciones autom\u00e1ticas a todos los miembros del equipo indicando el estado de la compilaci\u00f3n. Tambi\u00e9n suele proporcionar un panel de control con la indicaci\u00f3n del estado de cada build.","title":"Herramientas de integraci\u00f3n continua"},{"location":"cicd/#generacion-del-ejecutable","text":"El resultado de la compilaci\u00f3n autom\u00e1tica realizada por el servidor de integraci\u00f3n continua debe ser un artefacto desplegable en los distintos entornos en los que vamos a probar la aplicaci\u00f3n. La aplicaci\u00f3n s\u00f3lo se debe compilar una \u00fanica vez y el resultado debe almacenarse en un sitio accesible por cualquier proceso y miembro del equipo. Es fundamental que las herramientas usadas para construir la aplicaci\u00f3n puedan ser usadas desde l\u00ednea de comando. De esta forma es mucho m\u00e1s sencillo adaptar y configurar distintos tipos de scripts de compilaci\u00f3n ( build scripts ). Entre las herramientas m\u00e1s usadas destacamos las siguientes: Make (C, Unix) Rake (Ruby) Maven (Java) Gradle (Java, Scala, etc.) sbt (Scala, Play Framework) Igual que hay distintas herramientas de compilaci\u00f3n para los diferentes lenguajes de programaci\u00f3n, existen diferentes formatos en los que se guardan los artefactos binarios resultantes de la compilaci\u00f3n. Por ejemplo, el binario resultante de una aplicaci\u00f3n C es un fichero compilado que se ejecutar\u00e1 en el sistema operativo para el que haya sido compilada la aplicaci\u00f3n, mientras que el resultante de una aplicaci\u00f3n Java es un fichero JAR que podremos desplegar en cualquier m\u00e1quina en la que tengamos instalado un JRE ( Java Runtime Environment ). Adem\u00e1s tenemos el problema a\u00f1adido de generaci\u00f3n de distintos binarios para diferentes sistemas operativos. Por ejemplo, si estamos desarrollando una aplicaci\u00f3n de escritorio que va a funcionar en Windows, Linux y Mac deberemos generar los binarios correspondientes a esas distintas plataformas y despu\u00e9s testearlos de forma autom\u00e1tica en distintos ordenadores cada uno con su sistema operativo espec\u00edfico. En la actualidad se est\u00e1 haciendo cada vez m\u00e1s popular la utilizaci\u00f3n de im\u00e1genes Docker como artefacto binario a distribuir y ejecutar. Entre las ventajas de este enfoque se encuentran el ser multiplaforma (para ejecutarlas basta con tener instalado el Docker Engine ) y que los contenedores (servicios en ejecuci\u00f3n) se pueden configurar y combinar o ejecutar en clusters usando herramientas como Kubernetes . Es una buena pr\u00e1ctica darle a cada artefacto binario compilado un nombre distinto en el que aparezca el n\u00famero de versi\u00f3n. En el caso de la integraci\u00f3n continua, normalmente se le da al binario un nombre en el que aparece la fecha e incluso la hora de la compilaci\u00f3n. De esta forma, las distintas compilaciones pueden ser identificadas de forma \u00fanica. En el caso en que nuestra aplicaci\u00f3n dependa de paquetes externos es conveniente descargarlos y almacenarlos en un sitio centralizado de forma que no tengan que descargarse de Internet cada vez que se realiza una nueva compilaci\u00f3n. Para ello es conveniente configurar correctamente las cach\u00e9s del sistema de build que estemos utilizando. La aplicaci\u00f3n desplegable debe consistir en un \u00fanico artefacto con el nombre correcto que contenga todo lo necesario para ejecutarse en distintos entornos de prueba y pueda ser puesto en producci\u00f3n. El artefacto debe almacenarse en un lugar centralizado, accesible desde los distintos entornos de forma autom\u00e1tica. Por ejemplo, podemos usar un servidor web local y dejar el fichero en una URL concreta.","title":"Generaci\u00f3n del ejecutable"},{"location":"cicd/#principios-y-practicas-de-integracion-continua","text":"A continuaci\u00f3n presentamos en forma de \u00edtems un resumen de los elementos importantes de la integraci\u00f3n continua que hemos visto hasta ahora. Desarrollo de c\u00f3digo El sistema debe siempre poder ser construido (build) y probado con \u00e9xito. Todo el mundo hace merge de los cambios con frecuencia. Despu\u00e9s de cada commit, el sistema se integra inmediata y autom\u00e1ticamente. Se desarrolla el sistema en peque\u00f1os incrementos. Testing Los desarrolladores prueban su c\u00f3digo en sus espacios de trabajo privados. Despu\u00e9s mezclan los cambios en el repositorio. Servidor de integraci\u00f3n continua (CI server): Monitoriza el repositorio y comprueba los cambios cuando ocurren. Construye el sistema y ejecuta las pruebas unitarias y de integraci\u00f3n. Informa al equipo de la construcci\u00f3n con \u00e9xito o de los fallos. Errores en los build El equipo arregla el problema lo antes posible. Continuar para integrar y probar continuamente durante todo el proyecto. El \u00faltimo ejecutable compilado debe estar f\u00e1cilmente disponible El resultado de la compilaci\u00f3n debe ser un artefacto ejecutable disponible para desplegar en distintos entornos, incluso en producci\u00f3n.","title":"Principios y pr\u00e1cticas de integraci\u00f3n continua"},{"location":"cicd/#configuracion-del-despliegue","text":"Para hacer integraci\u00f3n continua es necesario tener m\u00faltiples entornos. En el entorno de trabajo del desarrollador se pueden lanzar los tests m\u00e1s r\u00e1pidos y utilizar una configuraci\u00f3n en la que usemos una base de datos en memoria (como H2) que acelere la velocidad de los tests que utilicen base de datos. Se puede configurar un primer entorno de integraci\u00f3n continua para que se ejecuten en \u00e9l tests m\u00e1s lentos y con una configuraci\u00f3n m\u00e1s parecida a la del entorno de producci\u00f3n. Por ejemplo, se puede utilizar una configuraci\u00f3n en la que se utilice una base de datos similar a la que se usa en producci\u00f3n, poblada con datos de prueba. En este entorno se deben lanzar los tests r\u00e1pidos y tambi\u00e9n tests m\u00e1s lentos de integraci\u00f3n. Y despu\u00e9s tendremos otros entornos cada vez m\u00e1s similares al entorno de producci\u00f3n en los que tambi\u00e9n se realizar\u00e1n todos los tests autom\u00e1ticos y otros tests manuales necesarios para tener la confirmaci\u00f3n de que todo est\u00e1 funcionando correctamente. En todos estos entornos deberemos instalar la misma aplicaci\u00f3n compilada, y tendremos que modificar en cada caso su configuraci\u00f3n. Es importante tener la capacidad de automatizar tanto la gesti\u00f3n de entornos de despliegue (arrancar las bases de datos correctas, configurar puertos, etc.) como la instalaci\u00f3n y ejecuci\u00f3n de la aplicaci\u00f3n en el entorno. Antiguamente los distintos entornos eran m\u00e1quinas f\u00edsicas distintas, con configuraciones distintas previamente instaladas en cada una de ellas. Hoy en d\u00eda es mucho m\u00e1s com\u00fan utilizar entornos virtuales f\u00e1cilmente construibles a partir de scripts y c\u00f3digo usando herramientas de virtualizaci\u00f3n como Docker, Kubernetes, etc. Tambi\u00e9n es habitual la utilizaci\u00f3n de entornos en la nube (Heroku, GitHub Actions, Amazon Web Services, etc.) tanto para prueba como para producci\u00f3n.","title":"Configuraci\u00f3n del despliegue"},{"location":"cicd/#entornos-de-despliegue","text":"Podemos diferenciar diferentes tipos de entornos (configuraci\u00f3n de servicios y servidores) en los que se despliega y prueba el build de la aplicaci\u00f3n. En general, ordenados de menor a mayor parecido a producci\u00f3n, podemos diferenciar. Local : ordenador del desarrollador. Se ejecutan tests unitarios de la caracter\u00edstica que se est\u00e1 desarrollando. Desarrollo/Trunk/Master : ordenador de integraci\u00f3n continua conectado a la rama de desarrollo en el que se ejecutan todos los tests unitarios continuamente. Integraci\u00f3n : Entorno en el que se sustituyen los mocks y bases de datos de memoria por servicios reales, aunque con copias parciales de los datos de producci\u00f3n. Test/QA : Entornos en los que se realizan pruebas funcionales, de interfaz de usuario, de performance o de seguridad, entre otros. Pueden ser manuales y/o automatizados. Stage/Preproducci\u00f3n : Entorno id\u00e9ntico al de producci\u00f3n en el que se hace la \u00faltima validaci\u00f3n de la nueva versi\u00f3n a desplegar a producci\u00f3n. Copia de la base de datos de producci\u00f3n y con servidores similares a los de producci\u00f3n, para poder comprobar rendimiento. Producci\u00f3n : Entorno que usan los clientes reales de la aplicaci\u00f3n.","title":"Entornos de despliegue"},{"location":"cicd/#configuracion-de-la-aplicacion","text":"La aplicaci\u00f3n debe poder funcionar en distintos entornos sin tener que ser recompilada. Para ello es necesario poder configurar su funcionamiento definiendo par\u00e1metros que podamos modificar previamente a su ejecuci\u00f3n sin tener que recompilarla. Existen multitud de elementos que podemos necesitar configurar dependiendo del entorno en que queremos que funcione la aplicaci\u00f3n. Por ejemplo: URL de conexi\u00f3n a la base de datos Usuario y contrase\u00f1a de conexi\u00f3n a la base de datos Puerto en el que la aplicaci\u00f3n va a recibir las peticiones Direcciones de los servicios a los que debe conectarse (por ejemplo, servicio SMTP de correo electr\u00f3nico) En cada entorno en los que va a funcionar la aplicaci\u00f3n estos par\u00e1metros van a tener unos valores distintos que hay que pasarle a la aplicaci\u00f3n cuando se ponga en funcionamiento. Existen diversas formas de definir estas propiedades. Las m\u00e1s usuales son: Mediante ficheros de configuraci\u00f3n de la aplicaci\u00f3n. Mediante variables de entorno cuyos valores establecemos con scripts antes de lanzar la aplicaci\u00f3n. Mediante argumentos del comando que lanza la aplicaci\u00f3n Estas distintas configuraciones deben estar tambi\u00e9n guardadas en el control de versiones, igual que el c\u00f3digo de la aplicaci\u00f3n, para poder tambi\u00e9n controlar su evoluci\u00f3n y sus cambios.","title":"Configuraci\u00f3n de la aplicaci\u00f3n"},{"location":"cicd/#configuracion-de-imagenes-docker","text":"Docker tambi\u00e9n tiene muchas estrategias que permiten configurar la ejecuci\u00f3n de una imagen. Una de las m\u00e1s utilizadas es hacer que la imagen utilice variables de entorno que pueden ser modificadas en al lanzar el contenedor. Para ello debemos definir en la imagen estas variables con el comando ENV , pudiendo dar valores por defecto. Por ejemplo, podemos definir el siguiente Dockerfile : FROM alpine ENV saludo=\"Hola, mundo!\" CMD echo $saludo Creamos la imagen: $ docker build -t saludo . Si lanzamos la imagen muestra el saludo por defecto: $ docker run saludo Hola, mundo! Y podemos configurar el saludo de varias formas. Por ejemplo, indicando el valor del par\u00e1metro al hacer run : $ docker run -e \"saludo=\u00bfQu\u00e9 tal est\u00e1s?\" saludo \u00bfQu\u00e9 tal est\u00e1s? O guardando el valor del par\u00e1metro en un fichero de propiedades que pasamos al ejecutar la imagen. Por ejemplo, en el fichero propiedades.txt escribimos lo siguiente: saludo=\u00bfQu\u00e9 passsa, colega? Y ejecutamos la imagen de la siguiente forma: $ docker run --env-file=propiedades.txt saludo \u00bfQu\u00e9 passsa, colega?","title":"Configuraci\u00f3n de im\u00e1genes Docker"},{"location":"cicd/#entrega-continua","text":"El concepto de Entrega continua ( Continuous Delivery ) es una extensi\u00f3n de la Integraci\u00f3n continua que se populariz\u00f3 a ra\u00edz del libro que publicaron en 2010 Jez Humble y David Farley (ver las referencias). Es un concepto que parte de la integraci\u00f3n continua para llegar a una automatizaci\u00f3n completa de la puesta en producci\u00f3n. El objetivo es conseguir una puesta en producci\u00f3n (release) del software: Poco arriesgada Frecuente Barata R\u00e1pida Predecible Reproducible En palabras de Jez Humble, la entrega continua consiste en: \u201cReduce the cost, time, and risk of delivering incremental changes to users\u201d Jez Humble (2013), Charla Adopting Continuous Delivery Otra frase muy importante, que ya hemos comentado alguna vez: \u201cHow long would it take your organization to deploy a change that involved just one single line of code? Do you do this on a repeatable, reliable basis?\u201d Mary Poppendieck Algunas t\u00e9cnicas que se utilizan en la Entrega continua (muchas de ellas ya las hemos visto): Peque\u00f1os cambios que se despliegan continuamente Todos los builds son candidatos al release Todo en el control de versiones (se debe poder probar cualquier release) Pipelines de despliegue (deployment pipelines) Integraci\u00f3n continua: automatizaci\u00f3n de builds, tests, despliegues, entornos","title":"Entrega continua"},{"location":"cicd/#pipeline-de-despliegue","text":"Tal y como hemos comentado cuando habl\u00e1bamos de integraci\u00f3n continua, un elemento central es de la automatizaci\u00f3n es el pipeline de despliegue. un pipeline de despliegue es una implementaci\u00f3n automatizada del proceso de construcci\u00f3n, despliegue, prueba y lanzamiento de nuestro sistema. La utilizaci\u00f3n de un pipeline de despliegue garantiza la visibilidad de todo el proceso, lo que garantiza un feedback temprano y un control continuo del mismo. Explicaci\u00f3n del libro de Jeff Humble: Cada cambio que se realiza sobre la configuraci\u00f3n de la aplicaci\u00f3n, su c\u00f3digo fuente o sus datos, lanza la creaci\u00f3n de una nueva instancia de el pipeline. Uno de los primeros pasos en el pipeline es crear los binarios y los instaladores. El resto de el pipeline ejecuta una serie de tests sobre los binarios para probar que pueden ser lanzados. Cada test que pasa el candidato a release nos da m\u00e1s confianza de que funcionar\u00e1 correctamente esta combinaci\u00f3n particular de c\u00f3digo binario, informaci\u00f3n de configuraci\u00f3n, entorno y datos. Si el candidato a release pasa todos los tests, puede ser lanzado. En el libro de Humble y Farley se muestra el siguiente esquema que representa sus distintos elementos. En la parte superior se muestra el sistema de control de versiones, en donde se almacena el c\u00f3digo del proyecto y los datos de las distintas configuraciones de los entornos y de la aplicaci\u00f3n. Las configuraciones de los entornos y de la aplicaci\u00f3n se deben guardar en el sistema de control de versiones para gestionar su evoluci\u00f3n y modificaci\u00f3n de la misma forma que gestionamos la evoluci\u00f3n del c\u00f3digo. En la parte inferior se muestra el repositorio de artefactos en donde se almacenan los binarios de la aplicaci\u00f3n. Puede ser, por ejemplo, Docker Hub en el caso de ser una aplicaci\u00f3n dockerizada. En la fase de commit el c\u00f3digo se compila y se lanzan los tests unitarios. Se generan los binarios que se almacenan en el repositorio de artefactos. En la fase de aceptaci\u00f3n se configuran y despliegan los binarios en un entorno similar al de producci\u00f3n. Se realizan test de aceptaci\u00f3n/integraci\u00f3n y se valida la aplicaci\u00f3n y se deja lista para ser publicada a producci\u00f3n por parte de Operaciones. En la fase de UAT (User Acceptance Testing) se realizan pruebas manuales en un entorno lo m\u00e1s parecido posible al de producci\u00f3n. En la fase capacidad se realizan tests de rendimiento. El binario se despliega en producci\u00f3n si todas las fases anteriores se pasan con \u00e9xito. En la siguiente figura se muestra un ejemplo de posible secuencia de despliegue: Son muy \u00fatiles los tableros de control de el pipeline de despliegue, como por ejemplo el que proporciona Jenkins.","title":"Pipeline de despliegue"},{"location":"cicd/#pequenos-cambios","text":"Tal y como hemos comentado en la introducci\u00f3n del tema, una pr\u00e1ctica fundamental de los equipos que utilizan las t\u00e9cnicas de entrega continua es subir a producci\u00f3n continuamente peque\u00f1os cambios con los que se van introduciendo poco a poco las nuevas funcionalidades. Por ejemplo, en la charla de Juan Ignacio S\u00e1nchez Continuous Integration at CartoDB ( v\u00eddeo ) se explica c\u00f3mo es el proceso de integraci\u00f3n continua en Carto , una importante empresa espa\u00f1ola de gesti\u00f3n de datos geogr\u00e1ficos. Algunas de las m\u00e9tricas que muestra en la charla reflejan claramente cu\u00e1l es el funcionamiento de la integraci\u00f3n continua en la empresa. Su producto opensource m\u00e1s importante es CartoDB (enlace a su repositorio GitHub ). En la \u00e9poca de la charla el producto integraba una media de 22 pull requests semanales y 15 despliegues en producci\u00f3n. Si todos los cambios son peque\u00f1os, \u00bfc\u00f3mo se introducen los cambios grandes en el proyecto?. Por ejemplo, nuevas caracter\u00edsticas complejas en las que se necesitan combinar distintas funcionalidades elementales. Es posible ir desarrollando, probando y colocando las piezas en el c\u00f3digo (sin mostrar en la interfaz de usuario) para que el sistema evolucione hacia un momento futuro en sea f\u00e1cil introducir una caracter\u00edstica totalmente nueva mediante un peque\u00f1o cambio. Como dice Kent Beck: Make the change easy, then make the easy change. Para ello podemos usar las siguientes estrategias: Codificaci\u00f3n y prueba de las peque\u00f1as funcionalidades por separado. Buen dise\u00f1o de c\u00f3digo, por ejemplo seleccionar una implementaci\u00f3n concreta utilizando interfaces y factor\u00edas, pero dejar la estructura lista para introducir futuros cambios. Peque\u00f1os cambios en las APIs compatibles con los tests de regresi\u00f3n. Uso de mocks. Interruptores de caracter\u00edsticas. Esta \u00faltima t\u00e9cnica es muy interesante. Consiste en definir interruptores o flags booleanos en el c\u00f3digo que hagan que ciertas caracter\u00edsticas se muestren o no en la aplicaci\u00f3n dependiendo de si los flags est\u00e1n o no activos. En este art\u00edculo se puede encontrar una explicaci\u00f3n en profundidad de m\u00faltiples t\u00e9cnicas usadas para implementar los interruptores de caracter\u00edsticas. Dependiendo de la t\u00e9cnica es posible hasta definir interruptores que se puedan modificar en tiempo de ejecuci\u00f3n e incluso que se puedan mostrar o no la funcionalidad a seg\u00fan qu\u00e9 usuarios implementando un sistema de canary release .","title":"Peque\u00f1os cambios"},{"location":"cicd/#canary-release","text":"La idea del canary release consiste en configurar un sistema de despliegue que permita mantener simult\u00e1neamente en producci\u00f3n dos versiones de la aplicaci\u00f3n. En el caso de una aplicaci\u00f3n web, podr\u00edamos configurar un proxy o router intermedio que se encargue de encauzar las peticiones de los usuarios a una versi\u00f3n de la aplicaci\u00f3n o a otra. Cuando se lanza una caracter\u00edstica nueva se puede configurar el proxy para que s\u00f3lo sea probada por una peque\u00f1a cantidad de usuarios y detectar posibles errores en este despliegue reducido. Cuando se haya comprobado con este peque\u00f1o grupo que todo funciona correctamente se modifica la configuraci\u00f3n del proxy para que todos accedan a la nueva versi\u00f3n. La configuraci\u00f3n del proxy puede llegar a ser bastante compleja, haciendo el filtro de usuarios en funci\u00f3n de par\u00e1metros que nos interesen (localizaci\u00f3n, tipo de usuario, etc.). Este sistema tambi\u00e9n puede utilizarse, junto con el de interruptores de caracter\u00edsticas, para realizar pruebas A/B de nuevas caracter\u00edsticas.","title":"Canary release"},{"location":"cicd/#devops","text":"Tradicionalmente el trabajo de los desarrolladores y el de los t\u00e9cnicos de operaciones (responsables de la puesta en producci\u00f3n del sistema, monitorizaci\u00f3n de servidores, etc. ) son contrapuestos. Dev elopers: Su trabajo es a\u00f1adir nuevas caracter\u00edsticas. Trabajan en entornos locales (\u201cen mim\u00e1quina funciona\u201d). Utilizan herramientas y lenguajes que permiten abstraer y automatizar. Op erations: Su trabajo es mantener el sitio web seguro, estable y r\u00e1pido. Detectar problemas, apagar fuegos. Los desarrolladores quieren introducir cambios r\u00e1pidamente en el sistema, mientras que a los t\u00e9cnicos de operaciones les gustar\u00eda mantener el sistema lo m\u00e1s estable posible. Los profesionales DevOps representan una nueva filosof\u00eda de trabajo, que combina elementos propios de los desarrolladores y de operaciones, incorporando todas las nuevas t\u00e9cnicas de las que hemos estado hablando en este tema y herramientas denominadas Infrastructure-as-code como Docker, Kubernetes, Ansible, Terraform,etc. en las que podemos definir la configuraci\u00f3n de entornos y servidores usando c\u00f3digo y ficheros almacenables en un sistema de control de versiones, en lugar de tener que configurar f\u00edsicamente el hardware.","title":"DevOps"},{"location":"cicd/#principos-y-buenas-practicas","text":"En la charla mencionada anteriormente sobre pr\u00e1cticas de integraci\u00f3n continua en Carto, Juan Ignacio S\u00e1nchez lista 10 buenas pr\u00e1cticas que ellos est\u00e1n siguiendo. Muchas ya las hemos visto, pero es interesante repasarlas todas juntas, validadas por la experiencia de su \u00e9xito en una empresa puntera de desarrollo de software. Son las siguientes: Mantener un repositorio de c\u00f3digo Automatizar la compilaci\u00f3n Hacer la compilaci\u00f3n auto-testeable (mediante test autom\u00e1ticos) Todo el mundo realiza commits en la rama principal todos los d\u00edas Cada commit en la rama principal debe ser compilado Mantener la compilaci\u00f3n r\u00e1pida Testear en un clon del entorno de producci\u00f3n Hacer f\u00e1cil de obtener los \u00faltimos productos compilados Todo el mundo puede ver los resultados de las \u00faltimas compilaciones Automatizar el despliegue","title":"Principos y buenas pr\u00e1cticas"},{"location":"cicd/#consejos-adicionales","text":"La integraci\u00f3n continua y el despliegue continuo son dos conceptos que pueden convertirse en aliados muy poderosos para nuestro equipo si ponemos los medios necesarios. Es obvio que nada de esto es posible sin una bater\u00eda de pruebas s\u00f3lidas, extensas y consistentes. Si no disponemos de pruebas unitarias de calidad y que cubran la totalidad de nuestro c\u00f3digo, lo m\u00e1s probable es que nuestro CI nos de muchos falsos positivos (fallos que no han sido detectados por las pruebas unitarias), que a su vez provocar\u00e1 que se desplieguen fallos en nuestros entornos de pre-producci\u00f3n y producci\u00f3n. Por ejemplo, si el proceso de integraci\u00f3n continua se ejecuta en nuestro servidor de Gitlab en vez de en el ordenador de cada desarrollador de nuestro equipo, estaremos facilitando la tarea de comprobaci\u00f3n automatizada de error. A\u00fan m\u00e1s importante, si dicho proceso se ejecuta en pocos minutos, estaremos creando un h\u00e1bito entre las personas del proyecto de enviar m\u00e1s a menudo sus cambios al repositorio, ya que hacerlo les permitir\u00e1 obtener una visi\u00f3n r\u00e1pida sobre la calidad de su trabajo. Esto a su vez har\u00e1 que la frecuencia de los cambios que se env\u00edan al repositorio sea m\u00e1s alta, mientras que el tama\u00f1o de cada cambio enviado sea m\u00e1s peque\u00f1o. Las tareas de revisi\u00f3n se har\u00e1n m\u00e1s amenas y f\u00e1ciles, y por lo tanto, mejora la velocidad con la que se integran dichos cambios en \u00abdevelop\u00bb. Nuestro equipo de control de calidad podr\u00e1 tener un flujo continuo de trabajo, lo cual reducir\u00e1 los tiempos \u00abmuertos\u00bb (personas \u00abbloqueadas\u00bb a la espera de que otras personas terminen su trabajo). Por \u00faltimo, cabe decir que no es obligatorio que el despliegue continuo tenga lugar en cuanto un cambio se mezcle en las ramas de \u00abdevelop\u00bb o \u00abmaster\u00bb. Existen muchos casos en los que el l\u00edder del equipo (o una persona con rol similar) deber\u00e1 tomar la decisi\u00f3n de dar luz verde al proceso de despliegue en base a m\u00e1s factores. Es decir, el proceso en s\u00ed puede estar automatizado (es lo mas recomendable), pero la acci\u00f3n que desencadena el despliegue puede ser humana.","title":"Consejos adicionales"},{"location":"cicd/#referencias","text":"Charla de Eduardo Ferro (2020): Continuous Delivery: Germinando una cultura \u00e1gil moderna . Martin Fowler (2006): Continuous Integration Jez Humble y David Farley (2010): Continuous Delivery Integraci\u00f3n continua y despliegue continuo","title":"Referencias"},{"location":"commands/","text":"Comandos de git Esta secci\u00f3n describe algunos de los comandos m\u00e1s interesantes de git Git stash (reserva) La orden git stash nos permite salvar moment\u00e1neamente el espacio de trabajo cuando tenemos que cambiar de rama o preparar la rama actual para sincronizar cambios. Las operaciones m\u00e1s importantes que podemos hacer con git stash son: git stash save Es equivalente a poner solo git stash pero nos permite realizar m\u00e1s acciones como: git stash save \"Tu mensaje\" git stash save -u El par\u00e1metro -u permite que se almacen tambi\u00e9n los ficheros sin seguimiento previo ( untracked en ingl\u00e9s, aquellos ficheros que no se han metido nunca en el repositorio). git stash list Permite mostrar la pila del stash. $ git stash list stash@{0}: On master: Stash con mensaje stash@{1}: WIP on master: 4ab21df First commit git stash apply Esta orden coge el stash que est\u00e1 arriba en la pila y lo aplica al espacio de trabajo actual. En este caso siempre es stash@{0} . El stash permanece en la pila. Se puede indicar como par\u00e1metro un stash en concreto. git stash pop Funciona igual que git apply con la diferencia de que el stash s\u00ed se borra de la pila. git stash show Muestra un resumen de los ficheros que se han modificado en ese stash. $ git stash show A.txt | 1 + B.txt | 3 +++ 2 file changed, 4 insertions(+) Para ver los cambios podemos usar el par\u00e1metro -p $ git stash show -p --- a/A.txt +++ b/A.txt @@ -45,6 +45,7 @@ nav: + This is a change Por defecto siempre muestra la cabeza de la pila. Igual que en casos anteriores podemos indicar un stash en concreto. $ git stash show stash@{1} git stash branch Permite crear una nueva rama a partir del \u00faltimo stash. Adem\u00e1s, el mismo es borrado de la pila. Se puede especificar uno en concreto si lo queremos, como en el resto de comandos. git stash branch nombre-de-nueva-rama stash@{1} git stash clear Este comando borrar todos los stash de la pila. Es destructiva y no se puede deshacer. git stash drop Permite borrar un stash en concreto (o el \u00faltimo si no se indica ninguno). Como con clear, borrarlo implica que no se puede recuperar. Git worktree Uno de los problemas m\u00e1s habituales es tener que tocar una rama distinta a la que tenemos actualmente. Eso implica que si estamos en medio de un trabajo tendr\u00edamos que hacer un commit o un stash, lo cual a veces es bastante molesto. Con git worktree podemos crear un directorio de trabajo que contenga otra rama distinta, de forma temporal. No supone otro clon del repositorio porque ambos usan el mismo. git worktree add Esta funci\u00f3n es la que crea el espacio de trabajo temporal. Imaginemos que estamos en una rama llamada develop : $ git worktree add ../project-master master $ git worktree add -b fix ../project-fix master La primera orden crea un directorio llamado project-master que contiene el estado de master. La segunda, que contiene el par\u00e1metro -b equivale a crear una nueva rama llamada fix, que se crea desde master (suponemos que no existe fix). git worktree list Muestra el listado de directorios y espacios de trabajo. $git worktree list /home/sergio/taller-de-git 3b63b4b [master] /home/sergio/fix 3b63b4b [fix] git worktree remove Borrar un espacio de trabajo. Hay que indicar el nombre entre corchetes que aparece en el listado $ git worktree delete fix git worktree prune Una cuesti\u00f3n importante, es que las ramas que est\u00e9n desplegadas en otro espacio de trabajo, se encuentran bloqueadas y no se pueden desbloquear en otro distinto. Esto significa que si estamos trabajando en la rama developer, creamos otro worktree en otro directorio de la rama master, no podemos hacer pasar a master. No es posible tener la misma rama en varios espacios de trabajo. Si se ha borrado el directorio a mano (en vez de usando remove), eso no implica que el bloqueo desparezca. Con esta orden podemos hacer que git compruebe que los espacios de trabajo secundario se comprueben de nuevo para ver si siguen existiendo y se elimine el bloqueo. Git blame Lo ideal en un equipo de desarrollo es que el c\u00f3digo pase por todas las manos para as\u00ed mejorar su calidad. Con git blame podemos saber qui\u00e9n fue el \u00faltimo en modificar una l\u00ednea concreta de c\u00f3digo, en qu\u00e9 commit y en qu\u00e9 fecha lo hizo. $ git blame ejemplo.php 33cdd02c (Sergio G\u00f3mez 2020-01-20 16:58:52 +0100 8) name: \"material\" 33cdd02c (Sergio G\u00f3mez 2020-01-20 16:58:52 +0100 9) language: \"es\"","title":"Comandos de git"},{"location":"commands/#comandos-de-git","text":"Esta secci\u00f3n describe algunos de los comandos m\u00e1s interesantes de git","title":"Comandos de git"},{"location":"commands/#git-stash-reserva","text":"La orden git stash nos permite salvar moment\u00e1neamente el espacio de trabajo cuando tenemos que cambiar de rama o preparar la rama actual para sincronizar cambios. Las operaciones m\u00e1s importantes que podemos hacer con git stash son:","title":"Git stash (reserva)"},{"location":"commands/#git-stash-save","text":"Es equivalente a poner solo git stash pero nos permite realizar m\u00e1s acciones como: git stash save \"Tu mensaje\" git stash save -u El par\u00e1metro -u permite que se almacen tambi\u00e9n los ficheros sin seguimiento previo ( untracked en ingl\u00e9s, aquellos ficheros que no se han metido nunca en el repositorio).","title":"git stash save"},{"location":"commands/#git-stash-list","text":"Permite mostrar la pila del stash. $ git stash list stash@{0}: On master: Stash con mensaje stash@{1}: WIP on master: 4ab21df First commit","title":"git stash list"},{"location":"commands/#git-stash-apply","text":"Esta orden coge el stash que est\u00e1 arriba en la pila y lo aplica al espacio de trabajo actual. En este caso siempre es stash@{0} . El stash permanece en la pila. Se puede indicar como par\u00e1metro un stash en concreto.","title":"git stash apply"},{"location":"commands/#git-stash-pop","text":"Funciona igual que git apply con la diferencia de que el stash s\u00ed se borra de la pila.","title":"git stash pop"},{"location":"commands/#git-stash-show","text":"Muestra un resumen de los ficheros que se han modificado en ese stash. $ git stash show A.txt | 1 + B.txt | 3 +++ 2 file changed, 4 insertions(+) Para ver los cambios podemos usar el par\u00e1metro -p $ git stash show -p --- a/A.txt +++ b/A.txt @@ -45,6 +45,7 @@ nav: + This is a change Por defecto siempre muestra la cabeza de la pila. Igual que en casos anteriores podemos indicar un stash en concreto. $ git stash show stash@{1}","title":"git stash show"},{"location":"commands/#git-stash-branch","text":"Permite crear una nueva rama a partir del \u00faltimo stash. Adem\u00e1s, el mismo es borrado de la pila. Se puede especificar uno en concreto si lo queremos, como en el resto de comandos. git stash branch nombre-de-nueva-rama stash@{1}","title":"git stash branch"},{"location":"commands/#git-stash-clear","text":"Este comando borrar todos los stash de la pila. Es destructiva y no se puede deshacer.","title":"git stash clear"},{"location":"commands/#git-stash-drop","text":"Permite borrar un stash en concreto (o el \u00faltimo si no se indica ninguno). Como con clear, borrarlo implica que no se puede recuperar.","title":"git stash drop"},{"location":"commands/#git-worktree","text":"Uno de los problemas m\u00e1s habituales es tener que tocar una rama distinta a la que tenemos actualmente. Eso implica que si estamos en medio de un trabajo tendr\u00edamos que hacer un commit o un stash, lo cual a veces es bastante molesto. Con git worktree podemos crear un directorio de trabajo que contenga otra rama distinta, de forma temporal. No supone otro clon del repositorio porque ambos usan el mismo.","title":"Git worktree"},{"location":"commands/#git-worktree-add","text":"Esta funci\u00f3n es la que crea el espacio de trabajo temporal. Imaginemos que estamos en una rama llamada develop : $ git worktree add ../project-master master $ git worktree add -b fix ../project-fix master La primera orden crea un directorio llamado project-master que contiene el estado de master. La segunda, que contiene el par\u00e1metro -b equivale a crear una nueva rama llamada fix, que se crea desde master (suponemos que no existe fix).","title":"git worktree add"},{"location":"commands/#git-worktree-list","text":"Muestra el listado de directorios y espacios de trabajo. $git worktree list /home/sergio/taller-de-git 3b63b4b [master] /home/sergio/fix 3b63b4b [fix]","title":"git worktree list"},{"location":"commands/#git-worktree-remove","text":"Borrar un espacio de trabajo. Hay que indicar el nombre entre corchetes que aparece en el listado $ git worktree delete fix","title":"git worktree remove"},{"location":"commands/#git-worktree-prune","text":"Una cuesti\u00f3n importante, es que las ramas que est\u00e9n desplegadas en otro espacio de trabajo, se encuentran bloqueadas y no se pueden desbloquear en otro distinto. Esto significa que si estamos trabajando en la rama developer, creamos otro worktree en otro directorio de la rama master, no podemos hacer pasar a master. No es posible tener la misma rama en varios espacios de trabajo. Si se ha borrado el directorio a mano (en vez de usando remove), eso no implica que el bloqueo desparezca. Con esta orden podemos hacer que git compruebe que los espacios de trabajo secundario se comprueben de nuevo para ver si siguen existiendo y se elimine el bloqueo.","title":"git worktree prune"},{"location":"commands/#git-blame","text":"Lo ideal en un equipo de desarrollo es que el c\u00f3digo pase por todas las manos para as\u00ed mejorar su calidad. Con git blame podemos saber qui\u00e9n fue el \u00faltimo en modificar una l\u00ednea concreta de c\u00f3digo, en qu\u00e9 commit y en qu\u00e9 fecha lo hizo. $ git blame ejemplo.php 33cdd02c (Sergio G\u00f3mez 2020-01-20 16:58:52 +0100 8) name: \"material\" 33cdd02c (Sergio G\u00f3mez 2020-01-20 16:58:52 +0100 9) language: \"es\"","title":"Git blame"},{"location":"containers/","text":"Contenedores Los contenedores son instancias de las im\u00e1genes que hemos creado o hemos descargado que se ejecutan de forma aislada. Listado La orden para ver el listado de contenedores del sistema es docker container ls o la forma abreviada docker ps . Si lo ejecutamos nos dar\u00e1 un listado vac\u00edo porque no hay ning\u00fan contenedor activo. Probemos con el par\u00e1metro --all o -a . $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4bd76e08b07f wordpress \"docker-\u2026\" 11 minutes ago Exited (0) peaceful_murdock 69a3c34c224d hello-world \"/hello\" 18 minutes ago Exited (0) blissful_goldwasser Estos contenedores est\u00e1n parados y se pueden volver a ejecutar, con el mismo estado que tuviera el sistema de archivos cuando se detuvieron. Ejecutar comandos dentro de un contenedor Ya hemos usado docker run para crear e iniciar un contenedor. Tambi\u00e9n podemos usar este comando para ejecutar programas que est\u00e9n dentro del contenedor. Por ejemplo: docker run --name ubuntu_bash --rm -i -t ubuntu bash Info Las primeras versiones de Docker eran m\u00e1s limitadas, respecto a la creaci\u00f3n de objetos. As\u00ed que sali\u00f3 con comandos como docker start , docker stop , etc. relacionados con los contenedores. Cuando surgieron m\u00e1s objetos no hab\u00eda consistencia entre los comandos de otros objetos (como docker volumes ls ) y los de los contenedores. As\u00ed que se ha creado una jerarqu\u00eda nueva de subcomandos bajo el comando container que son equivalentes y se mantienen por compatibilidad: Antiguo Nuevo docker run docker container run docker start docker container start docker stop docker container stop docker rm docker container rm docker inspect docker container inspect docker exec docker container exec No hay m\u00e1s diferencia entre ellos que el nombre. Pero esta forma de ejecutar cosas, crea un nuevo contenedor. Si queremos ejecutar un comando en un contenedor que ya est\u00e9 iniciado, debemos usar docker container exec . Ejecuta lo siguiente en otro terminal (no cierres el anterior). docker exec -w /tmp ubuntu_bash touch my_file.sh El par\u00e1metro -w indica el directorio de trabajo, despu\u00e9s indicamos el contenedor donde queremos ejecutar el comando ( ubuntu_bash ) y por \u00faltimo el comando a ejecutar ( touch my_file.sh ). Si en el primer terminal ejecutamos un listado del directorio tmp: # ls /tmp my_file.sh Vemos como podemos modificar un contenedor ya iniciado con docker container exec . Pulsa Control+C en el primer terminal para cerrar y borrar el contenedor. Iniciar un contenedor Con docker container start podemos iniciar un contenedor parado: $ docker container start peaceful_murdock peaceful_murdock $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4bd76e08b07f wordpress \"docker\u2026\" 14 minutes ago Up 0.0.0.0:8080->80/tcp peaceful_murdock Veremos que la web de instalaci\u00f3n de WordPress est\u00e1 de nuevo disponible. Solo que ahora el contenedor se ejecuta en segundo plano y no lo podemos detener como antes. Detener un contenedor Con docker container stop podemos detener un contenedor iniciado, indicando su id o su nombre $ docker container stop 4bd76e08b07f 4bd76e08b07f Tip Podemos hacer referencia a los contenedores por su ID o por su nombre. Borrar un contenedor Un contenedor detenido ocupa espacio. Si hemos dejado de necesitar un contenedor podemos borrarlo con docker container rm . Igualmente hay que indicar id o nombre. $ docker container rm 4bd76e08b07f 4bd76e08b07f Danger Hay que tener cuidado al borrar contenedores. Cuando un contenedor se borra se elimina cualquier informaci\u00f3n que contenga y no est\u00e9 almacenada en alg\u00fan lugar externo al propio contenedor.","title":"Contenedores"},{"location":"containers/#contenedores","text":"Los contenedores son instancias de las im\u00e1genes que hemos creado o hemos descargado que se ejecutan de forma aislada.","title":"Contenedores"},{"location":"containers/#listado","text":"La orden para ver el listado de contenedores del sistema es docker container ls o la forma abreviada docker ps . Si lo ejecutamos nos dar\u00e1 un listado vac\u00edo porque no hay ning\u00fan contenedor activo. Probemos con el par\u00e1metro --all o -a . $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4bd76e08b07f wordpress \"docker-\u2026\" 11 minutes ago Exited (0) peaceful_murdock 69a3c34c224d hello-world \"/hello\" 18 minutes ago Exited (0) blissful_goldwasser Estos contenedores est\u00e1n parados y se pueden volver a ejecutar, con el mismo estado que tuviera el sistema de archivos cuando se detuvieron.","title":"Listado"},{"location":"containers/#ejecutar-comandos-dentro-de-un-contenedor","text":"Ya hemos usado docker run para crear e iniciar un contenedor. Tambi\u00e9n podemos usar este comando para ejecutar programas que est\u00e9n dentro del contenedor. Por ejemplo: docker run --name ubuntu_bash --rm -i -t ubuntu bash Info Las primeras versiones de Docker eran m\u00e1s limitadas, respecto a la creaci\u00f3n de objetos. As\u00ed que sali\u00f3 con comandos como docker start , docker stop , etc. relacionados con los contenedores. Cuando surgieron m\u00e1s objetos no hab\u00eda consistencia entre los comandos de otros objetos (como docker volumes ls ) y los de los contenedores. As\u00ed que se ha creado una jerarqu\u00eda nueva de subcomandos bajo el comando container que son equivalentes y se mantienen por compatibilidad: Antiguo Nuevo docker run docker container run docker start docker container start docker stop docker container stop docker rm docker container rm docker inspect docker container inspect docker exec docker container exec No hay m\u00e1s diferencia entre ellos que el nombre. Pero esta forma de ejecutar cosas, crea un nuevo contenedor. Si queremos ejecutar un comando en un contenedor que ya est\u00e9 iniciado, debemos usar docker container exec . Ejecuta lo siguiente en otro terminal (no cierres el anterior). docker exec -w /tmp ubuntu_bash touch my_file.sh El par\u00e1metro -w indica el directorio de trabajo, despu\u00e9s indicamos el contenedor donde queremos ejecutar el comando ( ubuntu_bash ) y por \u00faltimo el comando a ejecutar ( touch my_file.sh ). Si en el primer terminal ejecutamos un listado del directorio tmp: # ls /tmp my_file.sh Vemos como podemos modificar un contenedor ya iniciado con docker container exec . Pulsa Control+C en el primer terminal para cerrar y borrar el contenedor.","title":"Ejecutar comandos dentro de un contenedor"},{"location":"containers/#iniciar-un-contenedor","text":"Con docker container start podemos iniciar un contenedor parado: $ docker container start peaceful_murdock peaceful_murdock $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4bd76e08b07f wordpress \"docker\u2026\" 14 minutes ago Up 0.0.0.0:8080->80/tcp peaceful_murdock Veremos que la web de instalaci\u00f3n de WordPress est\u00e1 de nuevo disponible. Solo que ahora el contenedor se ejecuta en segundo plano y no lo podemos detener como antes.","title":"Iniciar un contenedor"},{"location":"containers/#detener-un-contenedor","text":"Con docker container stop podemos detener un contenedor iniciado, indicando su id o su nombre $ docker container stop 4bd76e08b07f 4bd76e08b07f Tip Podemos hacer referencia a los contenedores por su ID o por su nombre.","title":"Detener un contenedor"},{"location":"containers/#borrar-un-contenedor","text":"Un contenedor detenido ocupa espacio. Si hemos dejado de necesitar un contenedor podemos borrarlo con docker container rm . Igualmente hay que indicar id o nombre. $ docker container rm 4bd76e08b07f 4bd76e08b07f Danger Hay que tener cuidado al borrar contenedores. Cuando un contenedor se borra se elimina cualquier informaci\u00f3n que contenga y no est\u00e9 almacenada en alg\u00fan lugar externo al propio contenedor.","title":"Borrar un contenedor"},{"location":"cvs/","text":"Sistemas de control de versiones Definici\u00f3n, clasificaci\u00f3n y funcionamiento Se llama control de versiones a la gesti\u00f3n de los diversos cambios que se realizan sobre los elementos de alg\u00fan producto o una configuraci\u00f3n del mismo. Una versi\u00f3n, revisi\u00f3n o edici\u00f3n de un producto, es el estado en el que se encuentra dicho producto en un momento dado de su desarrollo o modificaci\u00f3n. Aunque un sistema de control de versiones puede realizarse de forma manual, es muy aconsejable disponer de herramientas que faciliten esta gesti\u00f3n dando lugar a los llamados sistemas de control de versiones o SVC (del ingl\u00e9s System Version Control). Estos sistemas facilitan la administraci\u00f3n de las distintas versiones de cada producto desarrollado, as\u00ed como las posibles especializaciones realizadas (por ejemplo, para alg\u00fan cliente espec\u00edfico). Ejemplos de este tipo de herramientas son entre otros: CVS, Subversion, SourceSafe, ClearCase, Darcs, Bazaar , Plastic SCM, Git, Mercurial, Perforce. Terminolog\u00eda Repositorio (\"repository\") : El repositorio es el lugar en el que se almacenan los datos actualizados e hist\u00f3ricos de cambios. Revisi\u00f3n (\"revision\") : Una revisi\u00f3n es una versi\u00f3n determinada de la informaci\u00f3n que se gestiona. Hay sistemas que identifican las revisiones con un contador (Ej. subversion). Hay otros sistemas que identifican las revisiones mediante un c\u00f3digo de detecci\u00f3n de modificaciones (Ej. git usa SHA1). Etiqueta (\"tag\") : Los tags permiten identificar de forma f\u00e1cil revisiones importantes en el proyecto. Por ejemplo se suelen usar tags para identificar el contenido de las versiones publicadas del proyecto. Rama (\"branch\") : Un conjunto de archivos puede ser ramificado o bifurcado en un punto en el tiempo de manera que, a partir de ese momento, dos copias de esos archivos se pueden desarrollar a velocidades diferentes o en formas diferentes de forma independiente el uno del otro. Cambio (\"change\") : Un cambio (o diff, o delta) representa una modificaci\u00f3n espec\u00edfica de un documento bajo el control de versiones. La granularidad de la modificaci\u00f3n que es considerada como un cambio var\u00eda entre los sistemas de control de versiones. Desplegar (\"checkout\") : Es crear una copia de trabajo local desde el repositorio. Un usuario puede especificar una revisi\u00f3n en concreto u obtener la \u00faltima. El t\u00e9rmino 'checkout' tambi\u00e9n se puede utilizar como un sustantivo para describir la copia de trabajo. Confirmar (\"commit\") : Confirmar es escribir o mezclar los cambios realizados en la copia de trabajo del repositorio. Los t\u00e9rminos 'commit' y 'checkin' tambi\u00e9n se pueden utilizar como sustantivos para describir la nueva revisi\u00f3n que se crea como resultado de confirmar. Conflicto (\"conflict\") : Un conflicto se produce cuando diferentes partes realizan cambios en el mismo documento, y el sistema es incapaz de conciliar los cambios. Un usuario debe resolver el conflicto mediante la integraci\u00f3n de los cambios, o mediante la selecci\u00f3n de un cambio en favor del otro. Cabeza (\"head\") : Tambi\u00e9n a veces se llama tip (punta) y se refiere a la \u00faltima confirmaci\u00f3n, ya sea en el tronco ('trunk') o en una rama ('branch'). El tronco y cada rama tienen su propia cabeza, aunque HEAD se utiliza a veces libremente para referirse al tronco. Tronco (\"trunk\") : La \u00fanica l\u00ednea de desarrollo que no es una rama (a veces tambi\u00e9n llamada l\u00ednea base, l\u00ednea principal o m\u00e1ster). Fusionar, integrar, mezclar (\"merge\") : Una fusi\u00f3n o integraci\u00f3n es una operaci\u00f3n en la que se aplican dos tipos de cambios en un archivo o conjunto de archivos. Algunos escenarios de ejemplo son los siguientes: Un usuario, trabajando en un conjunto de archivos, actualiza o sincroniza su copia de trabajo con los cambios realizados y confirmados, por otros usuarios, en el repositorio. Un usuario intenta confirmar archivos que han sido actualizado por otros usuarios desde el \u00faltimo despliegue ('checkout'), y el software de control de versiones integra autom\u00e1ticamente los archivos (por lo general, despu\u00e9s de preguntarle al usuario si se debe proceder con la integraci\u00f3n autom\u00e1tica, y en algunos casos s\u00f3lo se hace si la fusi\u00f3n puede ser clara y razonablemente resuelta). Un conjunto de archivos se bifurca, un problema que exist\u00eda antes de la ramificaci\u00f3n se trabaja en una nueva rama, y la soluci\u00f3n se combina luego en la otra rama. Se crea una rama, el c\u00f3digo de los archivos es independiente editado, y la rama actualizada se incorpora m\u00e1s tarde en un \u00fanico tronco unificado. Clasificaci\u00f3n Podemos clasificar los sistemas de control de versiones atendiendo a la arquitectura utilizada para el almacenamiento del c\u00f3digo: locales, centralizados y distribuidos. Locales Los cambios son guardados localmente y no se comparten con nadie. Esta arquitectura es la antecesora de las dos siguientes. Centralizados Existe un repositorio centralizado de todo el c\u00f3digo, del cual es responsable un \u00fanico usuario (o conjunto de ellos). Se facilitan las tareas administrativas a cambio de reducir flexibilidad, pues todas las decisiones fuertes (como crear una nueva rama) necesitan la aprobaci\u00f3n del responsable. Algunos ejemplos son CVS y Subversion. Distribuidos Cada usuario tiene su propio repositorio. Los distintos repositorios pueden intercambiar y mezclar revisiones entre ellos. Es frecuente el uso de un repositorio, que est\u00e1 normalmente disponible, que sirve de punto de sincronizaci\u00f3n de los distintos repositorios locales. Ejemplos: Git y Mercurial. Ventajas de sistemas distribuidos No es necesario estar conectado para guardar cambios. Posibilidad de continuar trabajando si el repositorio remoto no est\u00e1 accesible. El repositorio central est\u00e1 m\u00e1s libre de ramas de pruebas. Se necesitan menos recursos para el repositorio remoto. M\u00e1s flexibles al permitir gestionar cada repositorio personal como se quiera.","title":"Sistemas de control de versiones"},{"location":"cvs/#sistemas-de-control-de-versiones","text":"","title":"Sistemas de control de versiones"},{"location":"cvs/#definicion-clasificacion-y-funcionamiento","text":"Se llama control de versiones a la gesti\u00f3n de los diversos cambios que se realizan sobre los elementos de alg\u00fan producto o una configuraci\u00f3n del mismo. Una versi\u00f3n, revisi\u00f3n o edici\u00f3n de un producto, es el estado en el que se encuentra dicho producto en un momento dado de su desarrollo o modificaci\u00f3n. Aunque un sistema de control de versiones puede realizarse de forma manual, es muy aconsejable disponer de herramientas que faciliten esta gesti\u00f3n dando lugar a los llamados sistemas de control de versiones o SVC (del ingl\u00e9s System Version Control). Estos sistemas facilitan la administraci\u00f3n de las distintas versiones de cada producto desarrollado, as\u00ed como las posibles especializaciones realizadas (por ejemplo, para alg\u00fan cliente espec\u00edfico). Ejemplos de este tipo de herramientas son entre otros: CVS, Subversion, SourceSafe, ClearCase, Darcs, Bazaar , Plastic SCM, Git, Mercurial, Perforce.","title":"Definici\u00f3n, clasificaci\u00f3n y funcionamiento"},{"location":"cvs/#terminologia","text":"Repositorio (\"repository\") : El repositorio es el lugar en el que se almacenan los datos actualizados e hist\u00f3ricos de cambios. Revisi\u00f3n (\"revision\") : Una revisi\u00f3n es una versi\u00f3n determinada de la informaci\u00f3n que se gestiona. Hay sistemas que identifican las revisiones con un contador (Ej. subversion). Hay otros sistemas que identifican las revisiones mediante un c\u00f3digo de detecci\u00f3n de modificaciones (Ej. git usa SHA1). Etiqueta (\"tag\") : Los tags permiten identificar de forma f\u00e1cil revisiones importantes en el proyecto. Por ejemplo se suelen usar tags para identificar el contenido de las versiones publicadas del proyecto. Rama (\"branch\") : Un conjunto de archivos puede ser ramificado o bifurcado en un punto en el tiempo de manera que, a partir de ese momento, dos copias de esos archivos se pueden desarrollar a velocidades diferentes o en formas diferentes de forma independiente el uno del otro. Cambio (\"change\") : Un cambio (o diff, o delta) representa una modificaci\u00f3n espec\u00edfica de un documento bajo el control de versiones. La granularidad de la modificaci\u00f3n que es considerada como un cambio var\u00eda entre los sistemas de control de versiones. Desplegar (\"checkout\") : Es crear una copia de trabajo local desde el repositorio. Un usuario puede especificar una revisi\u00f3n en concreto u obtener la \u00faltima. El t\u00e9rmino 'checkout' tambi\u00e9n se puede utilizar como un sustantivo para describir la copia de trabajo. Confirmar (\"commit\") : Confirmar es escribir o mezclar los cambios realizados en la copia de trabajo del repositorio. Los t\u00e9rminos 'commit' y 'checkin' tambi\u00e9n se pueden utilizar como sustantivos para describir la nueva revisi\u00f3n que se crea como resultado de confirmar. Conflicto (\"conflict\") : Un conflicto se produce cuando diferentes partes realizan cambios en el mismo documento, y el sistema es incapaz de conciliar los cambios. Un usuario debe resolver el conflicto mediante la integraci\u00f3n de los cambios, o mediante la selecci\u00f3n de un cambio en favor del otro. Cabeza (\"head\") : Tambi\u00e9n a veces se llama tip (punta) y se refiere a la \u00faltima confirmaci\u00f3n, ya sea en el tronco ('trunk') o en una rama ('branch'). El tronco y cada rama tienen su propia cabeza, aunque HEAD se utiliza a veces libremente para referirse al tronco. Tronco (\"trunk\") : La \u00fanica l\u00ednea de desarrollo que no es una rama (a veces tambi\u00e9n llamada l\u00ednea base, l\u00ednea principal o m\u00e1ster). Fusionar, integrar, mezclar (\"merge\") : Una fusi\u00f3n o integraci\u00f3n es una operaci\u00f3n en la que se aplican dos tipos de cambios en un archivo o conjunto de archivos. Algunos escenarios de ejemplo son los siguientes: Un usuario, trabajando en un conjunto de archivos, actualiza o sincroniza su copia de trabajo con los cambios realizados y confirmados, por otros usuarios, en el repositorio. Un usuario intenta confirmar archivos que han sido actualizado por otros usuarios desde el \u00faltimo despliegue ('checkout'), y el software de control de versiones integra autom\u00e1ticamente los archivos (por lo general, despu\u00e9s de preguntarle al usuario si se debe proceder con la integraci\u00f3n autom\u00e1tica, y en algunos casos s\u00f3lo se hace si la fusi\u00f3n puede ser clara y razonablemente resuelta). Un conjunto de archivos se bifurca, un problema que exist\u00eda antes de la ramificaci\u00f3n se trabaja en una nueva rama, y la soluci\u00f3n se combina luego en la otra rama. Se crea una rama, el c\u00f3digo de los archivos es independiente editado, y la rama actualizada se incorpora m\u00e1s tarde en un \u00fanico tronco unificado.","title":"Terminolog\u00eda"},{"location":"cvs/#clasificacion","text":"Podemos clasificar los sistemas de control de versiones atendiendo a la arquitectura utilizada para el almacenamiento del c\u00f3digo: locales, centralizados y distribuidos.","title":"Clasificaci\u00f3n"},{"location":"cvs/#locales","text":"Los cambios son guardados localmente y no se comparten con nadie. Esta arquitectura es la antecesora de las dos siguientes.","title":"Locales"},{"location":"cvs/#centralizados","text":"Existe un repositorio centralizado de todo el c\u00f3digo, del cual es responsable un \u00fanico usuario (o conjunto de ellos). Se facilitan las tareas administrativas a cambio de reducir flexibilidad, pues todas las decisiones fuertes (como crear una nueva rama) necesitan la aprobaci\u00f3n del responsable. Algunos ejemplos son CVS y Subversion.","title":"Centralizados"},{"location":"cvs/#distribuidos","text":"Cada usuario tiene su propio repositorio. Los distintos repositorios pueden intercambiar y mezclar revisiones entre ellos. Es frecuente el uso de un repositorio, que est\u00e1 normalmente disponible, que sirve de punto de sincronizaci\u00f3n de los distintos repositorios locales. Ejemplos: Git y Mercurial.","title":"Distribuidos"},{"location":"cvs/#ventajas-de-sistemas-distribuidos","text":"No es necesario estar conectado para guardar cambios. Posibilidad de continuar trabajando si el repositorio remoto no est\u00e1 accesible. El repositorio central est\u00e1 m\u00e1s libre de ramas de pruebas. Se necesitan menos recursos para el repositorio remoto. M\u00e1s flexibles al permitir gestionar cada repositorio personal como se quiera.","title":"Ventajas de sistemas distribuidos"},{"location":"data/","text":"Persistiendo datos Por defecto ya hemos indicado que un contenedor est\u00e1 aislado de todo. Hemos visto como podemos conectar el contenedor a un puerto de red para poder acceder a \u00e9l. Eso incluye al sistema de archivos que contiene. De tal manera que si se elimina el contenedor, se eliminan tambi\u00e9n sus archivos. Si queremos almacenar datos (una web, una base de datos, etc.) dentro de un contenedor necesitamos una manera de almacenarlos sin perderlos. Docker ofrece tres maneras: A trav\u00e9s de vol\u00famenes, que son objetos de Docker como las im\u00e1genes y los contenedores. Montando un directorio de la m\u00e1quina anfitri\u00f3n dentro del contenedor. Almacen\u00e1ndolo en la memoria del sistema (aunque tambi\u00e9n se perder\u00edan al reiniciar el servidor). Lo normal es usar vol\u00famenes, pero habr\u00e1 ocasiones en que es preferible montar directamente un directorio de nuestro espacio de trabajo. Por ejemplo, para guardar los datos de una base de datos usaremos vol\u00famenes, pero para guardar el c\u00f3digo de una aplicaci\u00f3n o de una p\u00e1gina web montaremos el directorio. La raz\u00f3n para esto \u00faltimo es que tanto nuestro entorno de desarrollo como el contenedor tengan acceso a los archivos del c\u00f3digo fuente. Los vol\u00famenes, al contrario que los directorios montados, no deben accederse desde la m\u00e1quina anfitri\u00f3n. Crear un volumen Como necesitamos crear una base de datos para nuestro blog con WordPress vamos a crear un volumen donde guardar la informaci\u00f3n: $ docker volume create wordpress-db wordpress-db Listar vol\u00famenes Con docker volume ls podemos visualizar todos los volumenes disponibles. $ docker volume ls DRIVER VOLUME NAME local wordpress-db Visualizar vol\u00famenes Los volumenes se crean en un directorio del sistema y no es recomendable acceder a \u00e9l, no al menos mientras haya un contenedor us\u00e1ndolo. En cualquier caso, si queremos ver los metadatos de un volumen podemos usar docker volume inspect $ docker volume inspect wordpress-db [ { \"CreatedAt\": \"yyyy-mm-ddThh:ii:ss+Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/wordpress-db/_data\", \"Name\": \"wordpress-db\", \"Options\": {}, \"Scope\": \"local\" } ] Borrar volumenes Como todos los objetos de Docker , los vol\u00famenes tambi\u00e9n pueden ser borrados, pero solo si no est\u00e1n en uso. Mucha precauci\u00f3n al borrar los vol\u00famenes, porque perder\u00edamos todos los datos que contenga. Para borrar un contenedor usaremos docker volume rm y el nombre del contenedor.","title":"Persistiendo datos"},{"location":"data/#persistiendo-datos","text":"Por defecto ya hemos indicado que un contenedor est\u00e1 aislado de todo. Hemos visto como podemos conectar el contenedor a un puerto de red para poder acceder a \u00e9l. Eso incluye al sistema de archivos que contiene. De tal manera que si se elimina el contenedor, se eliminan tambi\u00e9n sus archivos. Si queremos almacenar datos (una web, una base de datos, etc.) dentro de un contenedor necesitamos una manera de almacenarlos sin perderlos. Docker ofrece tres maneras: A trav\u00e9s de vol\u00famenes, que son objetos de Docker como las im\u00e1genes y los contenedores. Montando un directorio de la m\u00e1quina anfitri\u00f3n dentro del contenedor. Almacen\u00e1ndolo en la memoria del sistema (aunque tambi\u00e9n se perder\u00edan al reiniciar el servidor). Lo normal es usar vol\u00famenes, pero habr\u00e1 ocasiones en que es preferible montar directamente un directorio de nuestro espacio de trabajo. Por ejemplo, para guardar los datos de una base de datos usaremos vol\u00famenes, pero para guardar el c\u00f3digo de una aplicaci\u00f3n o de una p\u00e1gina web montaremos el directorio. La raz\u00f3n para esto \u00faltimo es que tanto nuestro entorno de desarrollo como el contenedor tengan acceso a los archivos del c\u00f3digo fuente. Los vol\u00famenes, al contrario que los directorios montados, no deben accederse desde la m\u00e1quina anfitri\u00f3n.","title":"Persistiendo datos"},{"location":"data/#crear-un-volumen","text":"Como necesitamos crear una base de datos para nuestro blog con WordPress vamos a crear un volumen donde guardar la informaci\u00f3n: $ docker volume create wordpress-db wordpress-db","title":"Crear un volumen"},{"location":"data/#listar-volumenes","text":"Con docker volume ls podemos visualizar todos los volumenes disponibles. $ docker volume ls DRIVER VOLUME NAME local wordpress-db","title":"Listar vol\u00famenes"},{"location":"data/#visualizar-volumenes","text":"Los volumenes se crean en un directorio del sistema y no es recomendable acceder a \u00e9l, no al menos mientras haya un contenedor us\u00e1ndolo. En cualquier caso, si queremos ver los metadatos de un volumen podemos usar docker volume inspect $ docker volume inspect wordpress-db [ { \"CreatedAt\": \"yyyy-mm-ddThh:ii:ss+Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/wordpress-db/_data\", \"Name\": \"wordpress-db\", \"Options\": {}, \"Scope\": \"local\" } ]","title":"Visualizar vol\u00famenes"},{"location":"data/#borrar-volumenes","text":"Como todos los objetos de Docker , los vol\u00famenes tambi\u00e9n pueden ser borrados, pero solo si no est\u00e1n en uso. Mucha precauci\u00f3n al borrar los vol\u00famenes, porque perder\u00edamos todos los datos que contenga. Para borrar un contenedor usaremos docker volume rm y el nombre del contenedor.","title":"Borrar volumenes"},{"location":"debian/","text":"Instalaci\u00f3n y configuraci\u00f3n de nuestra m\u00e1quina virtual Instalaci\u00f3n de Debian Como servidor, utilizaremos la distribuci\u00f3n Linux Debian. Tal y como podemos leer en la propia p\u00e1gina de Debian : Quote Un CD de \"instalaci\u00f3n por red\" o \"netinst\" es un \u00fanico CD que posibilita que instale el sistema completo. Este \u00fanico CD contiene s\u00f3lo la m\u00ednima cantidad de software para instalar el sistema base y obtener el resto de paquetes a trav\u00e9s de Internet. As\u00ed pues, procedamos a descargar la imagen de Debian netinstall aqu\u00ed Info Queda muy lejos de la intenci\u00f3n de este m\u00f3dulo explicar como instalar m\u00e1quinas virtuales puesto que es algo que se supone aprendido del curso anterior y/u otros m\u00f3dulos. As\u00ed pues, se dar\u00e1n unas pautas generales para instalar la m\u00e1quina correctamente. La instalaci\u00f3n de esta m\u00e1quina virtual deber\u00eda servir a lo largo del m\u00f3dulo utilizado cualquier hipervisor (VMWare, VirtualBox, KVM, HyperV...). No obstante, se utilizar\u00e1 Virtualbox para esta explicaci\u00f3n. En primer lugar, debemos crear una m\u00e1quina virtual nueva, indicando su ubicaci\u00f3n, su nombre y el tipo de sistema operativo: Le indicamos que monte como unidad de CD la iso de netinstall de Debian que hemos descargado previamente: Tambi\u00e9n estableceremos un \u00fanico interfaz de red. Para ello, en el \u00fanico adaptador de red que debe tener la m\u00e1quina virtual, debemos configurarlo como tipo puente, de forma que obtenga una IP en el rango de la red local en la que nos encontremos conectados (casa, instituto...). Sin entorno gr\u00e1fico la m\u00e1quina puede que funcione perfectamente con 1GB de RAM, no obstante se aconseja, si es posible, asignarle 2GB de RAM y, como m\u00ednimo, 2 procesadores. Pod\u00e9is instalar Debian tanto de forma gr\u00e1fica como de forma cl\u00e1sica en terminal. La primera de ellas es la que os recomiendo: Le d\u00e1is el nombre que quer\u00e1is a vuestra m\u00e1quina. Recomendable un nombre corto pues luego aparecer\u00e1 en el prompt del terminal ( usuario@nombredemaquina ) Os pedir\u00e1 tambi\u00e9n contrase\u00f1a de superusuario (root), nombre de vuestro usuario y contrase\u00f1a para este nuevo usuario: Tras ello, para simplificar nuestro proceso, le diremos que utilice todo el disco para la instalaci\u00f3n: E iremos dejando todas las opciones que nos vayan apareciendo por defecto y continuando la instalaci\u00f3n. Tras un rato, que puede ser m\u00e1s o menos largo, nos mostrar\u00e1 la opci\u00f3n de instalar un entorno gr\u00e1fico. En principio no nos hace falta ninguno y esta es la opci\u00f3n recomendada por un tema de economizaci\u00f3n de los recursos. Pero si por alguna raz\u00f3n quer\u00e9is instalar alguno, os recomiendo LXDE puesto que es el que menos recursos consume: Tambi\u00e9n deb\u00e9is marcar las opciones que aparecen en la imagen, SSH Server y Utilidades est\u00e1ndar . Le indicamos que s\u00ed que instale el gestor de arranque GRUB y continuamos con todas las opciones por defecto: Y le indicamos que lo instale en el \u00fanico disco que tenemos: /dev/sda1 (pinchad en el nombre o no lo instalar\u00e1 ah\u00ed) Completar\u00e1 el proceso y pedir\u00e1 reiniciar, cosa que har\u00e9is. Tras ello, si no ten\u00e9is entorno gr\u00e1fico aparecer\u00e1 un terminal pidiendo login. Si hubier\u00e1is instalado el entorno gr\u00e1fico, os aparecer\u00e1 algo as\u00ed: En ambos casos, introduciendo el nombre de usuario y contrase\u00f1a podremos loguearnos en el sistema. Dar permisos de sudo a nuestro usuario Una vez instalada nuestra Debian, tendremos un usuario raso que es el que le dijimos que crease durante la instalaci\u00f3n. Puesto que a lo largo de este m\u00f3dulo realizaremos incontables tareas de administraci\u00f3n, resulta un tanto inc\u00f3modo, as\u00ed como peligroso, el tener que cambiar de nuestro usuario a root cada vez que haya que instalar, configurar o modificar algo que as\u00ed lo requiera. As\u00ed pues, le daremos permisos de sudo a nuestro usuario. Estos permisos nos permitir\u00e1n que cualquier comando que ejecutemos en el terminal precedido de la palabra sudo se ejecute como root . De la misma forma, cualquier comando que ejecutemos con nuestro usuario sin sudo , ser\u00e1 ejecutado con los permisos de nuestro usuario, por lo que nos protegemos de liarla con un comando que no toca como root . Dicho esto, hay varias formas de proceder, veamos la m\u00e1s t\u00edpica y conocida. Se trata de modificar el archivo del sistema encargado de recoger estos permisos: /etc/sudoers . En primer lugar debemos conectarnos por SSH a nuestra m\u00e1quina Debian: ssh -l nombre_de_usuario IP_MV_Debian Donde: nombre_de_usuario es vuestro nombre de usuario (el que configurast\u00e9is durante la instalaci\u00f3n) IP_MV_Debian es la IP de la m\u00e1quina Debian Info Existen varias formas de conocer la IP de vuestra Debian pero quiz\u00e1s la m\u00e1s sencilla sea desde la propia m\u00e1quina virtual, con el comando: ip a Ah\u00ed veo que esa IP est\u00e1 dentro del rango de mi red local. Adem\u00e1s, puesto que la m\u00e1quina s\u00f3lo tiene una interfaz de red, no puede ser ninguna otra. Esa ser\u00e1 la IP a la que conectarse. Cambiando de nuestro usuario al usuario root : su root Ejecutamos la aplicaci\u00f3n visudo que se encarga directamente de modificar el archivo de sudoers: # /usr/sbin/visudo Y dejamos el archivo as\u00ed, claro est\u00e1, con vuestro propio nombre de usuario: # User privilege specification root ALL=(ALL:ALL) ALL nombreusuario ALL=(ALL:ALL) ALL Pulsamos CTRL+x y guardamos los cambios. Tras esto, debemos desloguearnos de nuestra sesi\u00f3n SSH y volver a loguearnos. Ahora podremos validar que ya podemos realizar acciones que requieran permisos de superusuario o root. Esta validaci\u00f3n puede realizarse con el comando: sudo -v Que en caso de no tener permisos nos devolver\u00e1 el siguiente mensaje: Sorry, user [ username ] may not run sudo on [ hostname ] . Y en caso de tenerlos, no devolver\u00e1 nada. Si a\u00fan as\u00ed no os quedase del todo claro, pod\u00e9is utilizar este comando: timeout 2 sudo id && echo Access granted || echo Access denied Que, en caso de tener los permisos de sudo devuelve: uid=0(root) gid=0(root) grupos=0(root) Access granted Y si no los tuviera, devuelve: [username] is not in the sudoers file. This incident will be reported. Access denied Configuraci\u00f3n En primer lugar nos crearemos nuestro par de claves, p\u00fablica y privada, en el ordenador que se conectar\u00e1 a nuestra debian, con el comando ( sin sudo ): ssh-keygen -b 4096 Si dej\u00e1is las opciones por defecto, crear\u00e1 una clave privada id_rsa y una clave p\u00fablica id_rsa.pub en el directorio /home/nombreusuario/.ssh . Os pedir\u00e1 una contrase\u00f1a para proteger el uso de la clave privada. Puesto que precisamente queremos agilizar el proceso de conexi\u00f3n por SSH para no introducir contrase\u00f1as, deb\u00e9is dejarla vac\u00eda . Una vez creado el par de claves, tal y como hemos visto en el apartado anterior, el servidor SSH (Debian) debe poseer nuestra clave p\u00fablica para que podamos autenticarnos con nuestra clave privada, que como su nombre indica, s\u00f3lo debemos poseer nosotros y por eso nos identifica un\u00edvocamente. Este proceso de copia se puede realizar f\u00e1cilmente con el comando: ssh-copy-id usuario@ip_servidor Para no tener ning\u00fan problema con los permisos sobre directorios y archivos, ejecutad en Debian: chmod 700 .ssh/ chmod 600 .ssh/authorized_keys Que no es m\u00e1s que una conexi\u00f3n SSH que adem\u00e1s copia la clave, por lo que: usuario: nombre de vuestro usuario en Debian ip_servidor: ip de la m\u00e1quina Debian Para Windows Este m\u00f3dulo est\u00e1 dise\u00f1ado desde un cliente Linux conect\u00e1ndose al servidor Linux, por lo que el cliente SSH est\u00e1 integrado en el propio terminal. Para Windows existen multitud de alternativas como cliente SSH, desde utilizar el propio WSL2 (Windows Subsystem Linux) de forma similar a lo que aqu\u00ed se describe, hasta utilizar cualquier otro de los varios clientes disponibles Por ejemplo, si utiliz\u00e1is Putty , deber\u00e9is seguir los pasos que detallan en este tutorial para configurar las claves. En caso de utilizar otro cliente, buscad la forma de hacerlo pues diferir\u00e1 en cada caso. Referencias \u00bfQu\u00e9 es un VPS? Todo lo que necesitas saber sobre servidores virtuales","title":"Instalaci\u00f3n y configuraci\u00f3n de nuestra m\u00e1quina virtual"},{"location":"debian/#instalacion-y-configuracion-de-nuestra-maquina-virtual","text":"","title":"Instalaci\u00f3n y configuraci\u00f3n de nuestra m\u00e1quina virtual"},{"location":"debian/#instalacion-de-debian","text":"Como servidor, utilizaremos la distribuci\u00f3n Linux Debian. Tal y como podemos leer en la propia p\u00e1gina de Debian : Quote Un CD de \"instalaci\u00f3n por red\" o \"netinst\" es un \u00fanico CD que posibilita que instale el sistema completo. Este \u00fanico CD contiene s\u00f3lo la m\u00ednima cantidad de software para instalar el sistema base y obtener el resto de paquetes a trav\u00e9s de Internet. As\u00ed pues, procedamos a descargar la imagen de Debian netinstall aqu\u00ed Info Queda muy lejos de la intenci\u00f3n de este m\u00f3dulo explicar como instalar m\u00e1quinas virtuales puesto que es algo que se supone aprendido del curso anterior y/u otros m\u00f3dulos. As\u00ed pues, se dar\u00e1n unas pautas generales para instalar la m\u00e1quina correctamente. La instalaci\u00f3n de esta m\u00e1quina virtual deber\u00eda servir a lo largo del m\u00f3dulo utilizado cualquier hipervisor (VMWare, VirtualBox, KVM, HyperV...). No obstante, se utilizar\u00e1 Virtualbox para esta explicaci\u00f3n. En primer lugar, debemos crear una m\u00e1quina virtual nueva, indicando su ubicaci\u00f3n, su nombre y el tipo de sistema operativo: Le indicamos que monte como unidad de CD la iso de netinstall de Debian que hemos descargado previamente: Tambi\u00e9n estableceremos un \u00fanico interfaz de red. Para ello, en el \u00fanico adaptador de red que debe tener la m\u00e1quina virtual, debemos configurarlo como tipo puente, de forma que obtenga una IP en el rango de la red local en la que nos encontremos conectados (casa, instituto...). Sin entorno gr\u00e1fico la m\u00e1quina puede que funcione perfectamente con 1GB de RAM, no obstante se aconseja, si es posible, asignarle 2GB de RAM y, como m\u00ednimo, 2 procesadores. Pod\u00e9is instalar Debian tanto de forma gr\u00e1fica como de forma cl\u00e1sica en terminal. La primera de ellas es la que os recomiendo: Le d\u00e1is el nombre que quer\u00e1is a vuestra m\u00e1quina. Recomendable un nombre corto pues luego aparecer\u00e1 en el prompt del terminal ( usuario@nombredemaquina ) Os pedir\u00e1 tambi\u00e9n contrase\u00f1a de superusuario (root), nombre de vuestro usuario y contrase\u00f1a para este nuevo usuario: Tras ello, para simplificar nuestro proceso, le diremos que utilice todo el disco para la instalaci\u00f3n: E iremos dejando todas las opciones que nos vayan apareciendo por defecto y continuando la instalaci\u00f3n. Tras un rato, que puede ser m\u00e1s o menos largo, nos mostrar\u00e1 la opci\u00f3n de instalar un entorno gr\u00e1fico. En principio no nos hace falta ninguno y esta es la opci\u00f3n recomendada por un tema de economizaci\u00f3n de los recursos. Pero si por alguna raz\u00f3n quer\u00e9is instalar alguno, os recomiendo LXDE puesto que es el que menos recursos consume: Tambi\u00e9n deb\u00e9is marcar las opciones que aparecen en la imagen, SSH Server y Utilidades est\u00e1ndar . Le indicamos que s\u00ed que instale el gestor de arranque GRUB y continuamos con todas las opciones por defecto: Y le indicamos que lo instale en el \u00fanico disco que tenemos: /dev/sda1 (pinchad en el nombre o no lo instalar\u00e1 ah\u00ed) Completar\u00e1 el proceso y pedir\u00e1 reiniciar, cosa que har\u00e9is. Tras ello, si no ten\u00e9is entorno gr\u00e1fico aparecer\u00e1 un terminal pidiendo login. Si hubier\u00e1is instalado el entorno gr\u00e1fico, os aparecer\u00e1 algo as\u00ed: En ambos casos, introduciendo el nombre de usuario y contrase\u00f1a podremos loguearnos en el sistema.","title":"Instalaci\u00f3n de Debian"},{"location":"debian/#dar-permisos-de-sudo-a-nuestro-usuario","text":"Una vez instalada nuestra Debian, tendremos un usuario raso que es el que le dijimos que crease durante la instalaci\u00f3n. Puesto que a lo largo de este m\u00f3dulo realizaremos incontables tareas de administraci\u00f3n, resulta un tanto inc\u00f3modo, as\u00ed como peligroso, el tener que cambiar de nuestro usuario a root cada vez que haya que instalar, configurar o modificar algo que as\u00ed lo requiera. As\u00ed pues, le daremos permisos de sudo a nuestro usuario. Estos permisos nos permitir\u00e1n que cualquier comando que ejecutemos en el terminal precedido de la palabra sudo se ejecute como root . De la misma forma, cualquier comando que ejecutemos con nuestro usuario sin sudo , ser\u00e1 ejecutado con los permisos de nuestro usuario, por lo que nos protegemos de liarla con un comando que no toca como root . Dicho esto, hay varias formas de proceder, veamos la m\u00e1s t\u00edpica y conocida. Se trata de modificar el archivo del sistema encargado de recoger estos permisos: /etc/sudoers . En primer lugar debemos conectarnos por SSH a nuestra m\u00e1quina Debian: ssh -l nombre_de_usuario IP_MV_Debian Donde: nombre_de_usuario es vuestro nombre de usuario (el que configurast\u00e9is durante la instalaci\u00f3n) IP_MV_Debian es la IP de la m\u00e1quina Debian Info Existen varias formas de conocer la IP de vuestra Debian pero quiz\u00e1s la m\u00e1s sencilla sea desde la propia m\u00e1quina virtual, con el comando: ip a Ah\u00ed veo que esa IP est\u00e1 dentro del rango de mi red local. Adem\u00e1s, puesto que la m\u00e1quina s\u00f3lo tiene una interfaz de red, no puede ser ninguna otra. Esa ser\u00e1 la IP a la que conectarse. Cambiando de nuestro usuario al usuario root : su root Ejecutamos la aplicaci\u00f3n visudo que se encarga directamente de modificar el archivo de sudoers: # /usr/sbin/visudo Y dejamos el archivo as\u00ed, claro est\u00e1, con vuestro propio nombre de usuario: # User privilege specification root ALL=(ALL:ALL) ALL nombreusuario ALL=(ALL:ALL) ALL Pulsamos CTRL+x y guardamos los cambios. Tras esto, debemos desloguearnos de nuestra sesi\u00f3n SSH y volver a loguearnos. Ahora podremos validar que ya podemos realizar acciones que requieran permisos de superusuario o root. Esta validaci\u00f3n puede realizarse con el comando: sudo -v Que en caso de no tener permisos nos devolver\u00e1 el siguiente mensaje: Sorry, user [ username ] may not run sudo on [ hostname ] . Y en caso de tenerlos, no devolver\u00e1 nada. Si a\u00fan as\u00ed no os quedase del todo claro, pod\u00e9is utilizar este comando: timeout 2 sudo id && echo Access granted || echo Access denied Que, en caso de tener los permisos de sudo devuelve: uid=0(root) gid=0(root) grupos=0(root) Access granted Y si no los tuviera, devuelve: [username] is not in the sudoers file. This incident will be reported. Access denied","title":"Dar permisos de sudo a nuestro usuario"},{"location":"debian/#configuracion","text":"En primer lugar nos crearemos nuestro par de claves, p\u00fablica y privada, en el ordenador que se conectar\u00e1 a nuestra debian, con el comando ( sin sudo ): ssh-keygen -b 4096 Si dej\u00e1is las opciones por defecto, crear\u00e1 una clave privada id_rsa y una clave p\u00fablica id_rsa.pub en el directorio /home/nombreusuario/.ssh . Os pedir\u00e1 una contrase\u00f1a para proteger el uso de la clave privada. Puesto que precisamente queremos agilizar el proceso de conexi\u00f3n por SSH para no introducir contrase\u00f1as, deb\u00e9is dejarla vac\u00eda . Una vez creado el par de claves, tal y como hemos visto en el apartado anterior, el servidor SSH (Debian) debe poseer nuestra clave p\u00fablica para que podamos autenticarnos con nuestra clave privada, que como su nombre indica, s\u00f3lo debemos poseer nosotros y por eso nos identifica un\u00edvocamente. Este proceso de copia se puede realizar f\u00e1cilmente con el comando: ssh-copy-id usuario@ip_servidor Para no tener ning\u00fan problema con los permisos sobre directorios y archivos, ejecutad en Debian: chmod 700 .ssh/ chmod 600 .ssh/authorized_keys Que no es m\u00e1s que una conexi\u00f3n SSH que adem\u00e1s copia la clave, por lo que: usuario: nombre de vuestro usuario en Debian ip_servidor: ip de la m\u00e1quina Debian Para Windows Este m\u00f3dulo est\u00e1 dise\u00f1ado desde un cliente Linux conect\u00e1ndose al servidor Linux, por lo que el cliente SSH est\u00e1 integrado en el propio terminal. Para Windows existen multitud de alternativas como cliente SSH, desde utilizar el propio WSL2 (Windows Subsystem Linux) de forma similar a lo que aqu\u00ed se describe, hasta utilizar cualquier otro de los varios clientes disponibles Por ejemplo, si utiliz\u00e1is Putty , deber\u00e9is seguir los pasos que detallan en este tutorial para configurar las claves. En caso de utilizar otro cliente, buscad la forma de hacerlo pues diferir\u00e1 en cada caso.","title":"Configuraci\u00f3n"},{"location":"debian/#referencias","text":"\u00bfQu\u00e9 es un VPS? Todo lo que necesitas saber sobre servidores virtuales","title":"Referencias"},{"location":"debian_teoria/","text":"Introducci\u00f3n En este m\u00f3dulo vamos a simular escenarios reales donde apenas trabajaremos en local, en nuestro propio ordenador. Simularemos, mediante una m\u00e1quina virtual que es en la que realmente trabajaremos, que todos nuestros despliegues ocurren en una m\u00e1quina remota, tal y como ocurre en la realidad. De hecho, se simular\u00e1 un escenario donde tengamos contratado un VPS (Virtual Private Server) y debamos conectarnos de forma remota al mismo para poder trabajar. Un escenario muy com\u00fan en el mundo real. \u00bfQu\u00e9 es un VPS? Un servidor es una computadora en la que tu proveedor de alojamiento web almacena los archivos y las bases de datos necesarios para tu sitio web. Cada vez que un visitante en l\u00ednea quiere acceder a tu sitio web, su navegador le env\u00eda una solicitud a tu servidor y transfiere los archivos necesarios a trav\u00e9s de Internet. El alojamiento VPS te proporciona un servidor en la nube que simula un servidor f\u00edsico; sin embargo, en realidad, la m\u00e1quina se comparte entre varios usuarios. Al usar la tecnolog\u00eda de virtualizaci\u00f3n, tu proveedor de alojamiento web instala una capa virtual sobre el sistema operativo del servidor. Esta capa divide el servidor en particiones y le permite a cada usuario instalar su propio sistema operativo y software. Por lo tanto, un servidor privado virtual (VPS) es tanto virtual como privado porque tienes control absoluto. Est\u00e1 separado de otros usuarios del servidor a nivel del sistema operativo. De hecho, la tecnolog\u00eda VPS es similar a la creaci\u00f3n de particiones en tu computadora cuando quieres ejecutar m\u00e1s de un sistema operativo (por ejemplo, Windows y Linux) sin tener que reiniciar. Un VPS te permite configurar tu sitio web dentro de un contenedor seguro con recursos garantizados (memoria, espacio en disco, n\u00facleos de CPU, etc.) que no tienes que compartir con otros usuarios. Con el hosting VPS, tienes el mismo acceso de nivel ra\u00edz que si alquilaras un servidor dedicado, pero a un costo mucho m\u00e1s bajo. El VPS es una soluci\u00f3n m\u00e1s segura y estable que el hosting compartido, con el que no obtienes espacio de servidor dedicado. Sin embargo, es de menor escala y m\u00e1s barato que alquilar un servidor completo. El hosting VPS generalmente es elegido por los propietarios de sitios web que tienen un tr\u00e1fico de nivel medio que excede los l\u00edmites de los planes de hosting compartido pero que a\u00fan no necesitan los recursos de un servidor dedicado. Conexi\u00f3n mediante SSH Aunque nuestra m\u00e1quina virtual est\u00e9 en nuestro ordenador, ya hemos dicho que estamos simulando un VPS remoto. Para conectarnos a una m\u00e1quina de forma remota y segura, la opci\u00f3n m\u00e1s recomendable es SSH. SSH o Secure Shell es un protocolo de red criptogr\u00e1fico para operar servicios de red de forma segura a trav\u00e9s de una red no protegida. Las aplicaciones t\u00edpicas incluyen l\u00ednea de comandos remota, inicio de sesi\u00f3n y ejecuci\u00f3n de comandos remota, pero cualquier servicio de red puede protegerse con SSH. SSH proporciona un canal seguro a trav\u00e9s de una red no segura mediante el uso de una arquitectura cliente-servidor , conectando una aplicaci\u00f3n cliente SSH con un servidor SSH. El puerto TCP est\u00e1ndar para SSH es 22 y se usa generalmente para acceder a sistemas operativos similares a Unix, pero tambi\u00e9n se puede usar en Microsoft Windows. Proporciona un mecanismo para autenticar un usuario remoto, transferir entradas desde el cliente al host y retransmitir la salida de vuelta al cliente. SSH tiene muchas aplicaciones diferentes: Gesti\u00f3n de servidores a los que no se puede acceder localmente Transferencia segura de archivos Creaci\u00f3n de copias de seguridad Conexi\u00f3n entre dos ordenadores con encriptaci\u00f3n de extremo a extremo Mantenimiento remoto desde otros ordenadores Autenticaci\u00f3n Los dos m\u00e9todos de autenticaci\u00f3n de usuario SSH m\u00e1s comunes que se utilizan son las contrase\u00f1as (cifrado sim\u00e9trico) y las claves SSH (cifrado asim\u00e9trico o de clave p\u00fablica). Los clientes env\u00edan contrase\u00f1as cifradas al servidor de forma segura. Sin embargo, las contrase\u00f1as son un m\u00e9todo de autenticaci\u00f3n arriesgado porque su solidez depende de que el usuario sepa qu\u00e9 hace que una contrase\u00f1a sea segura. Los pares de claves p\u00fablica-privada SSH encriptados asim\u00e9tricamente son una mejor opci\u00f3n. Una vez que el cliente descifra el mensaje, el servidor le otorga acceso al sistema. Es decir, SSH opta por el cifrado h\u00edbrido, donde se utiliza el cifrado asim\u00e9trico para intercambiar unas claves que ser\u00e1n las que se utilizar\u00e1n posteriormente en el intercambio de informaci\u00f3n. Este tipo de cifrado utiliza la misma clave para cifrar y para descifrar la informaci\u00f3n. Por este motivo, la clave debe ser secreta y s\u00f3lo conocida por el emisor y el receptor del mensaje. Cifrados sim\u00e9tricos o de clave privada Este tipo de cifrado utiliza la misma clave para cifrar y para descifrar la informaci\u00f3n. Por este motivo, la clave debe ser secreta y s\u00f3lo conocida por el emisor y el receptor del mensaje. Ventajas Muy r\u00e1pidos \u2192 cifrar y descifrar un mensaje cada vez requiere un cierto tiempo, que si el algoritmo es complejo, puede ser elevado. Inconvenientes Si alguien no autorizado consigue la clave, podr\u00e1 espiar la comunicaci\u00f3n sin problemas \u00bfC\u00f3mo hacemos para que emisor y receptar conozcan la clave en un primer momento? \u2192 no se puede transmitir por el canal inseguro \u2192 hay que transmitirla por otro canal seguro Ejemplos: PIN de la tarjeta del banco o archivo comprimido con contrase\u00f1a Cifrados asim\u00e9tricos o de clave p\u00fablica En este tipo de cifrados cada usuario utiliza un par de claves: una clave p\u00fablica y una clave privada. Un mensaje cifrado con la clave p\u00fablica s\u00f3lo se puede descifrar con su correspondiente clave privada y viceversa. La clave p\u00fablica es accesible a cualquier persona que quiera consultarla, no hace falta que sea transmitida por un canal seguro como en el caso anterior. La clave privada s\u00f3lo la debe conocer su due\u00f1o. Funcionamiento: El emisor cifra un mensaje con la clave p\u00fablica del receptor El receptor recibe el mensaje y es el \u00fanico que podr\u00e1 descifrarlo porque es el \u00fanico que posee la clave cifrada asociada Ventajas No se necesita un nuevo canal independiente y seguro para transmitir la clave Inconvenientes Son m\u00e1s lentos que los cifrados sim\u00e9tricos Hay que proteger muy bien la clave privada y tenerla siempre disponible para poder descifrar los mensajes (no es una contrase\u00f1a) Hay que asegurarse de que la clave p\u00fablica es de qui\u00e9n dice ser y no de un impostor que se est\u00e9 haciendo pasar por \u00e9l Nota Nosotros, para conectarnos por primera vez por SSH y comprobar la conectividad, utilizaremos el cifrado sim\u00e9trico (una contrase\u00f1a). Tras ello, simulando un entorno real que aporte comodidad (no introducir contrase\u00f1a cada vez que hagamos login ) pero tambi\u00e9n y sobre todo, por seguridad, utilizaremos cifrado asim\u00e9trico. Esto es, un par de claves. Referencias \u00bfQu\u00e9 es un VPS? Todo lo que necesitas saber sobre servidores virtuales","title":"Tema 1 - Introducci\u00f3n"},{"location":"debian_teoria/#introduccion","text":"En este m\u00f3dulo vamos a simular escenarios reales donde apenas trabajaremos en local, en nuestro propio ordenador. Simularemos, mediante una m\u00e1quina virtual que es en la que realmente trabajaremos, que todos nuestros despliegues ocurren en una m\u00e1quina remota, tal y como ocurre en la realidad. De hecho, se simular\u00e1 un escenario donde tengamos contratado un VPS (Virtual Private Server) y debamos conectarnos de forma remota al mismo para poder trabajar. Un escenario muy com\u00fan en el mundo real.","title":"Introducci\u00f3n"},{"location":"debian_teoria/#que-es-un-vps","text":"Un servidor es una computadora en la que tu proveedor de alojamiento web almacena los archivos y las bases de datos necesarios para tu sitio web. Cada vez que un visitante en l\u00ednea quiere acceder a tu sitio web, su navegador le env\u00eda una solicitud a tu servidor y transfiere los archivos necesarios a trav\u00e9s de Internet. El alojamiento VPS te proporciona un servidor en la nube que simula un servidor f\u00edsico; sin embargo, en realidad, la m\u00e1quina se comparte entre varios usuarios. Al usar la tecnolog\u00eda de virtualizaci\u00f3n, tu proveedor de alojamiento web instala una capa virtual sobre el sistema operativo del servidor. Esta capa divide el servidor en particiones y le permite a cada usuario instalar su propio sistema operativo y software. Por lo tanto, un servidor privado virtual (VPS) es tanto virtual como privado porque tienes control absoluto. Est\u00e1 separado de otros usuarios del servidor a nivel del sistema operativo. De hecho, la tecnolog\u00eda VPS es similar a la creaci\u00f3n de particiones en tu computadora cuando quieres ejecutar m\u00e1s de un sistema operativo (por ejemplo, Windows y Linux) sin tener que reiniciar. Un VPS te permite configurar tu sitio web dentro de un contenedor seguro con recursos garantizados (memoria, espacio en disco, n\u00facleos de CPU, etc.) que no tienes que compartir con otros usuarios. Con el hosting VPS, tienes el mismo acceso de nivel ra\u00edz que si alquilaras un servidor dedicado, pero a un costo mucho m\u00e1s bajo. El VPS es una soluci\u00f3n m\u00e1s segura y estable que el hosting compartido, con el que no obtienes espacio de servidor dedicado. Sin embargo, es de menor escala y m\u00e1s barato que alquilar un servidor completo. El hosting VPS generalmente es elegido por los propietarios de sitios web que tienen un tr\u00e1fico de nivel medio que excede los l\u00edmites de los planes de hosting compartido pero que a\u00fan no necesitan los recursos de un servidor dedicado.","title":"\u00bfQu\u00e9 es un VPS?"},{"location":"debian_teoria/#conexion-mediante-ssh","text":"Aunque nuestra m\u00e1quina virtual est\u00e9 en nuestro ordenador, ya hemos dicho que estamos simulando un VPS remoto. Para conectarnos a una m\u00e1quina de forma remota y segura, la opci\u00f3n m\u00e1s recomendable es SSH. SSH o Secure Shell es un protocolo de red criptogr\u00e1fico para operar servicios de red de forma segura a trav\u00e9s de una red no protegida. Las aplicaciones t\u00edpicas incluyen l\u00ednea de comandos remota, inicio de sesi\u00f3n y ejecuci\u00f3n de comandos remota, pero cualquier servicio de red puede protegerse con SSH. SSH proporciona un canal seguro a trav\u00e9s de una red no segura mediante el uso de una arquitectura cliente-servidor , conectando una aplicaci\u00f3n cliente SSH con un servidor SSH. El puerto TCP est\u00e1ndar para SSH es 22 y se usa generalmente para acceder a sistemas operativos similares a Unix, pero tambi\u00e9n se puede usar en Microsoft Windows. Proporciona un mecanismo para autenticar un usuario remoto, transferir entradas desde el cliente al host y retransmitir la salida de vuelta al cliente. SSH tiene muchas aplicaciones diferentes: Gesti\u00f3n de servidores a los que no se puede acceder localmente Transferencia segura de archivos Creaci\u00f3n de copias de seguridad Conexi\u00f3n entre dos ordenadores con encriptaci\u00f3n de extremo a extremo Mantenimiento remoto desde otros ordenadores","title":"Conexi\u00f3n mediante SSH"},{"location":"debian_teoria/#autenticacion","text":"Los dos m\u00e9todos de autenticaci\u00f3n de usuario SSH m\u00e1s comunes que se utilizan son las contrase\u00f1as (cifrado sim\u00e9trico) y las claves SSH (cifrado asim\u00e9trico o de clave p\u00fablica). Los clientes env\u00edan contrase\u00f1as cifradas al servidor de forma segura. Sin embargo, las contrase\u00f1as son un m\u00e9todo de autenticaci\u00f3n arriesgado porque su solidez depende de que el usuario sepa qu\u00e9 hace que una contrase\u00f1a sea segura. Los pares de claves p\u00fablica-privada SSH encriptados asim\u00e9tricamente son una mejor opci\u00f3n. Una vez que el cliente descifra el mensaje, el servidor le otorga acceso al sistema. Es decir, SSH opta por el cifrado h\u00edbrido, donde se utiliza el cifrado asim\u00e9trico para intercambiar unas claves que ser\u00e1n las que se utilizar\u00e1n posteriormente en el intercambio de informaci\u00f3n. Este tipo de cifrado utiliza la misma clave para cifrar y para descifrar la informaci\u00f3n. Por este motivo, la clave debe ser secreta y s\u00f3lo conocida por el emisor y el receptor del mensaje.","title":"Autenticaci\u00f3n"},{"location":"debian_teoria/#cifrados-simetricos-o-de-clave-privada","text":"Este tipo de cifrado utiliza la misma clave para cifrar y para descifrar la informaci\u00f3n. Por este motivo, la clave debe ser secreta y s\u00f3lo conocida por el emisor y el receptor del mensaje. Ventajas Muy r\u00e1pidos \u2192 cifrar y descifrar un mensaje cada vez requiere un cierto tiempo, que si el algoritmo es complejo, puede ser elevado. Inconvenientes Si alguien no autorizado consigue la clave, podr\u00e1 espiar la comunicaci\u00f3n sin problemas \u00bfC\u00f3mo hacemos para que emisor y receptar conozcan la clave en un primer momento? \u2192 no se puede transmitir por el canal inseguro \u2192 hay que transmitirla por otro canal seguro Ejemplos: PIN de la tarjeta del banco o archivo comprimido con contrase\u00f1a","title":"Cifrados sim\u00e9tricos o de clave privada"},{"location":"debian_teoria/#cifrados-asimetricos-o-de-clave-publica","text":"En este tipo de cifrados cada usuario utiliza un par de claves: una clave p\u00fablica y una clave privada. Un mensaje cifrado con la clave p\u00fablica s\u00f3lo se puede descifrar con su correspondiente clave privada y viceversa. La clave p\u00fablica es accesible a cualquier persona que quiera consultarla, no hace falta que sea transmitida por un canal seguro como en el caso anterior. La clave privada s\u00f3lo la debe conocer su due\u00f1o. Funcionamiento: El emisor cifra un mensaje con la clave p\u00fablica del receptor El receptor recibe el mensaje y es el \u00fanico que podr\u00e1 descifrarlo porque es el \u00fanico que posee la clave cifrada asociada Ventajas No se necesita un nuevo canal independiente y seguro para transmitir la clave Inconvenientes Son m\u00e1s lentos que los cifrados sim\u00e9tricos Hay que proteger muy bien la clave privada y tenerla siempre disponible para poder descifrar los mensajes (no es una contrase\u00f1a) Hay que asegurarse de que la clave p\u00fablica es de qui\u00e9n dice ser y no de un impostor que se est\u00e9 haciendo pasar por \u00e9l Nota Nosotros, para conectarnos por primera vez por SSH y comprobar la conectividad, utilizaremos el cifrado sim\u00e9trico (una contrase\u00f1a). Tras ello, simulando un entorno real que aporte comodidad (no introducir contrase\u00f1a cada vez que hagamos login ) pero tambi\u00e9n y sobre todo, por seguridad, utilizaremos cifrado asim\u00e9trico. Esto es, un par de claves.","title":"Cifrados asim\u00e9tricos o de clave p\u00fablica"},{"location":"debian_teoria/#referencias","text":"\u00bfQu\u00e9 es un VPS? Todo lo que necesitas saber sobre servidores virtuales","title":"Referencias"},{"location":"dns/","text":"Servicio DNS (Domain Name System) Introducci\u00f3n El sistema de nombres de dominio DNS (Domain Name System) proporciona un mecanismo eficaz para llevar a cabo la resoluci\u00f3n de nombres de dominio a direcciones IP. Como usuarios (humanos) nos es m\u00e1s f\u00e1cil dirigirnos a un nombre de dominio (de host, de web, de servidor de correo, etc.) utilizando un texto identificativo (por ejemplo, www.gva.es) que a la direcci\u00f3n IP pertinente (por ejemplo, 193.144.127.85). el servicio DNS no s\u00f3lo permite hacer la resoluci\u00f3n de nombres de dominio a direcciones IP, sino tambi\u00e9n la resoluci\u00f3n inversa. Es decir, a partir de una IP averiguar el nombre de dominio. El servicio DNS proporciona independencia del nombre de dominio respecto a la IP. As\u00ed un dominio puede cambiar de IP de forma transparente para los usuarios del dominio. Incluso es usual que un dominio se identifique con m\u00e1s de una IP como medida de redundancia contra la ca\u00edda del sistema o como balanceo de cargas. Otros servicios proporcionados por el DNS son la identificaci\u00f3n de los servidores de correo de un dominio, de cada uno de los hosts que pertenecen a la red, servidores de impresi\u00f3n, etc. Sistemas de nombres planos y jer\u00e1rquicos El problema de la identificaci\u00f3n de equipos se produce desde el principio de la existencia de las redes de ordenadores y no es algo espec\u00edfico de TCP/IP. Hac\u00eda falta un lenguaje humano para realizar esta identificaci\u00f3n. En los albores de las redes, cuando ARPANET (la red predecesora de Internet), los nombres los equipos se centralizaban en un archivo llamado host.txt (/etc/hosts en Linux), que inclu\u00eda el nombre del equipo y su IP. Esto es lo que se conoce como un sistema de nombres plano . Puede ser adecuado para redes peque\u00f1as pero no es escalable ni pr\u00e1ctico en redes grandes y mucho menos en Internet. Ejemplo de fichero de nombres plano: Elementos del sistema de nombres de dominio El espacio de nombres de dominio est\u00e1 formado por los nombres v\u00e1lidos utilizados para identificar servicios o m\u00e1quinas en una red. Se puede representar mediante una estructura jer\u00e1rquica de topolog\u00eda arb\u00f3rea , es decir, todos los nombres forman un \u00e1rbol invertido donde cada nodo se separa de los otros nodos por un punto . . Nombres de dominio Los nombres de dominio pueden estar formados por una o m\u00e1s cadenas de caracteres separadas por puntos y no se distingue entre may\u00fasculas y min\u00fasculas. Por ejemplo, www.deaw.es. es lo mismo que WWW.deaw.ES.. Los nombres de dominio se expresan como secuencias de etiquetas (labels) . Dominios ra\u00edz En teor\u00eda, todos los dominios deben de terminar con un punto (.). Es as\u00ed porque el \u00e1rbol de nombres de dominio (espacio de nombres de dominio) empieza con el dominio . que se conoce como dominio ra\u00edz (root) . En realidad es un elemento nulo de 0 caracteres que se representa con un punto (.). Un dominio se lee de derecha a izquierda, empezando por el punto . , aunque en la pr\u00e1ctica lo hacemos de izquierda a derecha. El punto inicial, generalmente se omite ya que los programas lo a\u00f1aden por defecto y es meramente formal, pero en ocasiones, ser\u00e1 necesario que indiquemos el nombre de dominio completo incluyendo el dominio ra\u00edz , es lo que se conoce como nombres de dominio completos ( Fully Qualified Domain Names, FQDN ). Dominios y subdominios Como consecuencia de la organizaci\u00f3n jer\u00e1rquica del espacio de nombres de dominios, podemos utilizar los t\u00e9rminos dominio y subdominio. Por ejemplo, deaw.es. es un subdominio del dominio es. y www.deaw.es . es un subdominio del dominio deaw.es. . Los dominios o subdominios que cuelgan del dominio ra\u00edz . se conocen como dominios de primer nivel o dominios de nivel superior (Top Level Domains, TLD), los que cuelgan de los dominios TLD se denominan dominios de segundo nivel y as\u00ed sucesivamente Zonas Una zona es una porci\u00f3n del espacio del espacio de nombre de dominio en el DNS cuya responsabilidad administrativa recae sobre un \u00fanico responsable. Los servidores que gestionan la zona tienen informaci\u00f3n completa sobre ella y se dice que son autorizados para esa zona. Las zonas se almacenan en archivos de texto o en bases de datos , seg\u00fan el tipo de software que se utilice para montar el servidor DNS y de como se configure. Tomemos como ejemplo el dominio deaw.es. y veamos parte de su archivo de zona ... deaw.es. IN NS ns1.deaw.es. ns1.deaw.es. IN A 192.168.1.20 natos.deaw.es. IN A 192.168.1.21 waor.deaw.es. IN A 192.168.1.22 www.deaw.es. IN CNAME natos.deaw.es. ftp.deaw.es. IN CNAME waor.deaw.es. ... A cada una de las l\u00edneas del fichero se las conoce como registros de recurso (RR: Resource Records) y definen los tipos de datos en el Domain Name System (DNS) . Se utilizan para almacenar datos sobre nombres de dominio y direcciones IP. Una base de datos o fichero de zona est\u00e1 formada por una serie de registros de recursos. Cada registro de recurso da informaci\u00f3n pertinente sobre un objeto determinado. Por ejemplo, los registros de tipo (A) asocian un nombre de host con una direcci\u00f3n IP, y los registros de puntero de b\u00fasqueda inversa (PTR) asocian una direcci\u00f3n IP con un nombre de host y un registro (NS) define un servidor DNS para la zona. El servidor DNS utiliza estos registros de recurso para resolver las consultas de los hosts de su zona. Cuando un servidor DNS es autorizado para una zona, es el responsable de los nombres de dominio para esa zona. En nuestro ejemplo, ns1.deaw.es es el servidor autorizado para la zona deaw.es. y en \u00e9l se definen los nombres que cuelgan de deaw.es como por ejemplo, www.deaw.es, ftp.deaw.es, natos.deaw.es, etc. La organizaci\u00f3n que administra el servidor DNS y por lo tanto la zona, puede delegar o no alguno de sus subdominios. Supongamos que de deaw.es. cuelgan los subdominios teoria.deaw.es. y practicas.deaw.es. y se decide delegar solo el subdominio practicas.deaw.es.. Esto implica que existir\u00e1 otro servidor DNS autorizado para el dominio practicas.deaw.es., que almacenar\u00e1 el fichero de zona para dicho dominio. Una zona no es lo mismo que un dominio. Un dominio es un subarbol del espacio de nombres de dominio y los datos asociados a los nombres de un dominio pueden estar almacenados en una o varias zonas , distribuidas en uno o varios servidores DNS . Info B\u00e1sicamente una zona es una porci\u00f3n de un dominio. Un servidor DNS puede ser autorizado sobre varias zonas , por ejemplo, el mismo servidor DNS puede ser autorizado para la zona deaw.es. y para la zona seguridadinformatica.es.. Tipos de RR (Resource Record) En esta subsecci\u00f3n vamos a ver cu\u00e1les son los registros de recursos o RR m\u00e1s utilizados. Antes debemos aclarar algunos conceptos: $TTL (Time To Live) El TTL o tiempo de vida determina, en segundos, durante cu\u00e1nto tiempo son validos los RR. Pueden indicarse en semanas ($TTL 1W), d\u00edas ($TTL 7D), horas ($TTL 168H) o minutos (10080M). En otras palabras, el TTL indica cu\u00e1nto tiempo tardar\u00e1n en aplicarse los cambios que le hagamos a un RR desde que los hacemos. En el ejemplo del p\u00e1rrafo anterior, los servidores DNS comprobar\u00e1n cada semana si se ha producido alg\u00fan cambio en esos RR. Debe declararse al inicio del archivo de zona. $ORIGIN La directiva $ORIGIN define el nombre del dominio que ser\u00e1 a\u00f1adido al final de cualquier nombre que no acabe en punto (nombres relativos o no cualificados) en los RR, para as\u00ed transformarlos en nombres FQDN (fully qualified domain name). Si un nombre acaba en punto, se considera un nombre FQDN y no se utilizar\u00eda $ORIGIN. Su sintaxis o forma de escribirlo ser\u00e1: $ORIGIN nombre-dominio Por ejemplo: $ORIGIN deaw.es. ;A partir de aqu\u00ed se a\u00f1ade deaw.es. a todos los nombres relativos ... Formato general de los RR El formato con el que se introducen los RR en los archivos de zona es del siguiente estilo: Nombre de dominio [TTL] Clase Tipo Tipo-Dato As\u00ed por ejemplo, un RR quedar\u00eda tal que as\u00ed: profesor.deaw.es 7200 IN A 192.168.10.254 Tipos de registros Aclarados los puntos anteriores, ahora s\u00ed vamos a ver los principales tipos de registros: Registro SOA (Start Of Authority) : Especifica informaci\u00f3n autoritaria sobre una zona DNS, incluyendo el servidor de nombre primario, el email del administrador, el n\u00famero de serial o versi\u00f3n de la zona, y varios temporizadores. Ejemplo: deaw.es. IN SOA ns1.deaw.es. super.deaw.es. ( 20190425001 ; serial 604800 ; refresh (7 d\u00edas) 86400 ; retry (1 d\u00eda) 2419200 ; expire (28 d\u00edas) 604800 ) ; TTL negativo (7 d\u00edas) ... Registro NS (Name Server) :Cuando se delega la administraci\u00f3n de subdominios en otros servidores, este registro indica cu\u00e1les son esos servidores autorizados. ... deaw.es. IN NS ns1.deaw.es. ;Servidor DNS maestro deaw.es. IN NS ns2.deaw.es. ;Servidor DNS esclavo deaw.es. IN NS dns.deaw.net. ;Servidor DNS esclavo ns1.deaw.es. IN A 192.168.10.20 ns2.deaw.es. IN A 192.168.10.21 ;DELEGACI\u00d3N practicas.deaw.es. IN NS ns1.practicas.deaw.es. redes.deaw.es. IN NS dns.deaw.net. El registro A (Address) , tambi\u00e9n conocido como registro de direcci\u00f3n, establece una correspondencia entre un nombre de dominio completamente cualificado (FQDN) y una direcci\u00f3n IP versi\u00f3n 4. ... ns1.deaw.es. IN A 192.168.10.20 ns2.deaw.es. IN A 192.168.10.21 natos.deaw.es. IN A 192.168.10.22 ... El registro CNAME (Canonical Name) permite crear alias para nombres de dominio especificados en registros A. ... natos.deaw.es. IN A 192.168.1.22 www.deaw.es. IN CNAME natos.deaw.es. ftp.deaw.es. IN CNAME natos.deaw.es. ... Un registro CNAME tambi\u00e9n puede apuntar a un nombre de otro dominio. ... www.deaw.es. IN CNAME www.deaw.com. ... El registro MX (Mail Exchange) permite definir los servidores encargados de la entrega de correo en el dominio y la prioridad entre ellos. Su sint\u00e1xis es la siguiente: ... deaw.es. IN MX 10 mail1.deaw.es. deaw.es. IN MX 20 mail2.deaw.es. mail1.deaw.es. IN A 192.168.1.100 mail2.deaw.es. IN A 192.168.1.101 ... El registro PTR (Pointer Record) establece una correspondencia entre direcciones IPv4 e IPv6 y nombres de dominio. Se utilizan en las zonas de resoluci\u00f3n inversa. En el caso de un bloque IPv4 de prefijo /24 , por ejemplo el 192.168.1.0/24 , los registros PTR ser\u00edan los siguientes: ... 20.1.168.192.in-addr.arpa. IN PTR ns1.deaw.es. 21.1.168.192.in-addr.arpa. IN PTR ns2.deaw.es. 22.1.168.192.in-addr.arpa. IN PTR natos.deaw.es. ... o lo que es lo mismo: ... 20 IN PTR ns1.deaw.es. 21 IN PTR ns2.deaw.es. 22 IN PTR natos.deaw.es. ... El registro TXT (plaint text) permite asociar informaci\u00f3n adicional a un dominio mediante m\u00faltiples cadenas de texto, con una longitud m\u00e1xima de 255 caracteres cada una de ellas. Por ejemplo, utilizado para almacenar claves de cifrado. ... @ IN TXT \"Servidor maestro de Servicios en Red\" @ IN TXT \"Servidor maestro de Servicios en Red\" Tipos de servidores DNS Servidor maestro o primario Un servidor maestro o primario, define una o varias zonas de las que es autorizado. Sus archivos de zona son de lectura y escritura y es en ellos donde el administrador del servidor a\u00f1ade, modifica o elimina nombres de dominio. Si un cliente DNS u otro servidor DNS le pregunta por alg\u00fan nombre de dominio para el que es autorizado , consulta con los ficheros de zona y responde a la pregunta. Si un cliente DNS u otro servidor DNS le pregunta por alg\u00fan nombre de dominio para el que no es autorizado , tendr\u00e1 que preguntar a otros servidores DNS o responder que no conoce la respuesta. Servidor esclavo o secundario Un servidor esclavo o secundario define una o varias zonas para las que es autorizado. La diferencia con respecto a un servidor maestro es que los ficheros de zona los obtiene de otro servidor autorizado para la zona, normalmente, de un servidor maestro mediante un procedimiento denominado transferencia de zona. Los ficheros de zona de los servidores esclavos son de solo lectura y por lo tanto, el administrador no tiene que editarlos. La modificaci\u00f3n de los archivos de zona debe realizarla el servidor maestro que transfiere la zona. El funcionamiento de como responden a los clientes DNS o a otros servidores DNS es similar al de un servidor maestro. Un servidor puede ser maestro para una o varias zonas y al mismo tiempo ser esclavo para otras. Pueden existir varios servidores esclavos para una misma zona . Las razones para esto suelen ser: Reducir y repartir la carga entre varios servidores DNS. Favorecer la tolerancia a fallos. Ofrecer mayor rapidez. Lo ideal es que los servidores DNS para una misma zona est\u00e9n ubicados en redes y localizaciones diferentes para evitar que, si ocurre alg\u00fan problema no les afecte simult\u00e1neamente y deje sin servicio de resoluci\u00f3n a los nombres de esa zona. Servidor cach\u00e9 Los servidores DNS se configuran como servidores cache para mejorar los tiempos de respuesta de las consultas, reducir la carga de los equipos y disminuir el tr\u00e1fico de red. Cuando un servidor DNS recibe una pregunta sobre un dominio para el cual no es autorizado, es decir, de un nombre del cual no tiene informaci\u00f3n, puede preguntar, si as\u00ed est\u00e1 configurado, a otros servidores para obtener la respuesta. Si el servidor act\u00faa como cache, guarda durante un tiempo (TTL: Time To Live) las respuestas a las \u00faltimas preguntas que ha realizado a otros servidores DNS. Cada vez que un cliente DNS u otro servidor DNS le formula una pregunta, comprueba si tiene la respuesta en su memoria cache, si la tiene, no tendr\u00e1 que preguntar a otro servidor DNS por la pregunta. Un servidor DNS es solo cache (cache only server) cuando: No tiene autoridad sobre ninguna zona. Pregunta a otros servidores DNS para resolver las preguntas de los clientes DNS y las guarda en su memoria cache. En el siguiente gr\u00e1fico se explica como dos clientes DNS hacen preguntas a un mismo servidor DNS que es autorizado para algunas zonas y adem\u00e1s act\u00faa como cach\u00e9. Servidor forwarder (reenviador) Cuando a un servidor DNS se le hace una pregunta sobre un nombre de dominio del que no dispone informaci\u00f3n (no es autorizado), este puede preguntar a otros servidores DNS. Simplificando, existen dos formas de procesar las consultas: El servidor DNS procesa la consulta preguntando a diversos servidores DNS y empezando por los servidores DNS ra\u00edz. Consulta iterativa. El servidor DNS reenv\u00eda la consulta a otro servidor DNS, denominado reenviador (forwarder), para que se encargue de resolverla. Consulta recursiva. Visto lo anterior, un reenviador (forwarder) es un servidor DNS que otros servidores DNS designan para reenviarle consultas. Son utilizados para minimizar las consultas y el tr\u00e1fico de peticiones DNS desde una red hacia Internet. Adem\u00e1s permiten a los equipos locales utilizar su cache DNs para minimizar los tiempos de respuesta. Servidor s\u00f3lo autorizado Un Servidor solo autorizado (authoritative only) es aquel que es autorizado para una o varias zonas como servidor maestro y/o esclavo y no responde a preguntas que no sean relativas a sus zonas. Es decir, no tiene activada la recursividad, no es reenviador y no act\u00faa como cache. Servidores ra\u00edz En Internet existen un conjunto de servidores DNS autorizados para el dominio ra\u00edz . , conocidos como servidores ra\u00edz (root servers). Contienen el fichero de la zona . que contiene informaci\u00f3n sobre los servidores DNS autorizados para cada uno de los dominios TLD. Los servidores ra\u00edz son una parte fundamental de Internet, son el primer paso en la traducci\u00f3n (resoluci\u00f3n) de los nombres de host en direcciones IP, que se utilizan en la comunicaci\u00f3n entre los hosts de Internet. Son claves en el proceso de resoluci\u00f3n de nombres de dominio en Internet, y deben de ser conocidos por todos los servidores DNS que respondan a preguntas sobre nombres para los que no son autorizados. Existen 13 servidores ra\u00edz en toda Internet y cada uno de ellos tiene m\u00faltiples copias distribuidas por todo el mundo, es decir, que f\u00edsicamente no solo son 13 servidores. Cada conjunto de copias de uno de los 13 servidores se identifica por una misma IP. Cuando un cliente realiza una pregunta a una IP de un servidor ra\u00edz, los routers de Internet encaminan la pregunta hacia la copia m\u00e1s cercana mediante un procedimiento denominado anycasting . Los nombres de los servidores ra\u00edz son de la forma letra.root-servers.net, donde letra va desde la A a la M. Listado de Servidores ra\u00edz Tipos de consultas: recursivas e iterativas Consultas recursivas Una consulta recursiva es aquella en la que el servidor DNS da una respuesta completa o exacta. Pueden darse tres tipos de respuesta: Positivas: se devuelve informaci\u00f3n sobre el dominio consultado Negativas: no se puede resolver el nombre de dominio Error: debido a un fallo en la red Consultas iterativas Una consulta iterativa es aquella en la que el servidor DNS proporciona una respuesta parcial. Existen cuatro posibles respuestas: Positivas: se devuelve informaci\u00f3n sobre el dominio consultado Negativas: no se puede resolver el nombre de dominio Referencia: el servidor DNS indica a otros servidores a los que se le puede consultar para resolver la pregunta Error: debido a un fallo en la red Ejemplos Completando la informaci\u00f3n de la imagen del primer ejemplo del apartado del reenviador forwarder : Completando la informaci\u00f3n de la imagen del segundo ejemplo del apartado del reenviador forwarder : Resoluci\u00f3n inversa La resoluci\u00f3n inversa consiste en obtener informaci\u00f3n de un nombre de dominio preguntando por la direcci\u00f3n IP en vez de preguntar por el nombre de domino como hemos explicado en apartados anteriores. Mapeo de direcciones y el dominio arpa El funcionamiento de la resoluci\u00f3n de direcciones IP es igual al de la resoluci\u00f3n de nombrres de dominio. Las direcciones IP se tratan como nombres que cuelgan del dominio in-addr.arpa para las direcciones IPv4, y del dominio ip6.arpa para las direcciones IPv6. Cuando usamos una direcci\u00f3n IP, por ejemplo 192.168.1.21 , para realizar una pregunta DNS inversa, en realidad estamos preguntando por el nombre de dominio 21.1.168.192.in-addr.arpa . La estructura jer\u00e1rquica de la direcci\u00f3n IP, tratada como nombre de dominio, es de derecha a izquierda, comenzando por el dominio in-addr.arpa . .arpa (Address and Routing Parameter Area) es un dominio de nivel superior gen\u00e9rico utilizado s\u00f3lo para la infraestructura de Internet. Los subdominios de .arpa o dominios de segundo nivel \u00abin-addr.arpa\u00bb e \u00abip6.arpa\u00bb son usados por los servidores DNS inversos para la obtenci\u00f3n de direcciones IPv4 e IPv6 respectivamente. Cuando mapeamos una direcci\u00f3n IP estamos asociando la direcci\u00f3n IP al nombre en el dominio .arpa. Por ejemplo la direcci\u00f3n 192.168.1.21 es mapeada al nombre 21.1.168.192.in-addr.arpa . Zonas de resoluci\u00f3n inversa Los servidores DNS almacenan zonas de resoluci\u00f3n inversa con registros de recursos (RR) que asocien nombres de dominio con direcciones IP. Las zonas de resoluci\u00f3n inversa pueden ser maestras o primarias y esclavas o secundarias. Las zonas de resoluci\u00f3n directa e inversa son independientes y es responsabilidad de los administradores de los servidores DNS que dichas zonas contengan informaci\u00f3n coherente y que no existan discrepancias. No es obligatorio que la entidad que administra una zona de resoluci\u00f3n directa de un dominio tenga que administrar la zona de resoluci\u00f3n inversa que se corresponda con las direcciones IPs asociadas a dicho dominio. ... deaw.es. IN NS ns1.deaw.es. ns1.deaw.es. IN A 192.168.1.20 natos.deaw.es. IN A 192.168.1.21 waor.deaw.es. IN A 192.168.1.22 altea.deaw.es. IN A 192.168.1.23 www.deaw.es. IN CNAME natos.deaw.es. ftp.deaw.es. IN CNAME waor.deaw.es. ... Archivo de zona de resoluci\u00f3n directa del dominio deaw.es. ... 1.168.192.in-addr.arpa. IN NS ns1.deaw.es. 20.1.168.192.in-addr.arpa. IN PTR ns1.deaw.es. 21.1.168.192.in-addr.arpa. IN PTR natos.deaw.es. 22.1.168.192.in-addr.arpa. IN PTR waor.deaw.es. 123.1.168.192.in-addr.arpa. IN PTR altea.deaw.es. ... Archivo de zona de resoluci\u00f3n inversa 1.168.192.in-addr.arpa que permite resolver consultas inversas sobre direcciones IP de la red 192.168.1.0/24 Proceso de resoluci\u00f3n El proceso de resoluci\u00f3n inversa es similar al de resoluci\u00f3n directa. Las direcciones IP se tratan como nombres de dominio. Por lo tanto, existen consultas recursivas, iterativas, cache, TTL... Por ejemplo, si un cliente DNS realiza una consulta recursiva de la IP 192.168.1.21 a un servidor DNS, \u00e9ste, si no lo tiene en cache, iniciar\u00e1 una serie de consultas iterativas a los servidores DNS ra\u00edz, a los servidores autorizados para el dominio 192.in-addr.arpa y as\u00ed sucesivamente. Herramientas Nslookup Es un programa para consultar servidores DNS. Se utiliza para saber si un servidor DNS resuelve correctamente los nombres DNS y las direcciones IP, para solucionar problemas frecuentes de los servidores DNS o, para diagnosticar problemas ocasionales de configuraci\u00f3n en los servidores DNS. Con nslookup podemos obtener la direcci\u00f3n IP asociada a un nombre DNS y viceversa, adem\u00e1s, podemos preguntar a los servidores de nombres informaci\u00f3n relativa a los registros de recursos (RR) de la/s zona/s de las que son autorizados. nslookup se usa de dos modos: interactivo y no interactivo. El modo interactivo permite al usuario consultar los servidores DNS para obtener informaci\u00f3n sobre varios hosts y dominios o para listar los hosts de un dominios. El modo no interactivo se usa para presentar solo el nombre y la informaci\u00f3n solicitada para un host o nombre DNS. Este comando funciona tanto en sistemas operativos UNIX/Linux como en Windows. En su momento se trat\u00f3 a nslookup como una aplicaci\u00f3n \u201cdeprecated\u201d u obsoleta, pero a d\u00eda de hoy parece que ha vuelto a considerarse apta para su uso normal. Dig Es un programa utilizado para preguntar a los servidores DNS. Herramienta utilizada para solucionar problemas de DNS gracias a su flexibilidad, facilidad de uso y claridad en la presentaci\u00f3n de la informaci\u00f3n. Normalmente, dig se usa pas\u00e1ndole argumentos desde la l\u00ednea de comandos (CLI), pero tambi\u00e9n tiene un modo de operar por lotes, leyendo las consultas desde un archivo. Este comando funciona tanto en sistemas operativos UNIX/Linux como en Windows Host Host es una herramienta CLI sencilla y f\u00e1cil de usar para realizar consultas DNS, que traducen nombres de dominio a direcciones IP y viceversa. Tambi\u00e9n se utiliza para consultar los registros DNS de las zonas que almacenan los servidores DNS, probar y validar el servidor DNS y la conectividad a Internet, registros de correo no deseado y listas negras, diagn\u00f3stico de problemas en el servidor DNS... Whois Aunque no es una herramienta de diagn\u00f3stico DNS si que nos ofrece informaci\u00f3n sobre el registro del dominio. Whois es un protocolo que permite realizar consultas a bases de datos que contienen informaci\u00f3n; del usuario, empresa u organizaci\u00f3n que registra un nombre de dominio y/o una direcci\u00f3n IP en Internet. El protocolo whois se encapsula en TCP y solo especifica el intercambio de peticiones y respuestas, no el formato de datos a intercambiar. Por eso, los resultados de las consultas whois pueden variar dependiendo de la base de datos whois a la que se pregunte. Referencias","title":"Tema 4 - Servicios de red implicados en el despliegue de aplicaciones web"},{"location":"dns/#servicio-dns-domain-name-system","text":"","title":"Servicio DNS (Domain Name System)"},{"location":"dns/#introduccion","text":"El sistema de nombres de dominio DNS (Domain Name System) proporciona un mecanismo eficaz para llevar a cabo la resoluci\u00f3n de nombres de dominio a direcciones IP. Como usuarios (humanos) nos es m\u00e1s f\u00e1cil dirigirnos a un nombre de dominio (de host, de web, de servidor de correo, etc.) utilizando un texto identificativo (por ejemplo, www.gva.es) que a la direcci\u00f3n IP pertinente (por ejemplo, 193.144.127.85). el servicio DNS no s\u00f3lo permite hacer la resoluci\u00f3n de nombres de dominio a direcciones IP, sino tambi\u00e9n la resoluci\u00f3n inversa. Es decir, a partir de una IP averiguar el nombre de dominio. El servicio DNS proporciona independencia del nombre de dominio respecto a la IP. As\u00ed un dominio puede cambiar de IP de forma transparente para los usuarios del dominio. Incluso es usual que un dominio se identifique con m\u00e1s de una IP como medida de redundancia contra la ca\u00edda del sistema o como balanceo de cargas. Otros servicios proporcionados por el DNS son la identificaci\u00f3n de los servidores de correo de un dominio, de cada uno de los hosts que pertenecen a la red, servidores de impresi\u00f3n, etc.","title":"Introducci\u00f3n"},{"location":"dns/#sistemas-de-nombres-planos-y-jerarquicos","text":"El problema de la identificaci\u00f3n de equipos se produce desde el principio de la existencia de las redes de ordenadores y no es algo espec\u00edfico de TCP/IP. Hac\u00eda falta un lenguaje humano para realizar esta identificaci\u00f3n. En los albores de las redes, cuando ARPANET (la red predecesora de Internet), los nombres los equipos se centralizaban en un archivo llamado host.txt (/etc/hosts en Linux), que inclu\u00eda el nombre del equipo y su IP. Esto es lo que se conoce como un sistema de nombres plano . Puede ser adecuado para redes peque\u00f1as pero no es escalable ni pr\u00e1ctico en redes grandes y mucho menos en Internet. Ejemplo de fichero de nombres plano:","title":"Sistemas de nombres planos y jer\u00e1rquicos"},{"location":"dns/#elementos-del-sistema-de-nombres-de-dominio","text":"El espacio de nombres de dominio est\u00e1 formado por los nombres v\u00e1lidos utilizados para identificar servicios o m\u00e1quinas en una red. Se puede representar mediante una estructura jer\u00e1rquica de topolog\u00eda arb\u00f3rea , es decir, todos los nombres forman un \u00e1rbol invertido donde cada nodo se separa de los otros nodos por un punto . .","title":"Elementos del sistema de nombres de dominio"},{"location":"dns/#nombres-de-dominio","text":"Los nombres de dominio pueden estar formados por una o m\u00e1s cadenas de caracteres separadas por puntos y no se distingue entre may\u00fasculas y min\u00fasculas. Por ejemplo, www.deaw.es. es lo mismo que WWW.deaw.ES.. Los nombres de dominio se expresan como secuencias de etiquetas (labels) .","title":"Nombres de dominio"},{"location":"dns/#dominios-raiz","text":"En teor\u00eda, todos los dominios deben de terminar con un punto (.). Es as\u00ed porque el \u00e1rbol de nombres de dominio (espacio de nombres de dominio) empieza con el dominio . que se conoce como dominio ra\u00edz (root) . En realidad es un elemento nulo de 0 caracteres que se representa con un punto (.). Un dominio se lee de derecha a izquierda, empezando por el punto . , aunque en la pr\u00e1ctica lo hacemos de izquierda a derecha. El punto inicial, generalmente se omite ya que los programas lo a\u00f1aden por defecto y es meramente formal, pero en ocasiones, ser\u00e1 necesario que indiquemos el nombre de dominio completo incluyendo el dominio ra\u00edz , es lo que se conoce como nombres de dominio completos ( Fully Qualified Domain Names, FQDN ).","title":"Dominios ra\u00edz"},{"location":"dns/#dominios-y-subdominios","text":"Como consecuencia de la organizaci\u00f3n jer\u00e1rquica del espacio de nombres de dominios, podemos utilizar los t\u00e9rminos dominio y subdominio. Por ejemplo, deaw.es. es un subdominio del dominio es. y www.deaw.es . es un subdominio del dominio deaw.es. . Los dominios o subdominios que cuelgan del dominio ra\u00edz . se conocen como dominios de primer nivel o dominios de nivel superior (Top Level Domains, TLD), los que cuelgan de los dominios TLD se denominan dominios de segundo nivel y as\u00ed sucesivamente","title":"Dominios y subdominios"},{"location":"dns/#zonas","text":"Una zona es una porci\u00f3n del espacio del espacio de nombre de dominio en el DNS cuya responsabilidad administrativa recae sobre un \u00fanico responsable. Los servidores que gestionan la zona tienen informaci\u00f3n completa sobre ella y se dice que son autorizados para esa zona. Las zonas se almacenan en archivos de texto o en bases de datos , seg\u00fan el tipo de software que se utilice para montar el servidor DNS y de como se configure. Tomemos como ejemplo el dominio deaw.es. y veamos parte de su archivo de zona ... deaw.es. IN NS ns1.deaw.es. ns1.deaw.es. IN A 192.168.1.20 natos.deaw.es. IN A 192.168.1.21 waor.deaw.es. IN A 192.168.1.22 www.deaw.es. IN CNAME natos.deaw.es. ftp.deaw.es. IN CNAME waor.deaw.es. ... A cada una de las l\u00edneas del fichero se las conoce como registros de recurso (RR: Resource Records) y definen los tipos de datos en el Domain Name System (DNS) . Se utilizan para almacenar datos sobre nombres de dominio y direcciones IP. Una base de datos o fichero de zona est\u00e1 formada por una serie de registros de recursos. Cada registro de recurso da informaci\u00f3n pertinente sobre un objeto determinado. Por ejemplo, los registros de tipo (A) asocian un nombre de host con una direcci\u00f3n IP, y los registros de puntero de b\u00fasqueda inversa (PTR) asocian una direcci\u00f3n IP con un nombre de host y un registro (NS) define un servidor DNS para la zona. El servidor DNS utiliza estos registros de recurso para resolver las consultas de los hosts de su zona. Cuando un servidor DNS es autorizado para una zona, es el responsable de los nombres de dominio para esa zona. En nuestro ejemplo, ns1.deaw.es es el servidor autorizado para la zona deaw.es. y en \u00e9l se definen los nombres que cuelgan de deaw.es como por ejemplo, www.deaw.es, ftp.deaw.es, natos.deaw.es, etc. La organizaci\u00f3n que administra el servidor DNS y por lo tanto la zona, puede delegar o no alguno de sus subdominios. Supongamos que de deaw.es. cuelgan los subdominios teoria.deaw.es. y practicas.deaw.es. y se decide delegar solo el subdominio practicas.deaw.es.. Esto implica que existir\u00e1 otro servidor DNS autorizado para el dominio practicas.deaw.es., que almacenar\u00e1 el fichero de zona para dicho dominio. Una zona no es lo mismo que un dominio. Un dominio es un subarbol del espacio de nombres de dominio y los datos asociados a los nombres de un dominio pueden estar almacenados en una o varias zonas , distribuidas en uno o varios servidores DNS . Info B\u00e1sicamente una zona es una porci\u00f3n de un dominio. Un servidor DNS puede ser autorizado sobre varias zonas , por ejemplo, el mismo servidor DNS puede ser autorizado para la zona deaw.es. y para la zona seguridadinformatica.es..","title":"Zonas"},{"location":"dns/#tipos-de-rr-resource-record","text":"En esta subsecci\u00f3n vamos a ver cu\u00e1les son los registros de recursos o RR m\u00e1s utilizados. Antes debemos aclarar algunos conceptos:","title":"Tipos de RR (Resource Record)"},{"location":"dns/#ttl-time-to-live","text":"El TTL o tiempo de vida determina, en segundos, durante cu\u00e1nto tiempo son validos los RR. Pueden indicarse en semanas ($TTL 1W), d\u00edas ($TTL 7D), horas ($TTL 168H) o minutos (10080M). En otras palabras, el TTL indica cu\u00e1nto tiempo tardar\u00e1n en aplicarse los cambios que le hagamos a un RR desde que los hacemos. En el ejemplo del p\u00e1rrafo anterior, los servidores DNS comprobar\u00e1n cada semana si se ha producido alg\u00fan cambio en esos RR. Debe declararse al inicio del archivo de zona.","title":"$TTL (Time To Live)"},{"location":"dns/#origin","text":"La directiva $ORIGIN define el nombre del dominio que ser\u00e1 a\u00f1adido al final de cualquier nombre que no acabe en punto (nombres relativos o no cualificados) en los RR, para as\u00ed transformarlos en nombres FQDN (fully qualified domain name). Si un nombre acaba en punto, se considera un nombre FQDN y no se utilizar\u00eda $ORIGIN. Su sintaxis o forma de escribirlo ser\u00e1: $ORIGIN nombre-dominio Por ejemplo: $ORIGIN deaw.es. ;A partir de aqu\u00ed se a\u00f1ade deaw.es. a todos los nombres relativos ...","title":"$ORIGIN"},{"location":"dns/#formato-general-de-los-rr","text":"El formato con el que se introducen los RR en los archivos de zona es del siguiente estilo: Nombre de dominio [TTL] Clase Tipo Tipo-Dato As\u00ed por ejemplo, un RR quedar\u00eda tal que as\u00ed: profesor.deaw.es 7200 IN A 192.168.10.254","title":"Formato general de los RR"},{"location":"dns/#tipos-de-registros","text":"Aclarados los puntos anteriores, ahora s\u00ed vamos a ver los principales tipos de registros: Registro SOA (Start Of Authority) : Especifica informaci\u00f3n autoritaria sobre una zona DNS, incluyendo el servidor de nombre primario, el email del administrador, el n\u00famero de serial o versi\u00f3n de la zona, y varios temporizadores. Ejemplo: deaw.es. IN SOA ns1.deaw.es. super.deaw.es. ( 20190425001 ; serial 604800 ; refresh (7 d\u00edas) 86400 ; retry (1 d\u00eda) 2419200 ; expire (28 d\u00edas) 604800 ) ; TTL negativo (7 d\u00edas) ... Registro NS (Name Server) :Cuando se delega la administraci\u00f3n de subdominios en otros servidores, este registro indica cu\u00e1les son esos servidores autorizados. ... deaw.es. IN NS ns1.deaw.es. ;Servidor DNS maestro deaw.es. IN NS ns2.deaw.es. ;Servidor DNS esclavo deaw.es. IN NS dns.deaw.net. ;Servidor DNS esclavo ns1.deaw.es. IN A 192.168.10.20 ns2.deaw.es. IN A 192.168.10.21 ;DELEGACI\u00d3N practicas.deaw.es. IN NS ns1.practicas.deaw.es. redes.deaw.es. IN NS dns.deaw.net. El registro A (Address) , tambi\u00e9n conocido como registro de direcci\u00f3n, establece una correspondencia entre un nombre de dominio completamente cualificado (FQDN) y una direcci\u00f3n IP versi\u00f3n 4. ... ns1.deaw.es. IN A 192.168.10.20 ns2.deaw.es. IN A 192.168.10.21 natos.deaw.es. IN A 192.168.10.22 ... El registro CNAME (Canonical Name) permite crear alias para nombres de dominio especificados en registros A. ... natos.deaw.es. IN A 192.168.1.22 www.deaw.es. IN CNAME natos.deaw.es. ftp.deaw.es. IN CNAME natos.deaw.es. ... Un registro CNAME tambi\u00e9n puede apuntar a un nombre de otro dominio. ... www.deaw.es. IN CNAME www.deaw.com. ... El registro MX (Mail Exchange) permite definir los servidores encargados de la entrega de correo en el dominio y la prioridad entre ellos. Su sint\u00e1xis es la siguiente: ... deaw.es. IN MX 10 mail1.deaw.es. deaw.es. IN MX 20 mail2.deaw.es. mail1.deaw.es. IN A 192.168.1.100 mail2.deaw.es. IN A 192.168.1.101 ... El registro PTR (Pointer Record) establece una correspondencia entre direcciones IPv4 e IPv6 y nombres de dominio. Se utilizan en las zonas de resoluci\u00f3n inversa. En el caso de un bloque IPv4 de prefijo /24 , por ejemplo el 192.168.1.0/24 , los registros PTR ser\u00edan los siguientes: ... 20.1.168.192.in-addr.arpa. IN PTR ns1.deaw.es. 21.1.168.192.in-addr.arpa. IN PTR ns2.deaw.es. 22.1.168.192.in-addr.arpa. IN PTR natos.deaw.es. ... o lo que es lo mismo: ... 20 IN PTR ns1.deaw.es. 21 IN PTR ns2.deaw.es. 22 IN PTR natos.deaw.es. ... El registro TXT (plaint text) permite asociar informaci\u00f3n adicional a un dominio mediante m\u00faltiples cadenas de texto, con una longitud m\u00e1xima de 255 caracteres cada una de ellas. Por ejemplo, utilizado para almacenar claves de cifrado. ... @ IN TXT \"Servidor maestro de Servicios en Red\" @ IN TXT \"Servidor maestro de Servicios en Red\"","title":"Tipos de registros"},{"location":"dns/#tipos-de-servidores-dns","text":"","title":"Tipos de servidores DNS"},{"location":"dns/#servidor-maestro-o-primario","text":"Un servidor maestro o primario, define una o varias zonas de las que es autorizado. Sus archivos de zona son de lectura y escritura y es en ellos donde el administrador del servidor a\u00f1ade, modifica o elimina nombres de dominio. Si un cliente DNS u otro servidor DNS le pregunta por alg\u00fan nombre de dominio para el que es autorizado , consulta con los ficheros de zona y responde a la pregunta. Si un cliente DNS u otro servidor DNS le pregunta por alg\u00fan nombre de dominio para el que no es autorizado , tendr\u00e1 que preguntar a otros servidores DNS o responder que no conoce la respuesta.","title":"Servidor maestro o primario"},{"location":"dns/#servidor-esclavo-o-secundario","text":"Un servidor esclavo o secundario define una o varias zonas para las que es autorizado. La diferencia con respecto a un servidor maestro es que los ficheros de zona los obtiene de otro servidor autorizado para la zona, normalmente, de un servidor maestro mediante un procedimiento denominado transferencia de zona. Los ficheros de zona de los servidores esclavos son de solo lectura y por lo tanto, el administrador no tiene que editarlos. La modificaci\u00f3n de los archivos de zona debe realizarla el servidor maestro que transfiere la zona. El funcionamiento de como responden a los clientes DNS o a otros servidores DNS es similar al de un servidor maestro. Un servidor puede ser maestro para una o varias zonas y al mismo tiempo ser esclavo para otras. Pueden existir varios servidores esclavos para una misma zona . Las razones para esto suelen ser: Reducir y repartir la carga entre varios servidores DNS. Favorecer la tolerancia a fallos. Ofrecer mayor rapidez. Lo ideal es que los servidores DNS para una misma zona est\u00e9n ubicados en redes y localizaciones diferentes para evitar que, si ocurre alg\u00fan problema no les afecte simult\u00e1neamente y deje sin servicio de resoluci\u00f3n a los nombres de esa zona.","title":"Servidor esclavo o secundario"},{"location":"dns/#servidor-cache","text":"Los servidores DNS se configuran como servidores cache para mejorar los tiempos de respuesta de las consultas, reducir la carga de los equipos y disminuir el tr\u00e1fico de red. Cuando un servidor DNS recibe una pregunta sobre un dominio para el cual no es autorizado, es decir, de un nombre del cual no tiene informaci\u00f3n, puede preguntar, si as\u00ed est\u00e1 configurado, a otros servidores para obtener la respuesta. Si el servidor act\u00faa como cache, guarda durante un tiempo (TTL: Time To Live) las respuestas a las \u00faltimas preguntas que ha realizado a otros servidores DNS. Cada vez que un cliente DNS u otro servidor DNS le formula una pregunta, comprueba si tiene la respuesta en su memoria cache, si la tiene, no tendr\u00e1 que preguntar a otro servidor DNS por la pregunta. Un servidor DNS es solo cache (cache only server) cuando: No tiene autoridad sobre ninguna zona. Pregunta a otros servidores DNS para resolver las preguntas de los clientes DNS y las guarda en su memoria cache. En el siguiente gr\u00e1fico se explica como dos clientes DNS hacen preguntas a un mismo servidor DNS que es autorizado para algunas zonas y adem\u00e1s act\u00faa como cach\u00e9.","title":"Servidor cach\u00e9"},{"location":"dns/#servidor-forwarder-reenviador","text":"Cuando a un servidor DNS se le hace una pregunta sobre un nombre de dominio del que no dispone informaci\u00f3n (no es autorizado), este puede preguntar a otros servidores DNS. Simplificando, existen dos formas de procesar las consultas: El servidor DNS procesa la consulta preguntando a diversos servidores DNS y empezando por los servidores DNS ra\u00edz. Consulta iterativa. El servidor DNS reenv\u00eda la consulta a otro servidor DNS, denominado reenviador (forwarder), para que se encargue de resolverla. Consulta recursiva. Visto lo anterior, un reenviador (forwarder) es un servidor DNS que otros servidores DNS designan para reenviarle consultas. Son utilizados para minimizar las consultas y el tr\u00e1fico de peticiones DNS desde una red hacia Internet. Adem\u00e1s permiten a los equipos locales utilizar su cache DNs para minimizar los tiempos de respuesta.","title":"Servidor forwarder (reenviador)"},{"location":"dns/#servidor-solo-autorizado","text":"Un Servidor solo autorizado (authoritative only) es aquel que es autorizado para una o varias zonas como servidor maestro y/o esclavo y no responde a preguntas que no sean relativas a sus zonas. Es decir, no tiene activada la recursividad, no es reenviador y no act\u00faa como cache.","title":"Servidor s\u00f3lo autorizado"},{"location":"dns/#servidores-raiz","text":"En Internet existen un conjunto de servidores DNS autorizados para el dominio ra\u00edz . , conocidos como servidores ra\u00edz (root servers). Contienen el fichero de la zona . que contiene informaci\u00f3n sobre los servidores DNS autorizados para cada uno de los dominios TLD. Los servidores ra\u00edz son una parte fundamental de Internet, son el primer paso en la traducci\u00f3n (resoluci\u00f3n) de los nombres de host en direcciones IP, que se utilizan en la comunicaci\u00f3n entre los hosts de Internet. Son claves en el proceso de resoluci\u00f3n de nombres de dominio en Internet, y deben de ser conocidos por todos los servidores DNS que respondan a preguntas sobre nombres para los que no son autorizados. Existen 13 servidores ra\u00edz en toda Internet y cada uno de ellos tiene m\u00faltiples copias distribuidas por todo el mundo, es decir, que f\u00edsicamente no solo son 13 servidores. Cada conjunto de copias de uno de los 13 servidores se identifica por una misma IP. Cuando un cliente realiza una pregunta a una IP de un servidor ra\u00edz, los routers de Internet encaminan la pregunta hacia la copia m\u00e1s cercana mediante un procedimiento denominado anycasting . Los nombres de los servidores ra\u00edz son de la forma letra.root-servers.net, donde letra va desde la A a la M. Listado de Servidores ra\u00edz","title":"Servidores ra\u00edz"},{"location":"dns/#tipos-de-consultas-recursivas-e-iterativas","text":"","title":"Tipos de consultas: recursivas e iterativas"},{"location":"dns/#consultas-recursivas","text":"Una consulta recursiva es aquella en la que el servidor DNS da una respuesta completa o exacta. Pueden darse tres tipos de respuesta: Positivas: se devuelve informaci\u00f3n sobre el dominio consultado Negativas: no se puede resolver el nombre de dominio Error: debido a un fallo en la red","title":"Consultas recursivas"},{"location":"dns/#consultas-iterativas","text":"Una consulta iterativa es aquella en la que el servidor DNS proporciona una respuesta parcial. Existen cuatro posibles respuestas: Positivas: se devuelve informaci\u00f3n sobre el dominio consultado Negativas: no se puede resolver el nombre de dominio Referencia: el servidor DNS indica a otros servidores a los que se le puede consultar para resolver la pregunta Error: debido a un fallo en la red","title":"Consultas iterativas"},{"location":"dns/#ejemplos","text":"Completando la informaci\u00f3n de la imagen del primer ejemplo del apartado del reenviador forwarder : Completando la informaci\u00f3n de la imagen del segundo ejemplo del apartado del reenviador forwarder :","title":"Ejemplos"},{"location":"dns/#resolucion-inversa","text":"La resoluci\u00f3n inversa consiste en obtener informaci\u00f3n de un nombre de dominio preguntando por la direcci\u00f3n IP en vez de preguntar por el nombre de domino como hemos explicado en apartados anteriores.","title":"Resoluci\u00f3n inversa"},{"location":"dns/#mapeo-de-direcciones-y-el-dominio-arpa","text":"El funcionamiento de la resoluci\u00f3n de direcciones IP es igual al de la resoluci\u00f3n de nombrres de dominio. Las direcciones IP se tratan como nombres que cuelgan del dominio in-addr.arpa para las direcciones IPv4, y del dominio ip6.arpa para las direcciones IPv6. Cuando usamos una direcci\u00f3n IP, por ejemplo 192.168.1.21 , para realizar una pregunta DNS inversa, en realidad estamos preguntando por el nombre de dominio 21.1.168.192.in-addr.arpa . La estructura jer\u00e1rquica de la direcci\u00f3n IP, tratada como nombre de dominio, es de derecha a izquierda, comenzando por el dominio in-addr.arpa . .arpa (Address and Routing Parameter Area) es un dominio de nivel superior gen\u00e9rico utilizado s\u00f3lo para la infraestructura de Internet. Los subdominios de .arpa o dominios de segundo nivel \u00abin-addr.arpa\u00bb e \u00abip6.arpa\u00bb son usados por los servidores DNS inversos para la obtenci\u00f3n de direcciones IPv4 e IPv6 respectivamente. Cuando mapeamos una direcci\u00f3n IP estamos asociando la direcci\u00f3n IP al nombre en el dominio .arpa. Por ejemplo la direcci\u00f3n 192.168.1.21 es mapeada al nombre 21.1.168.192.in-addr.arpa .","title":"Mapeo de direcciones y el dominio arpa"},{"location":"dns/#zonas-de-resolucion-inversa","text":"Los servidores DNS almacenan zonas de resoluci\u00f3n inversa con registros de recursos (RR) que asocien nombres de dominio con direcciones IP. Las zonas de resoluci\u00f3n inversa pueden ser maestras o primarias y esclavas o secundarias. Las zonas de resoluci\u00f3n directa e inversa son independientes y es responsabilidad de los administradores de los servidores DNS que dichas zonas contengan informaci\u00f3n coherente y que no existan discrepancias. No es obligatorio que la entidad que administra una zona de resoluci\u00f3n directa de un dominio tenga que administrar la zona de resoluci\u00f3n inversa que se corresponda con las direcciones IPs asociadas a dicho dominio. ... deaw.es. IN NS ns1.deaw.es. ns1.deaw.es. IN A 192.168.1.20 natos.deaw.es. IN A 192.168.1.21 waor.deaw.es. IN A 192.168.1.22 altea.deaw.es. IN A 192.168.1.23 www.deaw.es. IN CNAME natos.deaw.es. ftp.deaw.es. IN CNAME waor.deaw.es. ... Archivo de zona de resoluci\u00f3n directa del dominio deaw.es. ... 1.168.192.in-addr.arpa. IN NS ns1.deaw.es. 20.1.168.192.in-addr.arpa. IN PTR ns1.deaw.es. 21.1.168.192.in-addr.arpa. IN PTR natos.deaw.es. 22.1.168.192.in-addr.arpa. IN PTR waor.deaw.es. 123.1.168.192.in-addr.arpa. IN PTR altea.deaw.es. ... Archivo de zona de resoluci\u00f3n inversa 1.168.192.in-addr.arpa que permite resolver consultas inversas sobre direcciones IP de la red 192.168.1.0/24","title":"Zonas de resoluci\u00f3n inversa"},{"location":"dns/#proceso-de-resolucion","text":"El proceso de resoluci\u00f3n inversa es similar al de resoluci\u00f3n directa. Las direcciones IP se tratan como nombres de dominio. Por lo tanto, existen consultas recursivas, iterativas, cache, TTL... Por ejemplo, si un cliente DNS realiza una consulta recursiva de la IP 192.168.1.21 a un servidor DNS, \u00e9ste, si no lo tiene en cache, iniciar\u00e1 una serie de consultas iterativas a los servidores DNS ra\u00edz, a los servidores autorizados para el dominio 192.in-addr.arpa y as\u00ed sucesivamente.","title":"Proceso de resoluci\u00f3n"},{"location":"dns/#herramientas","text":"","title":"Herramientas"},{"location":"dns/#nslookup","text":"Es un programa para consultar servidores DNS. Se utiliza para saber si un servidor DNS resuelve correctamente los nombres DNS y las direcciones IP, para solucionar problemas frecuentes de los servidores DNS o, para diagnosticar problemas ocasionales de configuraci\u00f3n en los servidores DNS. Con nslookup podemos obtener la direcci\u00f3n IP asociada a un nombre DNS y viceversa, adem\u00e1s, podemos preguntar a los servidores de nombres informaci\u00f3n relativa a los registros de recursos (RR) de la/s zona/s de las que son autorizados. nslookup se usa de dos modos: interactivo y no interactivo. El modo interactivo permite al usuario consultar los servidores DNS para obtener informaci\u00f3n sobre varios hosts y dominios o para listar los hosts de un dominios. El modo no interactivo se usa para presentar solo el nombre y la informaci\u00f3n solicitada para un host o nombre DNS. Este comando funciona tanto en sistemas operativos UNIX/Linux como en Windows. En su momento se trat\u00f3 a nslookup como una aplicaci\u00f3n \u201cdeprecated\u201d u obsoleta, pero a d\u00eda de hoy parece que ha vuelto a considerarse apta para su uso normal.","title":"Nslookup"},{"location":"dns/#dig","text":"Es un programa utilizado para preguntar a los servidores DNS. Herramienta utilizada para solucionar problemas de DNS gracias a su flexibilidad, facilidad de uso y claridad en la presentaci\u00f3n de la informaci\u00f3n. Normalmente, dig se usa pas\u00e1ndole argumentos desde la l\u00ednea de comandos (CLI), pero tambi\u00e9n tiene un modo de operar por lotes, leyendo las consultas desde un archivo. Este comando funciona tanto en sistemas operativos UNIX/Linux como en Windows","title":"Dig"},{"location":"dns/#host","text":"Host es una herramienta CLI sencilla y f\u00e1cil de usar para realizar consultas DNS, que traducen nombres de dominio a direcciones IP y viceversa. Tambi\u00e9n se utiliza para consultar los registros DNS de las zonas que almacenan los servidores DNS, probar y validar el servidor DNS y la conectividad a Internet, registros de correo no deseado y listas negras, diagn\u00f3stico de problemas en el servidor DNS...","title":"Host"},{"location":"dns/#whois","text":"Aunque no es una herramienta de diagn\u00f3stico DNS si que nos ofrece informaci\u00f3n sobre el registro del dominio. Whois es un protocolo que permite realizar consultas a bases de datos que contienen informaci\u00f3n; del usuario, empresa u organizaci\u00f3n que registra un nombre de dominio y/o una direcci\u00f3n IP en Internet. El protocolo whois se encapsula en TCP y solo especifica el intercambio de peticiones y respuestas, no el formato de datos a intercambiar. Por eso, los resultados de las consultas whois pueden variar dependiendo de la base de datos whois a la que se pregunte.","title":"Whois"},{"location":"dns/#referencias","text":"","title":"Referencias"},{"location":"docker-compose/","text":"Levantar un WordPress con Docker Compose El cliente de Docker es engorroso para crear contenedores, as\u00ed como para crear el resto de objetos y vincularlos entre s\u00ed. Para automatizar la creaci\u00f3n, inicio y parada de un contenedor o un conjunto de ellos, Docker proporciona una herramiento llamada Docker Compose . Para esta parte vamos a detener y borrar lo que hemos creado: Example Borra el trabajo actual: docker container stop wordpress wordpress-db docker container rm wordpress wordpress-db docker volume rm wordpress-db Docker Compose Compose es una herramienta para definir y ejecutar aplicaciones multi-contenedor. Con un solo comando podremos crear e iniciar todos los servicios que necesitamos para nuestra aplicaci\u00f3n. Los casos de uso m\u00e1s habituales para docker-compose son: Entornos de desarrollo Entornos de testeo autom\u00e1ticos (integraci\u00f3n cont\u00ednua) Despliegue en host individuales (no clusters) Compose tiene comandos para manejar todo el ciclo de vida de nuestra aplicaci\u00f3n: Iniciar, detener y rehacer servicios. Ver el estado de los servicios. Visualizar los logs. Ejecutar un comando en un servicio. Creaci\u00f3n de contenedores automatizada En el mismo directorio donde est\u00e1bamos en el paso anterior ( ~/Sites/wordpress ), vamos a crear un fichero llamado docker-compose.yaml con el siguiente contenido: version : '3' services : db : image : mariadb:10.3.9 volumes : - data:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=secret - MYSQL_DATABASE=wordpress - MYSQL_USER=manager - MYSQL_PASSWORD=secret web : image : wordpress:4.9.8 depends_on : - db volumes : - ./target:/var/www/html environment : - WORDPRESS_DB_USER=manager - WORDPRESS_DB_PASSWORD=secret - WORDPRESS_DB_HOST=db ports : - 8080:80 volumes : data : Info YAML es un lenguaje de serializaci\u00f3n de datos dise\u00f1ado para ser le\u00eddo y escrito por personas. Se recomienda que sigas alg\u00fan tutorial para entender su formato: Aprende YAML en Y minutos . Los ficheros de Compose est\u00e1n divididos en tres secciones: services , volumes y networks ; y deben indicar un n\u00famero de versi\u00f3n. Nos permite realizar practicamente lo mismo que podemos hacer con el cliente de docker , pero de forma autom\u00e1tica. Note En este taller no entramos en el apartado de networks . Con este fichero podemos hacer lo mismo que hemos hecho en el cap\u00edtulo anterior, pero con la ventaja de describir todos nuestros requisitos en un solo archivo. Iniciar servicios Vamos a ejecutar esta aplicaci\u00f3n y luego procederemos a explicarla: Example Arranca la aplicaci\u00f3n con Compose : docker-compose up -d Cuando arrancamos la aplicaci\u00f3n, Compose nos informa de los servicios que ha ido levantando: $ docker-compose up -d Creating network \"wordpress_default\" with the default driver Creating volume \"wordpress_data\" with local driver Creating wordpress_db_1 ... Creating wordpress_db_1 ... done Creating wordpress_web_1 ... Creating wordpress_web_1 ... done El par\u00e1metro -d es similar al que hemos visto en docker run : nos permite levantar los servicios en segundo plano. Veamos los contenedores activos: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a07b5d4d3982 wordpress:4.9.8 \"docker.s\u2026\" 10 seconds ago Up 8 seconds 0.0.0.0:8080->80/tcp wordpress_web_1 d9204884cec5 mariadb:10.3.9 \"docker.s\u2026\" 11 seconds ago Up 10 seconds 3306/tcp wordpress_db_1 Tambi\u00e9n podemos ver los contenedores con Compose : $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------- wordpress_db_1 docker-entrypoint.sh mysqld Up 3306/tcp wordpress_web_1 docker-entrypoint.sh apach ... Up 0.0.0.0:8080->80/tcp Lo que tenemos que tener en cuenta es lo siguiente: docker-compose ps solo muestra informaci\u00f3n de los servicios que se define en docker-compose.yaml , mientras que docker muestra todos. Cuando creamos contenedores con docker sin indicar un nombre, por defecto asigna uno aleatorio; mientras que en Compose el prefijo es el nombre del directorio y el sufijo el nombre del servicio: wordpress _ db _1 . El n\u00famero indica el n\u00famero de instancia. Es posible levantar m\u00e1s de una instancia de un mismo servicio. Si accedemos a la direcci\u00f3n http://localhost:8080/ , veremos de nuevo la instalaci\u00f3n de WordPress. Detener servicios Podemos detener servicios con docker-compose stop Borrar servicios Podemos borrar servicios con docker-compose down Esto borra los contenedores, pero no los vol\u00famenes. As\u00ed que si hemos creado bien la aplicaci\u00f3n nuestros datos est\u00e1n a salvo. Si queremos borrar tambi\u00e9n los vol\u00famenes: docker-compose down -v Estructura de la configuraci\u00f3n Veamos la configuraci\u00f3n por partes: version : '3' Compose se actualiza a menudo, con lo que el archivo de configuraci\u00f3n va adquiriendo nuevas funcionalidades. La versi\u00f3n '3' (es una cadena, importante poner comillas) es la \u00faltima y para conocer todas sus caracter\u00edsticas mira la p\u00e1gina de referencia de la versi\u00f3n 3 de Compose . volumes : data : Ya hemos indicado que es importante guardar los datos vol\u00e1tiles de las aplicaciones en vol\u00famenes. En este caso hemos creado un volumen llamado data . Recordemos que Compose siempre a\u00f1ade como prefijo el nombre del directorio, con lo que el nombre real del volumen es wordpress_data . Podemos comprobarlo con el cliente de docker como hicimos en el cap\u00edtulo de vol\u00famenes: $ docker volume ls DRIVER VOLUME NAME local wordpress_data Nos saltamos la secci\u00f3n de redes ( networks ) y vamos a la secci\u00f3n de servicios, que son los contenedores que precisa o componen nuestra aplicaci\u00f3n. Primero la base de datos: services : db : image : mariadb:10.3.9 volumes : - data:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=secret - MYSQL_DATABASE=wordpress - MYSQL_USER=manager - MYSQL_PASSWORD=secret Despu\u00e9s de abrir la parte de servicios, el primer nivel indica el nombre del servicio db , que genera el contenedor wordpress_db . Lo que vemos a continuaci\u00f3n es lo mismo que hicimos en la secci\u00f3n anterior pero de forma parametrizada. Si recordamos, para levantar nuestra base de datos, indicamos la imagen (l\u00ednea 3), luego montamos los vol\u00famenes (l\u00ednea 4), y despu\u00e9s indicamos las variables de entorno que configuraban el contenedor (l\u00ednea 6). Es decir, lo anterior es equivalente, excepto por el nombre, a: $ docker run -d --name wordpress-db \\ --mount source = wordpress-db,target = /var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD = secret \\ -e MYSQL_DATABASE = wordpress \\ -e MYSQL_USER = manager \\ -e MYSQL_PASSWORD = secret mariadb:10.3.9 Y despu\u00e9s nuestro WordPress : services : web : image : wordpress:4.9.8 depends_on : - db volumes : - ./target:/var/www/html environment : - WORDPRESS_DB_USER=manager - WORDPRESS_DB_PASSWORD=secret - WORDPRESS_DB_HOST=db ports : - 8080:80 En este caso la equivalencia es al comando: $ docker run -d --name wordpress \\ --link wordpress-db:mysql \\ --mount type = bind,source = \" $( pwd ) \" /target,target = /var/www/html \\ -e WORDPRESS_DB_USER = manager \\ -e WORDPRESS_DB_PASSWORD = secret \\ -p 8080 :80 \\ wordpress:4.9.8 La equivalencia de los par\u00e1metros es la siguiente: par\u00e1metro Docker par\u00e1metro Composer --link depends_on --mount volumes -e environment -p, --publish ports image Note Si reiniciamos el ordenador, los contenedores estar\u00e1n detenidos (stop), podremos reiniciarlos con docker start o docker-compose start . Este es el comportamiento predeterminado y el que nos interesa en un entorno de desarrollo. Sin embargo, en otros entornos, o para casos concretos, igual queremos que un contenedor tenga el mismo estado en el que estaba antes de reiniciar la m\u00e1quina (iniciado o parado). Para eso usaremos el par\u00e1metro restart . En el caso de la base de datos de nuestro ejemplo, la configuraci\u00f3n quedar\u00eda como: services : db : image : mariadb:10.3.9 restart : unless-stopped volumes : - data:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=secret - MYSQL_DATABASE=wordpress - MYSQL_USER=manager - MYSQL_PASSWORD=secret El equivalente en la consola ser\u00eda: $ docker run -d --name wordpress-db \\ --restart unless-stopped --mount source = wordpress-db,target = /var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD = secret \\ -e MYSQL_DATABASE = wordpress \\ -e MYSQL_USER = manager \\ -e MYSQL_PASSWORD = secret mariadb:10.3.9 Otros valores son: no (por defecto), always y on-failure .","title":"Levantar un WordPress con Docker Compose"},{"location":"docker-compose/#levantar-un-wordpress-con-docker-compose","text":"El cliente de Docker es engorroso para crear contenedores, as\u00ed como para crear el resto de objetos y vincularlos entre s\u00ed. Para automatizar la creaci\u00f3n, inicio y parada de un contenedor o un conjunto de ellos, Docker proporciona una herramiento llamada Docker Compose . Para esta parte vamos a detener y borrar lo que hemos creado: Example Borra el trabajo actual: docker container stop wordpress wordpress-db docker container rm wordpress wordpress-db docker volume rm wordpress-db","title":"Levantar un WordPress con Docker Compose"},{"location":"docker-compose/#docker-compose","text":"Compose es una herramienta para definir y ejecutar aplicaciones multi-contenedor. Con un solo comando podremos crear e iniciar todos los servicios que necesitamos para nuestra aplicaci\u00f3n. Los casos de uso m\u00e1s habituales para docker-compose son: Entornos de desarrollo Entornos de testeo autom\u00e1ticos (integraci\u00f3n cont\u00ednua) Despliegue en host individuales (no clusters) Compose tiene comandos para manejar todo el ciclo de vida de nuestra aplicaci\u00f3n: Iniciar, detener y rehacer servicios. Ver el estado de los servicios. Visualizar los logs. Ejecutar un comando en un servicio.","title":"Docker Compose"},{"location":"docker-compose/#creacion-de-contenedores-automatizada","text":"En el mismo directorio donde est\u00e1bamos en el paso anterior ( ~/Sites/wordpress ), vamos a crear un fichero llamado docker-compose.yaml con el siguiente contenido: version : '3' services : db : image : mariadb:10.3.9 volumes : - data:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=secret - MYSQL_DATABASE=wordpress - MYSQL_USER=manager - MYSQL_PASSWORD=secret web : image : wordpress:4.9.8 depends_on : - db volumes : - ./target:/var/www/html environment : - WORDPRESS_DB_USER=manager - WORDPRESS_DB_PASSWORD=secret - WORDPRESS_DB_HOST=db ports : - 8080:80 volumes : data : Info YAML es un lenguaje de serializaci\u00f3n de datos dise\u00f1ado para ser le\u00eddo y escrito por personas. Se recomienda que sigas alg\u00fan tutorial para entender su formato: Aprende YAML en Y minutos . Los ficheros de Compose est\u00e1n divididos en tres secciones: services , volumes y networks ; y deben indicar un n\u00famero de versi\u00f3n. Nos permite realizar practicamente lo mismo que podemos hacer con el cliente de docker , pero de forma autom\u00e1tica. Note En este taller no entramos en el apartado de networks . Con este fichero podemos hacer lo mismo que hemos hecho en el cap\u00edtulo anterior, pero con la ventaja de describir todos nuestros requisitos en un solo archivo.","title":"Creaci\u00f3n de contenedores automatizada"},{"location":"docker-compose/#iniciar-servicios","text":"Vamos a ejecutar esta aplicaci\u00f3n y luego procederemos a explicarla: Example Arranca la aplicaci\u00f3n con Compose : docker-compose up -d Cuando arrancamos la aplicaci\u00f3n, Compose nos informa de los servicios que ha ido levantando: $ docker-compose up -d Creating network \"wordpress_default\" with the default driver Creating volume \"wordpress_data\" with local driver Creating wordpress_db_1 ... Creating wordpress_db_1 ... done Creating wordpress_web_1 ... Creating wordpress_web_1 ... done El par\u00e1metro -d es similar al que hemos visto en docker run : nos permite levantar los servicios en segundo plano. Veamos los contenedores activos: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a07b5d4d3982 wordpress:4.9.8 \"docker.s\u2026\" 10 seconds ago Up 8 seconds 0.0.0.0:8080->80/tcp wordpress_web_1 d9204884cec5 mariadb:10.3.9 \"docker.s\u2026\" 11 seconds ago Up 10 seconds 3306/tcp wordpress_db_1 Tambi\u00e9n podemos ver los contenedores con Compose : $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------- wordpress_db_1 docker-entrypoint.sh mysqld Up 3306/tcp wordpress_web_1 docker-entrypoint.sh apach ... Up 0.0.0.0:8080->80/tcp Lo que tenemos que tener en cuenta es lo siguiente: docker-compose ps solo muestra informaci\u00f3n de los servicios que se define en docker-compose.yaml , mientras que docker muestra todos. Cuando creamos contenedores con docker sin indicar un nombre, por defecto asigna uno aleatorio; mientras que en Compose el prefijo es el nombre del directorio y el sufijo el nombre del servicio: wordpress _ db _1 . El n\u00famero indica el n\u00famero de instancia. Es posible levantar m\u00e1s de una instancia de un mismo servicio. Si accedemos a la direcci\u00f3n http://localhost:8080/ , veremos de nuevo la instalaci\u00f3n de WordPress.","title":"Iniciar servicios"},{"location":"docker-compose/#detener-servicios","text":"Podemos detener servicios con docker-compose stop","title":"Detener servicios"},{"location":"docker-compose/#borrar-servicios","text":"Podemos borrar servicios con docker-compose down Esto borra los contenedores, pero no los vol\u00famenes. As\u00ed que si hemos creado bien la aplicaci\u00f3n nuestros datos est\u00e1n a salvo. Si queremos borrar tambi\u00e9n los vol\u00famenes: docker-compose down -v","title":"Borrar servicios"},{"location":"docker-compose/#estructura-de-la-configuracion","text":"Veamos la configuraci\u00f3n por partes: version : '3' Compose se actualiza a menudo, con lo que el archivo de configuraci\u00f3n va adquiriendo nuevas funcionalidades. La versi\u00f3n '3' (es una cadena, importante poner comillas) es la \u00faltima y para conocer todas sus caracter\u00edsticas mira la p\u00e1gina de referencia de la versi\u00f3n 3 de Compose . volumes : data : Ya hemos indicado que es importante guardar los datos vol\u00e1tiles de las aplicaciones en vol\u00famenes. En este caso hemos creado un volumen llamado data . Recordemos que Compose siempre a\u00f1ade como prefijo el nombre del directorio, con lo que el nombre real del volumen es wordpress_data . Podemos comprobarlo con el cliente de docker como hicimos en el cap\u00edtulo de vol\u00famenes: $ docker volume ls DRIVER VOLUME NAME local wordpress_data Nos saltamos la secci\u00f3n de redes ( networks ) y vamos a la secci\u00f3n de servicios, que son los contenedores que precisa o componen nuestra aplicaci\u00f3n. Primero la base de datos: services : db : image : mariadb:10.3.9 volumes : - data:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=secret - MYSQL_DATABASE=wordpress - MYSQL_USER=manager - MYSQL_PASSWORD=secret Despu\u00e9s de abrir la parte de servicios, el primer nivel indica el nombre del servicio db , que genera el contenedor wordpress_db . Lo que vemos a continuaci\u00f3n es lo mismo que hicimos en la secci\u00f3n anterior pero de forma parametrizada. Si recordamos, para levantar nuestra base de datos, indicamos la imagen (l\u00ednea 3), luego montamos los vol\u00famenes (l\u00ednea 4), y despu\u00e9s indicamos las variables de entorno que configuraban el contenedor (l\u00ednea 6). Es decir, lo anterior es equivalente, excepto por el nombre, a: $ docker run -d --name wordpress-db \\ --mount source = wordpress-db,target = /var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD = secret \\ -e MYSQL_DATABASE = wordpress \\ -e MYSQL_USER = manager \\ -e MYSQL_PASSWORD = secret mariadb:10.3.9 Y despu\u00e9s nuestro WordPress : services : web : image : wordpress:4.9.8 depends_on : - db volumes : - ./target:/var/www/html environment : - WORDPRESS_DB_USER=manager - WORDPRESS_DB_PASSWORD=secret - WORDPRESS_DB_HOST=db ports : - 8080:80 En este caso la equivalencia es al comando: $ docker run -d --name wordpress \\ --link wordpress-db:mysql \\ --mount type = bind,source = \" $( pwd ) \" /target,target = /var/www/html \\ -e WORDPRESS_DB_USER = manager \\ -e WORDPRESS_DB_PASSWORD = secret \\ -p 8080 :80 \\ wordpress:4.9.8 La equivalencia de los par\u00e1metros es la siguiente: par\u00e1metro Docker par\u00e1metro Composer --link depends_on --mount volumes -e environment -p, --publish ports image Note Si reiniciamos el ordenador, los contenedores estar\u00e1n detenidos (stop), podremos reiniciarlos con docker start o docker-compose start . Este es el comportamiento predeterminado y el que nos interesa en un entorno de desarrollo. Sin embargo, en otros entornos, o para casos concretos, igual queremos que un contenedor tenga el mismo estado en el que estaba antes de reiniciar la m\u00e1quina (iniciado o parado). Para eso usaremos el par\u00e1metro restart . En el caso de la base de datos de nuestro ejemplo, la configuraci\u00f3n quedar\u00eda como: services : db : image : mariadb:10.3.9 restart : unless-stopped volumes : - data:/var/lib/mysql environment : - MYSQL_ROOT_PASSWORD=secret - MYSQL_DATABASE=wordpress - MYSQL_USER=manager - MYSQL_PASSWORD=secret El equivalente en la consola ser\u00eda: $ docker run -d --name wordpress-db \\ --restart unless-stopped --mount source = wordpress-db,target = /var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD = secret \\ -e MYSQL_DATABASE = wordpress \\ -e MYSQL_USER = manager \\ -e MYSQL_PASSWORD = secret mariadb:10.3.9 Otros valores son: no (por defecto), always y on-failure .","title":"Estructura de la configuraci\u00f3n"},{"location":"dockerfile/","text":"Crear im\u00e1genes propias Ya hemos visto como usar im\u00e1genes de terceros para crear aplicaciones y servicios. Pero, \u00bfsi no hay ninguna imagen que tenga lo que queremos? \u00bfO si queremos hacer una imagen de nuestra aplicaci\u00f3n para distribuirla? Docker permite crear imagenes propias. Aunque podr\u00edamos hacerla partiendo de cero, es un esfuerzo que no tiene sentido. Existe ya im\u00e1genes base para crear las nuestras y es mucho m\u00e1s f\u00e1cil crear una imagen bas\u00e1ndose en otra que hacerlo todo nosotros. Podemos partir de una imagen base que parte de un lenguaje de programaci\u00f3n ( python , php ) o de alguna distribuci\u00f3n ( ubuntu , debian ). Mi primer Dockerfile Los Dockerfile son los archivos que contienen las instrucciones que crean las imagenes. Deben estar guardados dentro de un build context , es decir, un directorio. Este directorio es el que contiene todos los archivos necesarios para construir nuestra imagen, de ah\u00ed lo de build context . Creamos nuestro build context mkdir -p ~/Sites/hello-world cd ~/Sites/hello-world echo \"hello\" > hello Dentro de este directorio crearemos un archivo llamado Dockerfile con este contenido: FROM busybox COPY /hello / RUN cat /hello Directiva Explicaci\u00f3n FROM Indica la imagen base sobre la que se basa esta imagen COPY Copia un archivo del build context y lo guarda en la imagen RUN Ejecuta el comando indicado durante el proceso de creaci\u00f3n de imagen. Ahora para crear nuestra imagen usaremos docker build . docker build -t helloapp:v1 . El par\u00e1metro -t nos permite etiquetar la imagen con un nombre y una versi\u00f3n. El . indica que el build context es el directorio actual. El resultado de ejecutar lo anterior ser\u00eda: $ docker build -t helloapp:v1 . Sending build context to Docker daemon 3.072kB Step 1/3 : FROM busybox latest: Pulling from library/busybox 8c5a7da1afbc: Pull complete Digest: sha256:cb63aa0641a885f54de20f61d152187419e8f6b159ed11a251a09d115fdff9bd Status: Downloaded newer image for busybox:latest ---> e1ddd7948a1c Step 2/3 : COPY /hello / ---> 8a092965dbc9 Step 3/3 : RUN cat /hello ---> Running in 83b5498790ca hello Removing intermediate container 83b5498790ca ---> f738f117d4b6 Successfully built f738f117d4b6 Successfully tagged helloapp:v1 Y podremos ver que una nueva imagen est\u00e1 instalada en nuestro equipo: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE helloapp v1 f738f117d4b6 40 seconds ago 1.16MB Creando aplicaciones en contenedores Vamos a crear un aplicaci\u00f3n en python y la vamos a guardarla en un contenedor. Comenzamos creando un nuevo build context : mkdir -p ~/Sites/friendlyhello cd ~/Sites/friendlyhello El c\u00f3digo de la aplicaci\u00f3n es el siguiente, lo guardaremos en un archivo llamado app.py : from flask import Flask from redis import Redis , RedisError import os import socket # Connect to Redis redis = Redis ( host = \"redis\" , db = 0 , socket_connect_timeout = 2 , socket_timeout = 2 ) app = Flask ( __name__ ) @app . route ( \"/\" ) def hello (): try : visits = redis . incr ( \"counter\" ) except RedisError : visits = \"<i>cannot connect to Redis, counter disabled</i>\" html = \"<h3>Hello {name} !</h3>\" \\ \"<b>Hostname:</b> {hostname} <br/>\" \\ \"<b>Visits:</b> {visits} \" return html . format ( name = os . getenv ( \"NAME\" , \"world\" ), hostname = socket . gethostname (), visits = visits ) if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' , port = 80 ) Nuestra aplicaci\u00f3n tiene una serie de dependencias ( librer\u00edas de terceros ) que guardaremos en el archivo _requirements . txt_ : Flask Redis Y por \u00faltimo definimos nuestro Dockerfile : # Partimos de una base oficial de python FROM python:2.7-slim # El directorio de trabajo es desde donde se ejecuta el contenedor al iniciarse WORKDIR /app # Copiamos todos los archivos del build context al directorio /app del contenedor COPY . /app # Ejecutamos pip para instalar las dependencias en el contenedor RUN pip install --trusted-host pypi.python.org -r requirements.txt # Indicamos que este contenedor se comunica por el puerto 80/tcp EXPOSE 80 # Declaramos una variable de entorno ENV NAME World # Ejecuta nuestra aplicaci\u00f3n cuando se inicia el contenedor CMD [ \"python\" , \"app.py\" ] Para conocer todas las directivas visita la documentaci\u00f3n oficial de Dockerfile . En total debemos tener 3 archivos: $ ls app.py Dockerfile requirements.txt Ahora construimos la imagen de nuestra aplicaci\u00f3n: docker build -t friendlyhello . Y comprobamos que est\u00e1 creada: $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE friendlyhello latest 88a822b3107c 56 seconds ago 132MB Probar nuestro contenedor Vamos a arrancar nuestro contenedor y probar la aplicaci\u00f3n: docker run --rm -p 4000 :80 friendlyhello Tip Normalmente los contenedores son de usar y tirar, sobre todo cuando hacemos pruebas. El par\u00e1metro --rm borra autom\u00e1ticamente un contenedor cuando se para. Recordemos que los datos vol\u00e1tiles siempre se deben guardar en vol\u00famenes. Lo que arranca la aplicaci\u00f3n Flask: $ docker run --rm -p 4000 :80 friendlyhello * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: Do not use the development server in a production environment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:80/ (Press CTRL+C to quit) Comprobamos en el puerto 4000 si efectivamente est\u00e1 iniciada o no: http://localhost:4000 . Obtendremos un mensaje como este: Hello World! Hostname: 0367b056e66e Visits: cannot connect to Redis, counter disabled Ya tenemos una imagen lista para ser usada. Pulsamos Control+C para interrumpir y borrar nuestro contenedor. Creando la aplicaci\u00f3n En este caso nuestro contenedor no funciona por s\u00ed mismo. Es muy habitual que dependamos de servicios para poder iniciar la aplicaci\u00f3n, habitualmente bases de datos. En este caso necesitamos una base de datos Redis que no tenemos. Como vimos en el apartado anterior, vamos a aprovechar las caracter\u00edsticas de Compose para levantar nuestra aplicaci\u00f3n. Vamos a crear el siguiente archivo docker-compose.yaml : version : \"3\" services : web : build : . ports : - \"4000:80\" redis : image : redis ports : - \"6379:6379\" volumes : - \"./data:/data\" command : redis-server --appendonly yes La principal diferencia con respecto al cap\u00edtulo anterior, es que en un servicio podemos indicar una imagen (par\u00e1metro imagen ) o un build context (par\u00e1metro build ). Esta es una manera de integrar las dos herramientas que nos proporciona Docker : la creaci\u00f3n de im\u00e1genes y la composici\u00f3n de aplicaciones con servicios. Balanceo de carga Vamos a modificar nuestro docker-compose.yaml : version : \"3\" services : web : build : . redis : image : redis volumes : - \"./data:/data\" command : redis-server --appendonly yes lb : image : dockercloud/haproxy ports : - 4000:80 links : - web volumes : - /var/run/docker.sock:/var/run/docker.sock En este caso, el servicio web no va a tener acceso al exterior (hemos eliminado el par\u00e1metro ports ). En su lugar hemos a\u00f1adido un balanceador de carga (el servicio lb ). Vamos a arrancar esta nueva aplicaci\u00f3n, pero esta vez a\u00f1adiendo varios servicios web: docker-composer up -d --scale web=5 Esperamos a que terminen de iniciar los servicios: $ docker-compose up -d --scale web = 5 Creating network \"friendlyhello_default\" with the default driver Creating friendlyhello_redis_1 ... done Creating friendlyhello_web_1 ... done Creating friendlyhello_web_2 ... done Creating friendlyhello_web_3 ... done Creating friendlyhello_web_4 ... done Creating friendlyhello_web_5 ... done Creating friendlyhello_lb_1 ... done Podemos comprobar como del servicio web nos ha iniciado 5 instancias, cada uno con su sufijo num\u00e9rico correspondiente. Si usamos docker ps para ver los contenedores disponibles tendremos: $ docker ps CONTAINER ID IMAGE [...] PORTS NAMES 77acae1d0567 dockercloud/haproxy [...] 443/tcp, 1936/tcp, 0.0.0.0:4000->80/tcp friendlyhello_lb_1 5f12fb8b80c8 friendlyhello_web [...] 80/tcp friendlyhello_web_5 fb0024591665 friendlyhello_web [...] 80/tcp friendlyhello_web_2 a20d20bdd129 friendlyhello_web [...] 80/tcp friendlyhello_web_4 53d7db212df8 friendlyhello_web [...] 80/tcp friendlyhello_web_3 41218dbbb882 friendlyhello_web [...] 80/tcp friendlyhello_web_1 06f5bf6ed070 redis [...] 6379/tcp friendlyhello_redis_1 Vamos a fijarnos en el CONTAINER ID y vamos a volver a abrir nuestra aplicaci\u00f3n: http://localhost:4000 . Si en esta ocasi\u00f3n vamos recargando la p\u00e1gina, veremos como cambian los hostnames , que a su vez coinciden con los identificadores de los contenedores anteriores. Info Esta no es la manera adecuada de hacer balanceo de carga, puesto que todos los contenedores est\u00e1n en la misma m\u00e1quina, lo cual no tiene sentido. Solo es una demostraci\u00f3n. Para hacer balanceo de carga real necesitar\u00edamos tener o emular un clustes de m\u00e1quinas y crear un enjambre ( swarm ). Compartir im\u00e1genes Si tenemos una imagen que queramos compartir, necesitamos usar un registro. Existe incluso una imagen que nos permite crear uno propio, pero vamos a usar el repositorio p\u00fablico de Docker . Los pasos son: Crear una cuenta de usuario en el repositorio oficial de Docker . Pulsar sobre el bot\u00f3n \" Create Repository + \". En el formulario hay que rellenar solo un dato obligatoriamente: el nombre. Usaremos el de la imagen: friendlyhello . Nuestro nombre de usuario es el namespace y es obligatorio que tenga uno. Si estuvieramos en alguna organizaci\u00f3n podr\u00edamos elegir entre varios. El resto de campos lo dejamos como est\u00e1 por el momento. La cuenta gratuita solo deja tener un repositorio privado, asi que no lo malgastaremos aqu\u00ed. Ahora tenemos que conectar nuestro cliente de Docker con nuestra cuenta en el Hub . Usamos el comando docker login . $ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: username Password: ******** WARNING! Your password will be stored unencrypted in /home/sergio/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/# credentials-store Danger Las claves se guardan sin cifrar. Hay que configurar un almacen de claves o recordar hacer docker logout para borrarla. Visita la web de referencia para saber como crear un almacen . Para que las im\u00e1genes se puedan guardar, tenemos que etiquetarla con el mismo nombre que tengamos en nuestro repositorio m\u00e1s el namespace . Si nuestra cuenta es ' username ' y el repositorio es ' friendlyhello ', debemos crear la imagen con la etiqueta ' username/friendlyhello '. $ docker build -t username/friendlyhello . Tip Por defecto ya hemos dicho que la etiqueta si no se indica es latest . Podemos indicar m\u00e1s de una etiqueta para indicar versiones: $ docker build -t username/friendlyhello -t username/friendlyhello:0.1.0 . En la pr\u00f3xima que hagamos le subimos la versi\u00f3n en la etiqueta: $ docker build -t username/friendlyhello -t username/friendlyhello:0.2.0 . De esta manera nuestra imagen aparecer\u00e1 con tres etiquetas: latest y 0.2.0 que ser\u00e1n la misma en realidad, y 0.1.0 . Ahora ya podemos enviar nuestra imagen: $ docker push username/friendlyhello Ejercicios Cambia el docker-compose.yaml para usar tu imagen en vez de hacer build . Cambia el docker-compose.yaml para usar la imagen de alg\u00fan compa\u00f1ero.","title":"Crear im\u00e1genes propias"},{"location":"dockerfile/#crear-imagenes-propias","text":"Ya hemos visto como usar im\u00e1genes de terceros para crear aplicaciones y servicios. Pero, \u00bfsi no hay ninguna imagen que tenga lo que queremos? \u00bfO si queremos hacer una imagen de nuestra aplicaci\u00f3n para distribuirla? Docker permite crear imagenes propias. Aunque podr\u00edamos hacerla partiendo de cero, es un esfuerzo que no tiene sentido. Existe ya im\u00e1genes base para crear las nuestras y es mucho m\u00e1s f\u00e1cil crear una imagen bas\u00e1ndose en otra que hacerlo todo nosotros. Podemos partir de una imagen base que parte de un lenguaje de programaci\u00f3n ( python , php ) o de alguna distribuci\u00f3n ( ubuntu , debian ).","title":"Crear im\u00e1genes propias"},{"location":"dockerfile/#mi-primer-dockerfile","text":"Los Dockerfile son los archivos que contienen las instrucciones que crean las imagenes. Deben estar guardados dentro de un build context , es decir, un directorio. Este directorio es el que contiene todos los archivos necesarios para construir nuestra imagen, de ah\u00ed lo de build context . Creamos nuestro build context mkdir -p ~/Sites/hello-world cd ~/Sites/hello-world echo \"hello\" > hello Dentro de este directorio crearemos un archivo llamado Dockerfile con este contenido: FROM busybox COPY /hello / RUN cat /hello Directiva Explicaci\u00f3n FROM Indica la imagen base sobre la que se basa esta imagen COPY Copia un archivo del build context y lo guarda en la imagen RUN Ejecuta el comando indicado durante el proceso de creaci\u00f3n de imagen. Ahora para crear nuestra imagen usaremos docker build . docker build -t helloapp:v1 . El par\u00e1metro -t nos permite etiquetar la imagen con un nombre y una versi\u00f3n. El . indica que el build context es el directorio actual. El resultado de ejecutar lo anterior ser\u00eda: $ docker build -t helloapp:v1 . Sending build context to Docker daemon 3.072kB Step 1/3 : FROM busybox latest: Pulling from library/busybox 8c5a7da1afbc: Pull complete Digest: sha256:cb63aa0641a885f54de20f61d152187419e8f6b159ed11a251a09d115fdff9bd Status: Downloaded newer image for busybox:latest ---> e1ddd7948a1c Step 2/3 : COPY /hello / ---> 8a092965dbc9 Step 3/3 : RUN cat /hello ---> Running in 83b5498790ca hello Removing intermediate container 83b5498790ca ---> f738f117d4b6 Successfully built f738f117d4b6 Successfully tagged helloapp:v1 Y podremos ver que una nueva imagen est\u00e1 instalada en nuestro equipo: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE helloapp v1 f738f117d4b6 40 seconds ago 1.16MB","title":"Mi primer Dockerfile"},{"location":"dockerfile/#creando-aplicaciones-en-contenedores","text":"Vamos a crear un aplicaci\u00f3n en python y la vamos a guardarla en un contenedor. Comenzamos creando un nuevo build context : mkdir -p ~/Sites/friendlyhello cd ~/Sites/friendlyhello El c\u00f3digo de la aplicaci\u00f3n es el siguiente, lo guardaremos en un archivo llamado app.py : from flask import Flask from redis import Redis , RedisError import os import socket # Connect to Redis redis = Redis ( host = \"redis\" , db = 0 , socket_connect_timeout = 2 , socket_timeout = 2 ) app = Flask ( __name__ ) @app . route ( \"/\" ) def hello (): try : visits = redis . incr ( \"counter\" ) except RedisError : visits = \"<i>cannot connect to Redis, counter disabled</i>\" html = \"<h3>Hello {name} !</h3>\" \\ \"<b>Hostname:</b> {hostname} <br/>\" \\ \"<b>Visits:</b> {visits} \" return html . format ( name = os . getenv ( \"NAME\" , \"world\" ), hostname = socket . gethostname (), visits = visits ) if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' , port = 80 ) Nuestra aplicaci\u00f3n tiene una serie de dependencias ( librer\u00edas de terceros ) que guardaremos en el archivo _requirements . txt_ : Flask Redis Y por \u00faltimo definimos nuestro Dockerfile : # Partimos de una base oficial de python FROM python:2.7-slim # El directorio de trabajo es desde donde se ejecuta el contenedor al iniciarse WORKDIR /app # Copiamos todos los archivos del build context al directorio /app del contenedor COPY . /app # Ejecutamos pip para instalar las dependencias en el contenedor RUN pip install --trusted-host pypi.python.org -r requirements.txt # Indicamos que este contenedor se comunica por el puerto 80/tcp EXPOSE 80 # Declaramos una variable de entorno ENV NAME World # Ejecuta nuestra aplicaci\u00f3n cuando se inicia el contenedor CMD [ \"python\" , \"app.py\" ] Para conocer todas las directivas visita la documentaci\u00f3n oficial de Dockerfile . En total debemos tener 3 archivos: $ ls app.py Dockerfile requirements.txt Ahora construimos la imagen de nuestra aplicaci\u00f3n: docker build -t friendlyhello . Y comprobamos que est\u00e1 creada: $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE friendlyhello latest 88a822b3107c 56 seconds ago 132MB","title":"Creando aplicaciones en contenedores"},{"location":"dockerfile/#probar-nuestro-contenedor","text":"Vamos a arrancar nuestro contenedor y probar la aplicaci\u00f3n: docker run --rm -p 4000 :80 friendlyhello Tip Normalmente los contenedores son de usar y tirar, sobre todo cuando hacemos pruebas. El par\u00e1metro --rm borra autom\u00e1ticamente un contenedor cuando se para. Recordemos que los datos vol\u00e1tiles siempre se deben guardar en vol\u00famenes. Lo que arranca la aplicaci\u00f3n Flask: $ docker run --rm -p 4000 :80 friendlyhello * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: Do not use the development server in a production environment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:80/ (Press CTRL+C to quit) Comprobamos en el puerto 4000 si efectivamente est\u00e1 iniciada o no: http://localhost:4000 . Obtendremos un mensaje como este: Hello World! Hostname: 0367b056e66e Visits: cannot connect to Redis, counter disabled Ya tenemos una imagen lista para ser usada. Pulsamos Control+C para interrumpir y borrar nuestro contenedor.","title":"Probar nuestro contenedor"},{"location":"dockerfile/#creando-la-aplicacion","text":"En este caso nuestro contenedor no funciona por s\u00ed mismo. Es muy habitual que dependamos de servicios para poder iniciar la aplicaci\u00f3n, habitualmente bases de datos. En este caso necesitamos una base de datos Redis que no tenemos. Como vimos en el apartado anterior, vamos a aprovechar las caracter\u00edsticas de Compose para levantar nuestra aplicaci\u00f3n. Vamos a crear el siguiente archivo docker-compose.yaml : version : \"3\" services : web : build : . ports : - \"4000:80\" redis : image : redis ports : - \"6379:6379\" volumes : - \"./data:/data\" command : redis-server --appendonly yes La principal diferencia con respecto al cap\u00edtulo anterior, es que en un servicio podemos indicar una imagen (par\u00e1metro imagen ) o un build context (par\u00e1metro build ). Esta es una manera de integrar las dos herramientas que nos proporciona Docker : la creaci\u00f3n de im\u00e1genes y la composici\u00f3n de aplicaciones con servicios.","title":"Creando la aplicaci\u00f3n"},{"location":"dockerfile/#balanceo-de-carga","text":"Vamos a modificar nuestro docker-compose.yaml : version : \"3\" services : web : build : . redis : image : redis volumes : - \"./data:/data\" command : redis-server --appendonly yes lb : image : dockercloud/haproxy ports : - 4000:80 links : - web volumes : - /var/run/docker.sock:/var/run/docker.sock En este caso, el servicio web no va a tener acceso al exterior (hemos eliminado el par\u00e1metro ports ). En su lugar hemos a\u00f1adido un balanceador de carga (el servicio lb ). Vamos a arrancar esta nueva aplicaci\u00f3n, pero esta vez a\u00f1adiendo varios servicios web: docker-composer up -d --scale web=5 Esperamos a que terminen de iniciar los servicios: $ docker-compose up -d --scale web = 5 Creating network \"friendlyhello_default\" with the default driver Creating friendlyhello_redis_1 ... done Creating friendlyhello_web_1 ... done Creating friendlyhello_web_2 ... done Creating friendlyhello_web_3 ... done Creating friendlyhello_web_4 ... done Creating friendlyhello_web_5 ... done Creating friendlyhello_lb_1 ... done Podemos comprobar como del servicio web nos ha iniciado 5 instancias, cada uno con su sufijo num\u00e9rico correspondiente. Si usamos docker ps para ver los contenedores disponibles tendremos: $ docker ps CONTAINER ID IMAGE [...] PORTS NAMES 77acae1d0567 dockercloud/haproxy [...] 443/tcp, 1936/tcp, 0.0.0.0:4000->80/tcp friendlyhello_lb_1 5f12fb8b80c8 friendlyhello_web [...] 80/tcp friendlyhello_web_5 fb0024591665 friendlyhello_web [...] 80/tcp friendlyhello_web_2 a20d20bdd129 friendlyhello_web [...] 80/tcp friendlyhello_web_4 53d7db212df8 friendlyhello_web [...] 80/tcp friendlyhello_web_3 41218dbbb882 friendlyhello_web [...] 80/tcp friendlyhello_web_1 06f5bf6ed070 redis [...] 6379/tcp friendlyhello_redis_1 Vamos a fijarnos en el CONTAINER ID y vamos a volver a abrir nuestra aplicaci\u00f3n: http://localhost:4000 . Si en esta ocasi\u00f3n vamos recargando la p\u00e1gina, veremos como cambian los hostnames , que a su vez coinciden con los identificadores de los contenedores anteriores. Info Esta no es la manera adecuada de hacer balanceo de carga, puesto que todos los contenedores est\u00e1n en la misma m\u00e1quina, lo cual no tiene sentido. Solo es una demostraci\u00f3n. Para hacer balanceo de carga real necesitar\u00edamos tener o emular un clustes de m\u00e1quinas y crear un enjambre ( swarm ).","title":"Balanceo de carga"},{"location":"dockerfile/#compartir-imagenes","text":"Si tenemos una imagen que queramos compartir, necesitamos usar un registro. Existe incluso una imagen que nos permite crear uno propio, pero vamos a usar el repositorio p\u00fablico de Docker . Los pasos son: Crear una cuenta de usuario en el repositorio oficial de Docker . Pulsar sobre el bot\u00f3n \" Create Repository + \". En el formulario hay que rellenar solo un dato obligatoriamente: el nombre. Usaremos el de la imagen: friendlyhello . Nuestro nombre de usuario es el namespace y es obligatorio que tenga uno. Si estuvieramos en alguna organizaci\u00f3n podr\u00edamos elegir entre varios. El resto de campos lo dejamos como est\u00e1 por el momento. La cuenta gratuita solo deja tener un repositorio privado, asi que no lo malgastaremos aqu\u00ed. Ahora tenemos que conectar nuestro cliente de Docker con nuestra cuenta en el Hub . Usamos el comando docker login . $ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: username Password: ******** WARNING! Your password will be stored unencrypted in /home/sergio/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/# credentials-store Danger Las claves se guardan sin cifrar. Hay que configurar un almacen de claves o recordar hacer docker logout para borrarla. Visita la web de referencia para saber como crear un almacen . Para que las im\u00e1genes se puedan guardar, tenemos que etiquetarla con el mismo nombre que tengamos en nuestro repositorio m\u00e1s el namespace . Si nuestra cuenta es ' username ' y el repositorio es ' friendlyhello ', debemos crear la imagen con la etiqueta ' username/friendlyhello '. $ docker build -t username/friendlyhello . Tip Por defecto ya hemos dicho que la etiqueta si no se indica es latest . Podemos indicar m\u00e1s de una etiqueta para indicar versiones: $ docker build -t username/friendlyhello -t username/friendlyhello:0.1.0 . En la pr\u00f3xima que hagamos le subimos la versi\u00f3n en la etiqueta: $ docker build -t username/friendlyhello -t username/friendlyhello:0.2.0 . De esta manera nuestra imagen aparecer\u00e1 con tres etiquetas: latest y 0.2.0 que ser\u00e1n la misma en realidad, y 0.1.0 . Ahora ya podemos enviar nuestra imagen: $ docker push username/friendlyhello","title":"Compartir im\u00e1genes"},{"location":"dockerfile/#ejercicios","text":"Cambia el docker-compose.yaml para usar tu imagen en vez de hacer build . Cambia el docker-compose.yaml para usar la imagen de alg\u00fan compa\u00f1ero.","title":"Ejercicios"},{"location":"git/","text":"Aspectos b\u00e1sicos de Git Instalaci\u00f3n Instalando en Linux Si quieres instalar Git en Linux a trav\u00e9s de un instalador binario, en general puedes hacerlo a trav\u00e9s de la herramienta b\u00e1sica de gesti\u00f3n de paquetes que trae tu distribuci\u00f3n. Si est\u00e1s en Fedora, puedes usar yum: $ yum install git-core O si est\u00e1s en una distribuci\u00f3n basada en Debian como Ubuntu, prueba con apt-get: $ apt-get install git Instalando en Windows Instalar Git en Windows es muy f\u00e1cil. El proyecto msysGit tiene uno de los procesos de instalaci\u00f3n m\u00e1s sencillos. Simplemente descarga el archivo exe del instalador desde la p\u00e1gina de GitHub, y ejec\u00fatalo: http://msysgit.github.com/ Una vez instalado, tendr\u00e1s tanto la versi\u00f3n de l\u00ednea de comandos (incluido un cliente SSH que nos ser\u00e1 \u00fatil m\u00e1s adelante) como la interfaz gr\u00e1fica de usuario est\u00e1ndar. Se recomienda no modificar las opciones que trae por defecto el instalador. Instalando en MacOS En MacOS se recomienda tener instalada la herramienta homebrew . Despu\u00e9s, es tan f\u00e1cil como ejecutar: $ brew install git Configuraci\u00f3n Tu identidad Lo primero que deber\u00edas hacer cuando instalas Git es establecer tu nombre de usuario y direcci\u00f3n de correo electr\u00f3nico. Esto es importante porque las confirmaciones de cambios (commits) en Git usan esta informaci\u00f3n, y es introducida de manera inmutable en los commits que env\u00edas: $ git config --global user.name \"John Doe\" $ git config --global user.email johndoe@example.com Tambi\u00e9n se recomienda configurar el siguiente par\u00e1metro: $ git config --global push.default simple Bash Completion Bash completion es una utilidad que permite a bash completar \u00f3rdenes y par\u00e1metros. Por defecto suele venir desactivada en Ubuntu y es necesario modificar el archivo $HOME/.bashrc para poder activarla. Simplemente hay que descomentar las l\u00edneas que lo activan,","title":"Aspectos b\u00e1sicos de Git"},{"location":"git/#aspectos-basicos-de-git","text":"","title":"Aspectos b\u00e1sicos de Git"},{"location":"git/#instalacion","text":"","title":"Instalaci\u00f3n"},{"location":"git/#instalando-en-linux","text":"Si quieres instalar Git en Linux a trav\u00e9s de un instalador binario, en general puedes hacerlo a trav\u00e9s de la herramienta b\u00e1sica de gesti\u00f3n de paquetes que trae tu distribuci\u00f3n. Si est\u00e1s en Fedora, puedes usar yum: $ yum install git-core O si est\u00e1s en una distribuci\u00f3n basada en Debian como Ubuntu, prueba con apt-get: $ apt-get install git","title":"Instalando en Linux"},{"location":"git/#instalando-en-windows","text":"Instalar Git en Windows es muy f\u00e1cil. El proyecto msysGit tiene uno de los procesos de instalaci\u00f3n m\u00e1s sencillos. Simplemente descarga el archivo exe del instalador desde la p\u00e1gina de GitHub, y ejec\u00fatalo: http://msysgit.github.com/ Una vez instalado, tendr\u00e1s tanto la versi\u00f3n de l\u00ednea de comandos (incluido un cliente SSH que nos ser\u00e1 \u00fatil m\u00e1s adelante) como la interfaz gr\u00e1fica de usuario est\u00e1ndar. Se recomienda no modificar las opciones que trae por defecto el instalador.","title":"Instalando en Windows"},{"location":"git/#instalando-en-macos","text":"En MacOS se recomienda tener instalada la herramienta homebrew . Despu\u00e9s, es tan f\u00e1cil como ejecutar: $ brew install git","title":"Instalando en MacOS"},{"location":"git/#configuracion","text":"","title":"Configuraci\u00f3n"},{"location":"git/#tu-identidad","text":"Lo primero que deber\u00edas hacer cuando instalas Git es establecer tu nombre de usuario y direcci\u00f3n de correo electr\u00f3nico. Esto es importante porque las confirmaciones de cambios (commits) en Git usan esta informaci\u00f3n, y es introducida de manera inmutable en los commits que env\u00edas: $ git config --global user.name \"John Doe\" $ git config --global user.email johndoe@example.com Tambi\u00e9n se recomienda configurar el siguiente par\u00e1metro: $ git config --global push.default simple","title":"Tu identidad"},{"location":"git/#bash-completion","text":"Bash completion es una utilidad que permite a bash completar \u00f3rdenes y par\u00e1metros. Por defecto suele venir desactivada en Ubuntu y es necesario modificar el archivo $HOME/.bashrc para poder activarla. Simplemente hay que descomentar las l\u00edneas que lo activan,","title":"Bash Completion"},{"location":"gitflow/","text":"Flujo de trabajo con Git (git flow) La importancia de la organizaci\u00f3n del flujo de trabajo En la introducci\u00f3n vimos los diferentes esquemas de organizaci\u00f3n externa de los repositorios (es decir, en lo relativo a los usuarios que componen el equipo de trabajo). Pero el repositorio en s\u00ed tambi\u00e9n tiene su esquema de organizaci\u00f3n. En los ejemplos hemos visto que usabamos una rama m\u00e1ster y cre\u00e1bamos ramas para a\u00f1adir funcionalidades que luego integr\u00e1bamos. Es un forma de trabajar de las muchas que hay propuestas, posiblemente la m\u00e1s simple, pero tiene el inconveniente de dejar la rama m\u00e1ster a expensas de una mala actualizaci\u00f3n y quedarnos sin una rama estable. Por eso, hay otras propuestas mejores que permiten separar el trabajo de desarrollo con el mantenimiento de las versiones estables. Una de las m\u00e1s conocidas es la propuesta por Vincent Driessen y que podemos ver en la figura siguiente. Las ramas principales En este esquema hay dos ramas principales con un tiempo de vida indefinido: master ( origin/master ): el c\u00f3digo apuntado por HEAD siempre contiene un estado listo para producci\u00f3n. develop ( origin/develop ): el c\u00f3digo apuntado por HEAD siempre contiene los \u00faltimos cambios desarrollados para la pr\u00f3xima versi\u00f3n del software. Tambi\u00e9n se le puede llamar rama de integraci\u00f3n . No es necesariamente estable. Cuando el c\u00f3digo de la rama de desarrollo es lo suficientemente estable, se integra con la rama master y una nueva versi\u00f3n es lanzada. Las ramas auxiliares Para labores concretas, pueden usarse otro tipo de ramas, las cuales tienen un tiempo de vida definido. Es decir, cuando ya no son necesarias se eliminan: Ramas de funcionalidad (feature branches) Ramas de versi\u00f3n (release branches) Ramas de parches (hotfix branches) Feature branches Pueden partir de: develop Deben fusionarse con: develop Convenici\u00f3n de nombres: feature-NUMissue-*. Release branches Pueden partir de: develop Deben fusionarse con: develop y master Convenici\u00f3n de nombres: release-* Hotfix branches Pueden partir de: master Deben fusionarse con: develop y master Convenici\u00f3n de nombres: hotfix-* La extensi\u00f3n flow de Git Una de las ventajas de Git es que, adem\u00e1s, es extensible. Es decir, se pueden crear nuevas \u00f3rdenes como si de plugins se tratara. Una de las m\u00e1s usadas es gitflow , que est\u00e1 basada en el art\u00edculo que hablamos al principio de este cap\u00edtulo. Instalaci\u00f3n Aunque la fuente original de la extensi\u00f3n es del mismo autor del art\u00edculo, el c\u00f3digo no se encuentra ya muy actualizado y hay un fork bastante m\u00e1s activo en petervanderdoes/gitflow . En el wiki del repositorio est\u00e1n las instrucciones de instalaci\u00f3n para distintos sistemas. Una vez instalados tendremos una nueva \u00f3rden: git flow . Uso Para cambiar a las ramas master y develop, seguiremos usando git checkout , pero para trabajar con las ramas antes indicadas gitflow nos facilita las siguientes \u00f3rdenes: - git flow init: Inicializa el espacio de trabajo. De forma autom\u00e1tica, crea las ramas que necesitamos y permite configurar el nombre de las mismas. $ git flow init Initialized empty Git repository in ~/project/.git/ No branches exist yet. Base branches must be created now. Branch name for production releases: [master] Branch name for \"next release\" development: [develop] How to name your supporting branch prefixes? Feature branches? [feature/] Release branches? [release/] Hotfix branches? [hotfix/] Support branches? [support/] Version tag prefix? [] $ git branch * develop master Podemos ver que por defecto (usando intro en vez de escribir nada) pone nombres por defecto a cada rama. Con git branch comprobamos que ramas existen y en cual nos encontramos. - git flow feature: Permite crear y trabajar con ramas de funcionalidades. $ git flow feature start feature_branch As\u00ed creamos una rama 'feature/feature_branch' y nos mueve autom\u00e1ticamente a ella. En esta haremos los cambios que queramos en nuestro repositorio. Cuando queramos acabar de usar la rama, haremos un commit y la finalizaremos: $ git flow feature stop feature_branch Esto finaliza nuestra rama y la integra autom\u00e1ticamente a la rama develop. Si queremos seguir cambiando nuestro repositorio abriremos una nueva rama feature. - git flow release: Permite crear y trabajar con ramas de versiones. Cuando entendemos que despues de todas las funcionalidades (features, cambios en nuestro repositorio) nuestro trabajo esta listo para ser publicado, abriremos una rama release, que nacera de nuestra rama develop. $ git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' Usaremos un tag para identificar de que release se trata. Ahora podemos hacer los cambios que estimemos oportuno para integrar todas las features que el repositorio ha sufrido hasta el momento. Tras hacer commit a todo el proceso, podemos cerrar la rama release. $git flow release finish '0.1.0' Esto la integrar\u00e1 de forma autom\u00e1tica con master (con esto finalizamos el proceso de 'subir a producci\u00f3n' nuestro codigo) y con la rama develop, para que las futuras features est\u00e9n al d\u00eda. - git flow hotfix: Permite crear y trabajar con ramas de parches. Esto lo usaremos para hacer cambios rapidos que no puedan esperar a la proxima integracion de una release. $ git flow hotfix start hotfix_branch Tras hacer commit finalizamos la rama hotfix. Esta se fusionar\u00e1 con nuestra rama master y con nuestra rama develop para que esta tambi\u00e9n est\u00e9 al d\u00eda de los \u00faltimos cambios. $ git flow hotfix finish hotfix_branch","title":"Flujo de trabajo con Git (git flow)"},{"location":"gitflow/#flujo-de-trabajo-con-git-git-flow","text":"","title":"Flujo de trabajo con Git (git flow)"},{"location":"gitflow/#la-importancia-de-la-organizacion-del-flujo-de-trabajo","text":"En la introducci\u00f3n vimos los diferentes esquemas de organizaci\u00f3n externa de los repositorios (es decir, en lo relativo a los usuarios que componen el equipo de trabajo). Pero el repositorio en s\u00ed tambi\u00e9n tiene su esquema de organizaci\u00f3n. En los ejemplos hemos visto que usabamos una rama m\u00e1ster y cre\u00e1bamos ramas para a\u00f1adir funcionalidades que luego integr\u00e1bamos. Es un forma de trabajar de las muchas que hay propuestas, posiblemente la m\u00e1s simple, pero tiene el inconveniente de dejar la rama m\u00e1ster a expensas de una mala actualizaci\u00f3n y quedarnos sin una rama estable. Por eso, hay otras propuestas mejores que permiten separar el trabajo de desarrollo con el mantenimiento de las versiones estables. Una de las m\u00e1s conocidas es la propuesta por Vincent Driessen y que podemos ver en la figura siguiente.","title":"La importancia de la organizaci\u00f3n del flujo de trabajo"},{"location":"gitflow/#las-ramas-principales","text":"En este esquema hay dos ramas principales con un tiempo de vida indefinido: master ( origin/master ): el c\u00f3digo apuntado por HEAD siempre contiene un estado listo para producci\u00f3n. develop ( origin/develop ): el c\u00f3digo apuntado por HEAD siempre contiene los \u00faltimos cambios desarrollados para la pr\u00f3xima versi\u00f3n del software. Tambi\u00e9n se le puede llamar rama de integraci\u00f3n . No es necesariamente estable. Cuando el c\u00f3digo de la rama de desarrollo es lo suficientemente estable, se integra con la rama master y una nueva versi\u00f3n es lanzada.","title":"Las ramas principales"},{"location":"gitflow/#las-ramas-auxiliares","text":"Para labores concretas, pueden usarse otro tipo de ramas, las cuales tienen un tiempo de vida definido. Es decir, cuando ya no son necesarias se eliminan: Ramas de funcionalidad (feature branches) Ramas de versi\u00f3n (release branches) Ramas de parches (hotfix branches)","title":"Las ramas auxiliares"},{"location":"gitflow/#feature-branches","text":"Pueden partir de: develop Deben fusionarse con: develop Convenici\u00f3n de nombres: feature-NUMissue-*.","title":"Feature branches"},{"location":"gitflow/#release-branches","text":"Pueden partir de: develop Deben fusionarse con: develop y master Convenici\u00f3n de nombres: release-*","title":"Release branches"},{"location":"gitflow/#hotfix-branches","text":"Pueden partir de: master Deben fusionarse con: develop y master Convenici\u00f3n de nombres: hotfix-*","title":"Hotfix branches"},{"location":"gitflow/#la-extension-flow-de-git","text":"Una de las ventajas de Git es que, adem\u00e1s, es extensible. Es decir, se pueden crear nuevas \u00f3rdenes como si de plugins se tratara. Una de las m\u00e1s usadas es gitflow , que est\u00e1 basada en el art\u00edculo que hablamos al principio de este cap\u00edtulo.","title":"La extensi\u00f3n flow de Git"},{"location":"gitflow/#instalacion","text":"Aunque la fuente original de la extensi\u00f3n es del mismo autor del art\u00edculo, el c\u00f3digo no se encuentra ya muy actualizado y hay un fork bastante m\u00e1s activo en petervanderdoes/gitflow . En el wiki del repositorio est\u00e1n las instrucciones de instalaci\u00f3n para distintos sistemas. Una vez instalados tendremos una nueva \u00f3rden: git flow .","title":"Instalaci\u00f3n"},{"location":"gitflow/#uso","text":"Para cambiar a las ramas master y develop, seguiremos usando git checkout , pero para trabajar con las ramas antes indicadas gitflow nos facilita las siguientes \u00f3rdenes:","title":"Uso"},{"location":"gitflow/#-git-flow-init","text":"Inicializa el espacio de trabajo. De forma autom\u00e1tica, crea las ramas que necesitamos y permite configurar el nombre de las mismas. $ git flow init Initialized empty Git repository in ~/project/.git/ No branches exist yet. Base branches must be created now. Branch name for production releases: [master] Branch name for \"next release\" development: [develop] How to name your supporting branch prefixes? Feature branches? [feature/] Release branches? [release/] Hotfix branches? [hotfix/] Support branches? [support/] Version tag prefix? [] $ git branch * develop master Podemos ver que por defecto (usando intro en vez de escribir nada) pone nombres por defecto a cada rama. Con git branch comprobamos que ramas existen y en cual nos encontramos.","title":"- git flow init:"},{"location":"gitflow/#-git-flow-feature","text":"Permite crear y trabajar con ramas de funcionalidades. $ git flow feature start feature_branch As\u00ed creamos una rama 'feature/feature_branch' y nos mueve autom\u00e1ticamente a ella. En esta haremos los cambios que queramos en nuestro repositorio. Cuando queramos acabar de usar la rama, haremos un commit y la finalizaremos: $ git flow feature stop feature_branch Esto finaliza nuestra rama y la integra autom\u00e1ticamente a la rama develop. Si queremos seguir cambiando nuestro repositorio abriremos una nueva rama feature.","title":"- git flow feature:"},{"location":"gitflow/#-git-flow-release","text":"Permite crear y trabajar con ramas de versiones. Cuando entendemos que despues de todas las funcionalidades (features, cambios en nuestro repositorio) nuestro trabajo esta listo para ser publicado, abriremos una rama release, que nacera de nuestra rama develop. $ git flow release start 0.1.0 Switched to a new branch 'release/0.1.0' Usaremos un tag para identificar de que release se trata. Ahora podemos hacer los cambios que estimemos oportuno para integrar todas las features que el repositorio ha sufrido hasta el momento. Tras hacer commit a todo el proceso, podemos cerrar la rama release. $git flow release finish '0.1.0' Esto la integrar\u00e1 de forma autom\u00e1tica con master (con esto finalizamos el proceso de 'subir a producci\u00f3n' nuestro codigo) y con la rama develop, para que las futuras features est\u00e9n al d\u00eda.","title":"- git flow release:"},{"location":"gitflow/#-git-flow-hotfix","text":"Permite crear y trabajar con ramas de parches. Esto lo usaremos para hacer cambios rapidos que no puedan esperar a la proxima integracion de una release. $ git flow hotfix start hotfix_branch Tras hacer commit finalizamos la rama hotfix. Esta se fusionar\u00e1 con nuestra rama master y con nuestra rama develop para que esta tambi\u00e9n est\u00e9 al d\u00eda de los \u00faltimos cambios. $ git flow hotfix finish hotfix_branch","title":"- git flow hotfix:"},{"location":"github-avanzado/","text":"Github avanzado Esta secci\u00f3n trata de c\u00f3mo colaborar con proyectos de terceros. Clonar un repositorio Nos vamos a la web del proyecto en el que queremos colaborar. En este caso el proyecto se encuentra en https://github.com/sgomez/miniblog . Pulsamos en el bot\u00f3n de fork y eso crear\u00e1 una copia en nuestro perfil. Una vez se termine de clonar el repositorio, nos encontraremos con el espacio de trabajo del mismo: En la parte superior informaci\u00f3n sobre los commits, ramas, etiquetas, etc. Justo debajo un explorador de archivos. En la parte derecha un selector para cambiar de contexto entre: explorador de c\u00f3digo, peticiones de colaboraci\u00f3n (pull request), wiki, configuraci\u00f3n, etc. Justo abajo a la derecha informaci\u00f3n sobre como clonar localmente o descargar un proyecto. Github nos permite clonar localmente un proyecto por tres v\u00edas: HTTPS, SSH y Subversion. Seleccionamos SSH y copiamos el texto que despu\u00e9s a\u00f1adiremos a la orden git clone como en la primera l\u00ednea del siguiente grupo de \u00f3rdenes: $ git clone git@github.com:miusuario/miniblog.git $ cd miniblog $ composer.phar install $ php console create-schema Lo que hace el c\u00f3digo anterior es: Clona el repositorio localmente Entramos en la copia Instalamos las dependencias que la aplicaci\u00f3n tiene Arrancamos un servidor web para pruebas Y probamos que nuestra aplicaci\u00f3n funciona: $ php -S localhost:9999 -t web/ Podemos usar dos direcciones para probarla: Frontend: http://localhost:9999/index_dev.php Backend: http://localhost:9999/index_dev.php/admin/ con usuario admin y contrase\u00f1a 1234. Sincronizar con el repositorio original Cuando clonamos un repositorio de otro usuario hacemos una copia del original. Pero esa copia es igual al momento en el que hicimos la copia. Cuando el repositorio original cambie, que lo har\u00e1, nuestro repositorio no se actualizar\u00e1 solo. \u00a1Son dos repositorios diferentes! Necesitamos una manera de poder incorporar los cambios que vaya teniendo el repositorio original en el nuestro. Para eso crearemos una nueva rama remota. Por convenio, y como vimos anteriormente, ya existe una rama remota llamada origin que apunta al repositorio de donde clonamos el proyecto, en este caso apunta a nuestro fork en github: $ git remote show origin * remote origin Fetch URL: git@github.com:miusuario/miniblog.git Push URL: git@github.com:miusuario/miniblog.git HEAD branch (remote HEAD is ambiguous, may be one of the following): develop master Remote branches: develop tracked master tracked Local branch configured for 'git pull': master merges with remote master Local ref configured for 'git push': master pushes to master (up to date) Tambi\u00e9n por convenio, la rama remota que hace referencia al repositorio original se llama upstream y se crea de la siguiente manera: $ git remote add upstream git@github.com:sgomez/miniblog.git $ git remote show upstream * remote upstream Fetch URL: git@github.com:sgomez/miniblog.git Push URL: git@github.com:sgomez/miniblog.git HEAD branch: master Remote branches: develop new (next fetch will store in remotes/upstream) master new (next fetch will store in remotes/upstream) Local ref configured for 'git push': master pushes to master (local out of date) En este caso, la URI debe ser siempre la del proyecto original. Y ahora para incorporar actualizaciones, usaremos el merge en dos pasos: $ git fetch upstream $ git merge upstream/master Recordemos que fetch solo trae los cambios que existan en el repositorio remoto sin hacer ning\u00fan cambio en nuestro repositorio. Es la orden merge la que se encarga de que todo est\u00e9 sincronizado. En este caso decimos que queremos fusionar con la rama master que est\u00e1 en el repositorio upstream . Creando nuevas funcionalidades Vamos a crear una nueva funcionalidad: vamos a a\u00f1adir una licencia de uso. Para ello preferentemente crearemos una nueva rama. $ git checkout -b add-license $ echo \"LICENCIA MIT\" > LICESE # el error es intencionado $ git add LICESE $ git commit -m \"Archivo de licencia de uso\" En principio habr\u00eda que probar que todo funciona bien y entonces integraremos en la rama master de nuestro repositorio y enviamos los cambios a Github: $ git checkout master $ git merge add-license --no-ff $ git branch -d add-license # Borramos la rama que ya no nos sirve para nada $ git push --set-upstream origin add-license # Enviamos la rama a nuestro repositorio origin Si volvemos a Github, veremos que nos avisa de que hemos subido una nueva rama y si queremos crear un pull request. Pulsamos y entramos en la petici\u00f3n de Pull Request . Este es el momento para revisar cualquier error antes de enviar al due\u00f1o del repositorio. Como vemos hemos cometido uno, nombrando el fichero, si lo correguimos debemos hacer otro push para ir actualizando la rama. Cuando est\u00e9 lista volvemos aqu\u00ed y continuamos. Hay que dejar una descripci\u00f3n del cambio que vamos a hacer. Una vez hemos terminado y nos aseguramos que todo est\u00e1 correcto, pulsamos Send pull request y le llegar\u00e1 nuestra petici\u00f3n al due\u00f1o del proyecto. Sin embargo, para esta prueba, no vamos a cambiar el nombre del archivo y dejaremos el error como est\u00e1. As\u00ed de esta manera al administrador del proyecto le llegar\u00e1 el Pull Request y la lista de cambios. Ahora en principio, cabr\u00eda esperar que el administrador aprobara los cambios, pero podr\u00eda pasar que nos indicara que cambiemos algo. En ese caso solo habr\u00eda que modificar la rama y volverla a enviar. $ git mv LICESE LICENSE $ git commit -m \"Fix: Nombre de archivo LICENSE\" $ git push Ahora s\u00ed, el administrador puede aprobar la fusi\u00f3n y borrar la rama del repositorio. El panel de Github permite aceptar los cambios directamente o informa de como hacer una copia de la rama ofrecida por el usuario para hacer cambios, como puede verse en la siguiente imagen. Una vez que se han aceptado los cambios, podemos borrar la rama y actualizar nuestro repositorio con los datos del remoto como hicimos antes. \u00bfPor qu\u00e9 actualizar desde el remoto y no desde nuetra rama add-license ? Pues porque usualmente el administrador puede haber modificado los cambios que le hemos propuesto, o incluso una tercera persona. Recordemos el cariz colaborativo que tiene Github. $ git checkout master $ git branch -d add-license # Esto borra la rama local $ git push origin --delete add-license # Esto borra la rama remota. Tambi\u00e9n puede hacerse desde la web. Todo esto es algo complicado... S\u00ed, lo es, al menos al principio. Git tiene una parte muy sencilla que es el uso del repositorio local (\u00f3rdenes tales como add, rm, mv y commit). El siguiente nivel de complejidad lo componen las \u00f3rdenes para trabajar con ramas y fusionarlas (checkout, branch, merge, rebase) y por \u00faltimo, las que trabajan con repositorios remotos (pull, push, fetch, remote). Adem\u00e1s hay otra serie de \u00f3rdenes para tener informaci\u00f3n (diff, log, status) o hacer operaciones de mantenimiento (fsck, gc). Lo importante para no perderse en Git, es seguir la siguiente m\u00e1xima: No avanzar al siguiente nivel de complejidad, hasta no haber entendido completamente el anterior. Muy poco sentido tiene ponernos a crear ramas en github si a\u00fan no entendemos c\u00f3mo se crean localmente y para que deben usarse. En la parte de referencias hay varios manuales en l\u00ednea, incluso tutoriales interactivos. Tambi\u00e9n hay mucha documentaci\u00f3n disponible en Github que suele venir muy bien explicada. En caso de que tengamos un problema que no sepamos resolver, una web muy buena es StackOverflow . Es una web de preguntas y respuestas para profesionales; es muy dif\u00edcil que se os plantee una duda que no haya sido ya preguntada y respondida en esa web. Eso s\u00ed, el ingl\u00e9s es imprescindible. \u00daltimo paso, documentaci\u00f3n. Github permite crear documentaci\u00f3n. En primer lugar, generando un archivo llamado README.md . Tambi\u00e9n permite crear una web propia para el proyecto y, adem\u00e1s, una wiki. Para marcar el texto, se utiliza un lenguaje de marcado de texto denominado Markdown . En la siguiente web hay un tutorial interactivo: http://www.markdowntutorial.com/ . Como en principio, no es necesario saber Markdown para poder trabajar con Git o con Github, no vamos a incidir m\u00e1s en este asunto. En el propio GitHub podemos encontrar algunas plantillas que nos sirvan de referencia. Algunos ejemplos: Plantilla b\u00e1sica Plantilla avanzada Documentaci\u00f3n del curso Esta documentaci\u00f3n est\u00e1 hecha en Markdown y pasada a HTML gracia a la herramienta mkdocs . La plantilla usada es Material for MkDocs . El material est\u00e1 publicado con licencia Atribuci\u00f3n-NoComercial 4.0 Internacional (CC BY-NC 4.0)","title":"Github avanzado"},{"location":"github-avanzado/#github-avanzado","text":"Esta secci\u00f3n trata de c\u00f3mo colaborar con proyectos de terceros.","title":"Github avanzado"},{"location":"github-avanzado/#clonar-un-repositorio","text":"Nos vamos a la web del proyecto en el que queremos colaborar. En este caso el proyecto se encuentra en https://github.com/sgomez/miniblog . Pulsamos en el bot\u00f3n de fork y eso crear\u00e1 una copia en nuestro perfil. Una vez se termine de clonar el repositorio, nos encontraremos con el espacio de trabajo del mismo: En la parte superior informaci\u00f3n sobre los commits, ramas, etiquetas, etc. Justo debajo un explorador de archivos. En la parte derecha un selector para cambiar de contexto entre: explorador de c\u00f3digo, peticiones de colaboraci\u00f3n (pull request), wiki, configuraci\u00f3n, etc. Justo abajo a la derecha informaci\u00f3n sobre como clonar localmente o descargar un proyecto. Github nos permite clonar localmente un proyecto por tres v\u00edas: HTTPS, SSH y Subversion. Seleccionamos SSH y copiamos el texto que despu\u00e9s a\u00f1adiremos a la orden git clone como en la primera l\u00ednea del siguiente grupo de \u00f3rdenes: $ git clone git@github.com:miusuario/miniblog.git $ cd miniblog $ composer.phar install $ php console create-schema Lo que hace el c\u00f3digo anterior es: Clona el repositorio localmente Entramos en la copia Instalamos las dependencias que la aplicaci\u00f3n tiene Arrancamos un servidor web para pruebas Y probamos que nuestra aplicaci\u00f3n funciona: $ php -S localhost:9999 -t web/ Podemos usar dos direcciones para probarla: Frontend: http://localhost:9999/index_dev.php Backend: http://localhost:9999/index_dev.php/admin/ con usuario admin y contrase\u00f1a 1234.","title":"Clonar un repositorio"},{"location":"github-avanzado/#sincronizar-con-el-repositorio-original","text":"Cuando clonamos un repositorio de otro usuario hacemos una copia del original. Pero esa copia es igual al momento en el que hicimos la copia. Cuando el repositorio original cambie, que lo har\u00e1, nuestro repositorio no se actualizar\u00e1 solo. \u00a1Son dos repositorios diferentes! Necesitamos una manera de poder incorporar los cambios que vaya teniendo el repositorio original en el nuestro. Para eso crearemos una nueva rama remota. Por convenio, y como vimos anteriormente, ya existe una rama remota llamada origin que apunta al repositorio de donde clonamos el proyecto, en este caso apunta a nuestro fork en github: $ git remote show origin * remote origin Fetch URL: git@github.com:miusuario/miniblog.git Push URL: git@github.com:miusuario/miniblog.git HEAD branch (remote HEAD is ambiguous, may be one of the following): develop master Remote branches: develop tracked master tracked Local branch configured for 'git pull': master merges with remote master Local ref configured for 'git push': master pushes to master (up to date) Tambi\u00e9n por convenio, la rama remota que hace referencia al repositorio original se llama upstream y se crea de la siguiente manera: $ git remote add upstream git@github.com:sgomez/miniblog.git $ git remote show upstream * remote upstream Fetch URL: git@github.com:sgomez/miniblog.git Push URL: git@github.com:sgomez/miniblog.git HEAD branch: master Remote branches: develop new (next fetch will store in remotes/upstream) master new (next fetch will store in remotes/upstream) Local ref configured for 'git push': master pushes to master (local out of date) En este caso, la URI debe ser siempre la del proyecto original. Y ahora para incorporar actualizaciones, usaremos el merge en dos pasos: $ git fetch upstream $ git merge upstream/master Recordemos que fetch solo trae los cambios que existan en el repositorio remoto sin hacer ning\u00fan cambio en nuestro repositorio. Es la orden merge la que se encarga de que todo est\u00e9 sincronizado. En este caso decimos que queremos fusionar con la rama master que est\u00e1 en el repositorio upstream .","title":"Sincronizar con el repositorio original"},{"location":"github-avanzado/#creando-nuevas-funcionalidades","text":"Vamos a crear una nueva funcionalidad: vamos a a\u00f1adir una licencia de uso. Para ello preferentemente crearemos una nueva rama. $ git checkout -b add-license $ echo \"LICENCIA MIT\" > LICESE # el error es intencionado $ git add LICESE $ git commit -m \"Archivo de licencia de uso\" En principio habr\u00eda que probar que todo funciona bien y entonces integraremos en la rama master de nuestro repositorio y enviamos los cambios a Github: $ git checkout master $ git merge add-license --no-ff $ git branch -d add-license # Borramos la rama que ya no nos sirve para nada $ git push --set-upstream origin add-license # Enviamos la rama a nuestro repositorio origin Si volvemos a Github, veremos que nos avisa de que hemos subido una nueva rama y si queremos crear un pull request. Pulsamos y entramos en la petici\u00f3n de Pull Request . Este es el momento para revisar cualquier error antes de enviar al due\u00f1o del repositorio. Como vemos hemos cometido uno, nombrando el fichero, si lo correguimos debemos hacer otro push para ir actualizando la rama. Cuando est\u00e9 lista volvemos aqu\u00ed y continuamos. Hay que dejar una descripci\u00f3n del cambio que vamos a hacer. Una vez hemos terminado y nos aseguramos que todo est\u00e1 correcto, pulsamos Send pull request y le llegar\u00e1 nuestra petici\u00f3n al due\u00f1o del proyecto. Sin embargo, para esta prueba, no vamos a cambiar el nombre del archivo y dejaremos el error como est\u00e1. As\u00ed de esta manera al administrador del proyecto le llegar\u00e1 el Pull Request y la lista de cambios. Ahora en principio, cabr\u00eda esperar que el administrador aprobara los cambios, pero podr\u00eda pasar que nos indicara que cambiemos algo. En ese caso solo habr\u00eda que modificar la rama y volverla a enviar. $ git mv LICESE LICENSE $ git commit -m \"Fix: Nombre de archivo LICENSE\" $ git push Ahora s\u00ed, el administrador puede aprobar la fusi\u00f3n y borrar la rama del repositorio. El panel de Github permite aceptar los cambios directamente o informa de como hacer una copia de la rama ofrecida por el usuario para hacer cambios, como puede verse en la siguiente imagen. Una vez que se han aceptado los cambios, podemos borrar la rama y actualizar nuestro repositorio con los datos del remoto como hicimos antes. \u00bfPor qu\u00e9 actualizar desde el remoto y no desde nuetra rama add-license ? Pues porque usualmente el administrador puede haber modificado los cambios que le hemos propuesto, o incluso una tercera persona. Recordemos el cariz colaborativo que tiene Github. $ git checkout master $ git branch -d add-license # Esto borra la rama local $ git push origin --delete add-license # Esto borra la rama remota. Tambi\u00e9n puede hacerse desde la web.","title":"Creando nuevas funcionalidades"},{"location":"github-avanzado/#todo-esto-es-algo-complicado","text":"S\u00ed, lo es, al menos al principio. Git tiene una parte muy sencilla que es el uso del repositorio local (\u00f3rdenes tales como add, rm, mv y commit). El siguiente nivel de complejidad lo componen las \u00f3rdenes para trabajar con ramas y fusionarlas (checkout, branch, merge, rebase) y por \u00faltimo, las que trabajan con repositorios remotos (pull, push, fetch, remote). Adem\u00e1s hay otra serie de \u00f3rdenes para tener informaci\u00f3n (diff, log, status) o hacer operaciones de mantenimiento (fsck, gc). Lo importante para no perderse en Git, es seguir la siguiente m\u00e1xima: No avanzar al siguiente nivel de complejidad, hasta no haber entendido completamente el anterior. Muy poco sentido tiene ponernos a crear ramas en github si a\u00fan no entendemos c\u00f3mo se crean localmente y para que deben usarse. En la parte de referencias hay varios manuales en l\u00ednea, incluso tutoriales interactivos. Tambi\u00e9n hay mucha documentaci\u00f3n disponible en Github que suele venir muy bien explicada. En caso de que tengamos un problema que no sepamos resolver, una web muy buena es StackOverflow . Es una web de preguntas y respuestas para profesionales; es muy dif\u00edcil que se os plantee una duda que no haya sido ya preguntada y respondida en esa web. Eso s\u00ed, el ingl\u00e9s es imprescindible.","title":"Todo esto es algo complicado..."},{"location":"github-avanzado/#ultimo-paso-documentacion","text":"Github permite crear documentaci\u00f3n. En primer lugar, generando un archivo llamado README.md . Tambi\u00e9n permite crear una web propia para el proyecto y, adem\u00e1s, una wiki. Para marcar el texto, se utiliza un lenguaje de marcado de texto denominado Markdown . En la siguiente web hay un tutorial interactivo: http://www.markdowntutorial.com/ . Como en principio, no es necesario saber Markdown para poder trabajar con Git o con Github, no vamos a incidir m\u00e1s en este asunto. En el propio GitHub podemos encontrar algunas plantillas que nos sirvan de referencia. Algunos ejemplos: Plantilla b\u00e1sica Plantilla avanzada","title":"\u00daltimo paso, documentaci\u00f3n."},{"location":"github-avanzado/#documentacion-del-curso","text":"Esta documentaci\u00f3n est\u00e1 hecha en Markdown y pasada a HTML gracia a la herramienta mkdocs . La plantilla usada es Material for MkDocs . El material est\u00e1 publicado con licencia Atribuci\u00f3n-NoComercial 4.0 Internacional (CC BY-NC 4.0)","title":"Documentaci\u00f3n del curso"},{"location":"github-flow/","text":"Flujo de trabajo en GitHub Paso 0. Abrir una incidencia (issue) Habitualmente el trabajo puede partir a ra\u00edz de una reporte por parte de un miembro del equipo o de una persona externa. Para eso tenemos la secci\u00f3n Issues . Una issue cuando se crea se compone de un t\u00edtulo y una descripci\u00f3n en Markdown. Si la persona es miembro del equipo, opcionalmente puede asignarle una serie de metadatos: etiquetas (labels), hitos (milestone), proyecto al que pertenece o responsables encargados de cerrar la incidencia. Una vez creado, al mismo se le asignar\u00e1 un n\u00famero.\u00e7 Example Vamos a crear una incidencia llamada \"Crear archivo de autores\", donde indiquemos que vamos a crear un archivo AUTHORS.md con la lista de desarrolladores del proyecto. Paso 1. Crear una rama Crearemos una rama cada vez que queramos implementar una nueva caracter\u00edstica al proyecto que estamos realizando. La misma puede estar provocada por una incidencia o no. Tip Es una buena costumbre crear en Issues el listado de casos de uso, requisitos, hostorias de usuario o tareas (como lo queramos llamar), para tener un registro del trabajo que llevamos y el que nos queda. El nombre de la rama puede ser el que creamos conveniente, pero hay que intentar ser coherente y usar siempre el mismo m\u00e9todo, sobre todo si trabajamos en equipo. Un m\u00e9todo puede ser el siguiente: $ # tipo-n\u00famero/descripci\u00f3n $ git checkout -b feature-1/create-changelog $ git checkout -b hotfix-2/updated-database En entornos de trabajo multiusuario se puede usar el siguiente: $ # usuario/tipo-n\u00famero/descripci\u00f3n $ git checkout -b sgomez/feature-1/create-changelog $ git checkout -b sgomez/hotfix-2/updated-database De esa manera, podemos seguir f\u00e1cilmente qui\u00e9n abri\u00f3 la rama, en qu\u00e9 consiste y a qu\u00e9 issues est\u00e1 conectada. Pero como decimos es m\u00e1s un convenio que una imposici\u00f3n, pudi\u00e9ndole poner el nombre que queramos. Vamos a crear la rama y los commits correspondientes y subir la rama con push al servidor. $ git checkout -b sgomez/feature-1/create-changelog $ git add AUTHORS.md $ git commit -m \"A\u00f1adido fichero de autores\" El archivo puede contener, por ejemplo, lo siguiente: # AUTHORS * Sergio G\u00f3mez <sergio@uco.es> Hacemos push y obtenemos algo como esto: $ git push fatal: The current branch sgomez/feature-1/create-changelog has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin sgomez/feature-1/create-changelog Como la rama es nueva, git no sabe d\u00f3nde debe hacer push. Le indicamos que debe hacerla en origin y adem\u00e1s que guarde la vinculaci\u00f3n (equivalente al par\u00e1metro -u que vimos en el cap\u00edtulo anterior). Probamos de nuevo: $ git push -u origin sgomez/feature-1/create-changelog Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 4 threads Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 1.03 KiB | 1.03 MiB/s, done. Total 3 (delta 0), reused 0 (delta 0) remote: remote: Create a pull request for 'sgomez/feature-1/create-changelog' on GitHub by visiting: remote: https://github.com/sgomez/taller-de-git/pull/new/sgomez/feature-1/create-changelog remote: To github.com:sgomez/taller-de-git.git * [new branch] sgomez/feature-1/create-changelog -> sgomez/feature-1/create-changelog Branch 'sgomez/feature-1/create-changelog' set up to track remote branch 'sgomez/feature-1/create-changelog' from 'origin'. Ahora la rama ya se ha subido y nos informa, adem\u00e1s, de que podemos crear un Pull Request (PR). Si vamos al enlace que nos aparece veremos lo siguiente: Aqu\u00ed podemos informar de en qu\u00e9 consiste la rama que estamos enviando. Si ya tenemos una issue abierta, no es necesario repetir la misma informaci\u00f3n. Podemos hacer referencia con el siguiente texto: Closes #1 Esto lo que le indica a GitHub que esta PR cierra el issues n\u00famero 1. Cuando se haga el merge de la rama, autom\u00e1ticamente se cerrar\u00e1 la incidencia. Lo hacemos y le damos a crear. Paso 2. Crear commits A partir de ahora podemos seguir creando commits en local y enviarlos hasta que terminemos de trabajar. Editamos el archivo AUTHORS.md . # AUTHORS * Sergio G\u00f3mez <sergio@uco.es> * John Doe Y mandamos otro commit $ git commit -am \"Actualizado AUTHORS.md\" $ git push Si volvemos a la p\u00e1gina de PR, veremos que aparece el nuevo commit que acabamos de enviar. Paso 3. Discutir GitHub permite que entre los desarrolladores se pueda abrir una discusi\u00f3n sobre el c\u00f3digo, de tal manera que el trabajo de crear la rama sea colaborativo. Se puede incluso pedir revisiones por parte de terceros y que esas revisiones sean obligatorias antes de aceptar los cambios. Paso 4. Desplegar Una vez que hemos terminado de crear la funci\u00f3n de la rama ya podemos incorporar los cambios a master . Este trabajo ya no es necesario hacerlo en local y GitHub nos proporciona 3 maneras de hacerlo: Crear un merge commit Esta opci\u00f3n es el equivalente a hacer lo siguiente en nuestro repositorio: $ git checkout master $ git merge --no-ff sgomez/feature-1/create-changelog $ git push Es decir, el equivalente a hacer un merge entre nuestra rama y master. Info GitHub siempre desactiva el fast forward . Crear un rebase y merge Esta opci\u00f3n es el equivalente a hacer lo siguiente en nuestro repositorio $ git rebase master $ git checkout master $ git merge --no-ff sgomez/feature-1/create-changelog $ git push Es decir, nos aseguramos de que nuestra rama est\u00e1 al final de master haciendo rebase , como vimos en el cap\u00edtulo de ramas, y posteriormente se hace el merge. Crear un squash commit y un merge Esta opci\u00f3n es el equivalente a hacer lo siguiente en nuestro repositorio: $ git checkout master $ git merge --squash sgomez/feature-1/create-changelog $ git push Esta opci\u00f3n es algo especial. En vez de aplicar cada uno de los commits en la rama master, ya sea directamente ( fast forward ) o no, lo que hace es crear un solo commit con los cambios de todos los commits de la rama. El efecto final es como si en la rama solo hubiera producido un solo commit. Vamos a seleccionar este \u00faltimo (squash and merge) y le damos al bot\u00f3n para activarlo. Nos saldr\u00e1 una caja para que podamos crear una descripci\u00f3n del commit y le damos a confirmar. Ya hemos terminado y nos aparecer\u00e1 una opci\u00f3n para borrar la rama, lo m\u00e1s recomendado para no tener ramas obsoletas. Las consecuencias de esta acci\u00f3n son las siguientes: El PR aparecer\u00e1 como estado merged y en la lista de PR como cerrado. El issue que abrimos se habr\u00e1 cerrado autom\u00e1ticamente. En el listado de commits aparecer\u00e1 solo uno con un enlace al PR (en vez de los dos commits que hicimos). Paso 5. Sincronizar Hemos cambiado el repositorio en GitHub, pero nuestra rama master no contiene los mismos cambios que el de origin. As\u00ed que nos toca sincronizar y borrar la rama obsoleta: $ git checkout master $ git pull --rebase --autostash $ git branch -D sgomez/feature-1/create-changelog Info \u00bfPor qu\u00e9 squash and merge y no un merge o rebase ? De nuevo depende de los gustos de cada equipo de desarrollo. Las cracter\u00edsticas de squash es que elimina (relativamente) rastros de errores intermedios mientras se implementaba la rama, deja menos commits en la rama master y nos enlace al PR donde se implementaron los cambios. Para algunas personas estas caracter\u00edsticas son unas ventajas, para otras no. Lo mejor es experimentar cada opci\u00f3n y cada uno decida como quiere trabajar.","title":"Flujo de trabajo en GitHub"},{"location":"github-flow/#flujo-de-trabajo-en-github","text":"","title":"Flujo de trabajo en GitHub"},{"location":"github-flow/#paso-0-abrir-una-incidencia-issue","text":"Habitualmente el trabajo puede partir a ra\u00edz de una reporte por parte de un miembro del equipo o de una persona externa. Para eso tenemos la secci\u00f3n Issues . Una issue cuando se crea se compone de un t\u00edtulo y una descripci\u00f3n en Markdown. Si la persona es miembro del equipo, opcionalmente puede asignarle una serie de metadatos: etiquetas (labels), hitos (milestone), proyecto al que pertenece o responsables encargados de cerrar la incidencia. Una vez creado, al mismo se le asignar\u00e1 un n\u00famero.\u00e7 Example Vamos a crear una incidencia llamada \"Crear archivo de autores\", donde indiquemos que vamos a crear un archivo AUTHORS.md con la lista de desarrolladores del proyecto.","title":"Paso 0. Abrir una incidencia (issue)"},{"location":"github-flow/#paso-1-crear-una-rama","text":"Crearemos una rama cada vez que queramos implementar una nueva caracter\u00edstica al proyecto que estamos realizando. La misma puede estar provocada por una incidencia o no. Tip Es una buena costumbre crear en Issues el listado de casos de uso, requisitos, hostorias de usuario o tareas (como lo queramos llamar), para tener un registro del trabajo que llevamos y el que nos queda. El nombre de la rama puede ser el que creamos conveniente, pero hay que intentar ser coherente y usar siempre el mismo m\u00e9todo, sobre todo si trabajamos en equipo. Un m\u00e9todo puede ser el siguiente: $ # tipo-n\u00famero/descripci\u00f3n $ git checkout -b feature-1/create-changelog $ git checkout -b hotfix-2/updated-database En entornos de trabajo multiusuario se puede usar el siguiente: $ # usuario/tipo-n\u00famero/descripci\u00f3n $ git checkout -b sgomez/feature-1/create-changelog $ git checkout -b sgomez/hotfix-2/updated-database De esa manera, podemos seguir f\u00e1cilmente qui\u00e9n abri\u00f3 la rama, en qu\u00e9 consiste y a qu\u00e9 issues est\u00e1 conectada. Pero como decimos es m\u00e1s un convenio que una imposici\u00f3n, pudi\u00e9ndole poner el nombre que queramos. Vamos a crear la rama y los commits correspondientes y subir la rama con push al servidor. $ git checkout -b sgomez/feature-1/create-changelog $ git add AUTHORS.md $ git commit -m \"A\u00f1adido fichero de autores\" El archivo puede contener, por ejemplo, lo siguiente: # AUTHORS * Sergio G\u00f3mez <sergio@uco.es> Hacemos push y obtenemos algo como esto: $ git push fatal: The current branch sgomez/feature-1/create-changelog has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin sgomez/feature-1/create-changelog Como la rama es nueva, git no sabe d\u00f3nde debe hacer push. Le indicamos que debe hacerla en origin y adem\u00e1s que guarde la vinculaci\u00f3n (equivalente al par\u00e1metro -u que vimos en el cap\u00edtulo anterior). Probamos de nuevo: $ git push -u origin sgomez/feature-1/create-changelog Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 4 threads Compressing objects: 100% (2/2), done. Writing objects: 100% (3/3), 1.03 KiB | 1.03 MiB/s, done. Total 3 (delta 0), reused 0 (delta 0) remote: remote: Create a pull request for 'sgomez/feature-1/create-changelog' on GitHub by visiting: remote: https://github.com/sgomez/taller-de-git/pull/new/sgomez/feature-1/create-changelog remote: To github.com:sgomez/taller-de-git.git * [new branch] sgomez/feature-1/create-changelog -> sgomez/feature-1/create-changelog Branch 'sgomez/feature-1/create-changelog' set up to track remote branch 'sgomez/feature-1/create-changelog' from 'origin'. Ahora la rama ya se ha subido y nos informa, adem\u00e1s, de que podemos crear un Pull Request (PR). Si vamos al enlace que nos aparece veremos lo siguiente: Aqu\u00ed podemos informar de en qu\u00e9 consiste la rama que estamos enviando. Si ya tenemos una issue abierta, no es necesario repetir la misma informaci\u00f3n. Podemos hacer referencia con el siguiente texto: Closes #1 Esto lo que le indica a GitHub que esta PR cierra el issues n\u00famero 1. Cuando se haga el merge de la rama, autom\u00e1ticamente se cerrar\u00e1 la incidencia. Lo hacemos y le damos a crear.","title":"Paso 1. Crear una rama"},{"location":"github-flow/#paso-2-crear-commits","text":"A partir de ahora podemos seguir creando commits en local y enviarlos hasta que terminemos de trabajar. Editamos el archivo AUTHORS.md . # AUTHORS * Sergio G\u00f3mez <sergio@uco.es> * John Doe Y mandamos otro commit $ git commit -am \"Actualizado AUTHORS.md\" $ git push Si volvemos a la p\u00e1gina de PR, veremos que aparece el nuevo commit que acabamos de enviar.","title":"Paso 2. Crear commits"},{"location":"github-flow/#paso-3-discutir","text":"GitHub permite que entre los desarrolladores se pueda abrir una discusi\u00f3n sobre el c\u00f3digo, de tal manera que el trabajo de crear la rama sea colaborativo. Se puede incluso pedir revisiones por parte de terceros y que esas revisiones sean obligatorias antes de aceptar los cambios.","title":"Paso 3. Discutir"},{"location":"github-flow/#paso-4-desplegar","text":"Una vez que hemos terminado de crear la funci\u00f3n de la rama ya podemos incorporar los cambios a master . Este trabajo ya no es necesario hacerlo en local y GitHub nos proporciona 3 maneras de hacerlo:","title":"Paso 4. Desplegar"},{"location":"github-flow/#crear-un-merge-commit","text":"Esta opci\u00f3n es el equivalente a hacer lo siguiente en nuestro repositorio: $ git checkout master $ git merge --no-ff sgomez/feature-1/create-changelog $ git push Es decir, el equivalente a hacer un merge entre nuestra rama y master. Info GitHub siempre desactiva el fast forward .","title":"Crear un merge commit"},{"location":"github-flow/#crear-un-rebase-y-merge","text":"Esta opci\u00f3n es el equivalente a hacer lo siguiente en nuestro repositorio $ git rebase master $ git checkout master $ git merge --no-ff sgomez/feature-1/create-changelog $ git push Es decir, nos aseguramos de que nuestra rama est\u00e1 al final de master haciendo rebase , como vimos en el cap\u00edtulo de ramas, y posteriormente se hace el merge.","title":"Crear un rebase y merge"},{"location":"github-flow/#crear-un-squash-commit-y-un-merge","text":"Esta opci\u00f3n es el equivalente a hacer lo siguiente en nuestro repositorio: $ git checkout master $ git merge --squash sgomez/feature-1/create-changelog $ git push Esta opci\u00f3n es algo especial. En vez de aplicar cada uno de los commits en la rama master, ya sea directamente ( fast forward ) o no, lo que hace es crear un solo commit con los cambios de todos los commits de la rama. El efecto final es como si en la rama solo hubiera producido un solo commit. Vamos a seleccionar este \u00faltimo (squash and merge) y le damos al bot\u00f3n para activarlo. Nos saldr\u00e1 una caja para que podamos crear una descripci\u00f3n del commit y le damos a confirmar. Ya hemos terminado y nos aparecer\u00e1 una opci\u00f3n para borrar la rama, lo m\u00e1s recomendado para no tener ramas obsoletas. Las consecuencias de esta acci\u00f3n son las siguientes: El PR aparecer\u00e1 como estado merged y en la lista de PR como cerrado. El issue que abrimos se habr\u00e1 cerrado autom\u00e1ticamente. En el listado de commits aparecer\u00e1 solo uno con un enlace al PR (en vez de los dos commits que hicimos).","title":"Crear un squash commit y un merge"},{"location":"github-flow/#paso-5-sincronizar","text":"Hemos cambiado el repositorio en GitHub, pero nuestra rama master no contiene los mismos cambios que el de origin. As\u00ed que nos toca sincronizar y borrar la rama obsoleta: $ git checkout master $ git pull --rebase --autostash $ git branch -D sgomez/feature-1/create-changelog Info \u00bfPor qu\u00e9 squash and merge y no un merge o rebase ? De nuevo depende de los gustos de cada equipo de desarrollo. Las cracter\u00edsticas de squash es que elimina (relativamente) rastros de errores intermedios mientras se implementaba la rama, deja menos commits en la rama master y nos enlace al PR donde se implementaron los cambios. Para algunas personas estas caracter\u00edsticas son unas ventajas, para otras no. Lo mejor es experimentar cada opci\u00f3n y cada uno decida como quiere trabajar.","title":"Paso 5. Sincronizar"},{"location":"github-zenodo/","text":"Citar proyectos en GitHub Extra\u00eddo de la gu\u00eda oficial de GitHub . A trav\u00e9s de una aplicaci\u00f3n de terceros (Zenodo, financiado por el CERN), es posible crear un DOI para uno de nuestros proyectos. Estos son los pasos Paso 1. Elegir un repositorio Este repositorio debe ser abierto (p\u00fablico), o de lo contrario Zenodo no podr\u00e1 acceder al mismo. Hay que recordar escoger una licencia para el proyecto. Esta web puede ayudarnos http://choosealicense.com/ . Paso 2. Entrar en Zenodo Iremos a Zenodo y haremos login con GitHub. Lo \u00fanico que tenemos que hacer en esta parte es autorizar a Zenodo a conectar con nuestra cuenta de GitHub. Important Si deseas archivar un repositorio que pertenece a una organizaci\u00f3n en GitHub, deber\u00e1s asegurarte de que el administrador de la organizaci\u00f3n haya habilitado el acceso de terceros a la aplicaci\u00f3n Zenodo. Paso 3. Seleccionar los repositorios En este punto, hemos autorizado a Zenodo para configurar los permisos necesarios para permitir el archivado y la emisi\u00f3n del DOI. Para habilitar esta funcionalidad, simplemente haremo clic en el bot\u00f3n que est\u00e1 junto a cada uno de los repositorios que queremos archivar. Important Zenodo solo puede acceder a los repositorios p\u00fablicos, as\u00ed que debemos asegurarnos de que el repositorio que deseamos archivar sea p\u00fablico. Paso 4. Crear una nueva release Por defecto, Zenodo realiza un archivo de nuestro repositorio de GitHub cada vez que crea una nueva versi\u00f3n. Como a\u00fan no tenemos ninguna, tenemos que volver a la vista del repositorio principal y hacer clic en el elemento del encabezado de versiones ( releases ). Paso 5. Acu\u00f1ar un DOI Antes de que Zenodo pueda emitir un DOI para nuestro repositorio, deberemos proporcionar cierta informaci\u00f3n sobre el repositorio de GitHub que acaba de archivar. Una vez que estemos satisfechos con la descripci\u00f3n, heremos clic en el bot\u00f3n publicar. Paso 6. Publicar De vuelta a nuestra p\u00e1gina de Zenodo, ahora deber\u00edamos ver el repositorio listado con una nueva insignia que muestra nuestro nuevo DOI. Tip Podemos colocar la insigna en nuestro proyecto. Para eso haremos clic en la imagen DOI gris y azul. Se abrir\u00e1 una ventana emergente y el texto que aparece como Markdown es el que deberemos copiar en nuestro archivo README.md .","title":"Citar proyectos en GitHub"},{"location":"github-zenodo/#citar-proyectos-en-github","text":"Extra\u00eddo de la gu\u00eda oficial de GitHub . A trav\u00e9s de una aplicaci\u00f3n de terceros (Zenodo, financiado por el CERN), es posible crear un DOI para uno de nuestros proyectos. Estos son los pasos","title":"Citar proyectos en GitHub"},{"location":"github-zenodo/#paso-1-elegir-un-repositorio","text":"Este repositorio debe ser abierto (p\u00fablico), o de lo contrario Zenodo no podr\u00e1 acceder al mismo. Hay que recordar escoger una licencia para el proyecto. Esta web puede ayudarnos http://choosealicense.com/ .","title":"Paso 1. Elegir un repositorio"},{"location":"github-zenodo/#paso-2-entrar-en-zenodo","text":"Iremos a Zenodo y haremos login con GitHub. Lo \u00fanico que tenemos que hacer en esta parte es autorizar a Zenodo a conectar con nuestra cuenta de GitHub. Important Si deseas archivar un repositorio que pertenece a una organizaci\u00f3n en GitHub, deber\u00e1s asegurarte de que el administrador de la organizaci\u00f3n haya habilitado el acceso de terceros a la aplicaci\u00f3n Zenodo.","title":"Paso 2. Entrar en Zenodo"},{"location":"github-zenodo/#paso-3-seleccionar-los-repositorios","text":"En este punto, hemos autorizado a Zenodo para configurar los permisos necesarios para permitir el archivado y la emisi\u00f3n del DOI. Para habilitar esta funcionalidad, simplemente haremo clic en el bot\u00f3n que est\u00e1 junto a cada uno de los repositorios que queremos archivar. Important Zenodo solo puede acceder a los repositorios p\u00fablicos, as\u00ed que debemos asegurarnos de que el repositorio que deseamos archivar sea p\u00fablico.","title":"Paso 3. Seleccionar los repositorios"},{"location":"github-zenodo/#paso-4-crear-una-nueva-release","text":"Por defecto, Zenodo realiza un archivo de nuestro repositorio de GitHub cada vez que crea una nueva versi\u00f3n. Como a\u00fan no tenemos ninguna, tenemos que volver a la vista del repositorio principal y hacer clic en el elemento del encabezado de versiones ( releases ).","title":"Paso 4. Crear una nueva release"},{"location":"github-zenodo/#paso-5-acunar-un-doi","text":"Antes de que Zenodo pueda emitir un DOI para nuestro repositorio, deberemos proporcionar cierta informaci\u00f3n sobre el repositorio de GitHub que acaba de archivar. Una vez que estemos satisfechos con la descripci\u00f3n, heremos clic en el bot\u00f3n publicar.","title":"Paso 5. Acu\u00f1ar un DOI"},{"location":"github-zenodo/#paso-6-publicar","text":"De vuelta a nuestra p\u00e1gina de Zenodo, ahora deber\u00edamos ver el repositorio listado con una nueva insignia que muestra nuestro nuevo DOI. Tip Podemos colocar la insigna en nuestro proyecto. Para eso haremos clic en la imagen DOI gris y azul. Se abrir\u00e1 una ventana emergente y el texto que aparece como Markdown es el que deberemos copiar en nuestro archivo README.md .","title":"Paso 6. Publicar"},{"location":"github/","text":"Github Github es lo que se denomina una forja, un repositorio de proyectos que usan Git como sistema de control de versiones. Es la forja m\u00e1s popular, ya que alberga m\u00e1s de 10 millones de repositorios. Debe su popularidad a sus funcionalidades sociales, principalmente dos: la posibilidad de hacer forks de otros proyectos y la posibilidad de cooperar aportando c\u00f3digo para arreglar errores o mejorar el c\u00f3digo. Si bien, no es que fuera una novedad, s\u00ed lo es lo f\u00e1cil que resulta hacerlo. A ra\u00edz de este proyecto han surgido otros como Gitorius o Gitlab , pero Github sigue siendo el m\u00e1s popular y el que tiene mejores y mayores caracter\u00edsticas. algunas de estas son: Un wiki para documentar el proyecto, que usa MarkDown como lenguaje de marca. Un portal web para cada proyecto. Funcionalidades de redes sociales como followers. Gr\u00e1ficos estad\u00edsticos. Revisi\u00f3n de c\u00f3digo y comentarios. Sistemas de seguimiento de incidencias. Lo primero es entrar en el portal ( https://github.com/ ) para crearnos una cuenta si no la tenemos a\u00fan. Tu clave p\u00fablica/privada Muchos servidores Git utilizan la autentificaci\u00f3n a trav\u00e9s de claves p\u00fablicas SSH. Y, para ello, cada usuario del sistema ha de generarse una, si es que no la tiene ya. El proceso para hacerlo es similar en casi cualquier sistema operativo. Ante todo, asegurarte que no tengas ya una clave. (comprueba que el directorio $HOME/usuario/.ssh no tiene un archivo id_dsa.pub o id_rsa.pub). Para crear una nueva clave usamos la siguiente orden: $ ssh-keygen -t rsa -C \"Cuenta Thinstation\" Warning Tu clave RSA te identifica contra los repositorios remotos, aseg\u00farate de no compartir la clave privada con nadie. Por defecto la clave se crea como solo lectura . Configuraci\u00f3n Vamos a aprovechar para a\u00f1adir la clave RSA que generamos antes, para poder acceder desde git a los repositorios. Para ellos nos vamos al men\u00fa de configuraci\u00f3n de usuario ( Settings ) Nos vamos al men\u00fa 'SSH and GPG Keys' y a\u00f1adimos una nueva clave. En Title indicamos una descripci\u00f3n que nos ayude a saber de d\u00f3nde procede la clave y en key volcamos el contenido del archivo ~/.ssh/id_rsa.pub . Y guardamos la clave. Con esto ya tendriamos todo nuestro entorno para poder empezar a trabajar desde nuestro equipo. Clientes gr\u00e1ficos para GitHub Adem\u00e1s, para Github existe un cliente propio tanto para Windows como para MacOSX: Cliente Windows: http://windows.github.com/ Cliente MacOSX: http://mac.github.com/ Para Linux no hay cliente propio, pero s\u00ed hay plugin para la mayor\u00eda de editores de texto como atom, netbeans, eclipe o los editores de jetbrains. De todas maneras, estos clientes solo tienen el fin de facilitar el uso de Github, pero no son necesarios para usarlo. Es perfectamente v\u00e1lido usar el cliente de consola de Git o cualquier otro cliente gen\u00e9rico para Git. Uno de los m\u00e1s usados actualmente es GitKraken . Crear un repositorio Vamos a crear un repositorio donde guardar nuestro proyecto. Para ello pulsamos el signo + que hay en la barra superior y seleccionamos New repository . Ahora tenemos que designar un nombre para nuestro repositorio, por ejemplo: ' taller-de-git '. Nada m\u00e1s crear el repositorio nos saldr\u00e1 una pantalla con instrucciones precisas de como proceder a continuaci\u00f3n. B\u00e1sicamente podemos partir de tres situaciones: Todav\u00eda no hemos creado ning\u00fan repositorio en nuestro equipo. Ya tenemos un repositorio creado y queremos sincronizarlo con Github. Queremos importar un repositorio de otro sistema de control de versiones distinto. Nuestra situaci\u00f3n es la segunda, as\u00ed que nos aseguramos de que hemos elegido SSH como protocolo. A continuaci\u00f3n pulsamos el icono del portapapeles y ejecutamos las dos ordenes que nos indica la web en nuestro terminal. $ git remote add origin git@github.com:sgomez/taller-de-git.git $ git push -u origin master Counting objects: 33, done. Delta compression using up to 4 threads. Compressing objects: 100% (24/24), done. Writing objects: 100% (33/33), 3.35 KiB | 1.12 MiB/s, done. Total 33 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), done. To github.com:sgomez/taller-de-git.git * [new branch] master -> master Branch master set up to track remote branch master from origin by rebasing. Si recargamos la p\u00e1gina veremos que ya aparece nuestro proyecto. Clonar un repositorio Una vez que ya tengamos sincronizado el repositorio contra Github, eventualmente vamos a querer descargarlo en otro de nuestros ordenadores para poder trabajar en \u00e9l. Esta acci\u00f3n se denomina clonar y para ello usaremos la orden git clone . En la p\u00e1gina principal de nuestro proyecto podemos ver un bot\u00f3n que indica Clone or download . Si la pulsamos nos da, de nuevo, la opci\u00f3n de elegir entre clonar con ssh o https . Recordad que si est\u00e1is en otro equipo y quer\u00e9is seguir utilizando ssh deber\u00e9is generar otra para de claves privada/p\u00fablica como hicimos en la secci\u00f3n de Aspectos b\u00e1sicos de Git y instalarla en nuestro perfil de Github, como vimos anteriormente. Para clonar nuestro repositorio y poder trabajar con \u00e9l todo lo que debemos hacer es lo siguiente: $ git clone git@github.com:sgomez/taller-de-git.git $ cd taller-de-git Ramas remotas Si ahora vemos el estado de nuestro proyecto veremos algo similar a esto: $ git hist --all * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola (HEAD -> master, origin/master) [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Aparece que hay una nueva rama llamada origin/master . Esta rama indica el estado de sincronizaci\u00f3n de nuestro repositorio con un repositorio remoto llamado origin . En este caso el de Github . Info Por norma se llama autom\u00e1ticamente origin al primer repositorio con el que sincronizamos nuestro repositorio. Podemos ver la configuraci\u00f3n de este repositorio remoto con la orden git remote : $ git remote show origin * remote origin Fetch URL: git@github.com:sgomez/taller-de-git.git Push URL: git@github.com:sgomez/taller-de-git.git HEAD branch: master Remote branch: master tracked Local ref configured for 'git push': master pushes to master (up to date) De la respuesta tenemos que fijarnos en las l\u00edneas que indican fetch y push puesto que son las acciones de sincronizaci\u00f3n de nuestro repositorio con el remoto. Mientras que fetch se encarga de traer los cambios desde el repositorio remoto al nuestro, push los env\u00eda. Enviando actualizaciones Vamos a a\u00f1adir una licencia a nuestra aplicaci\u00f3n. Creamos un fichero LICENSE con el siguiente contenido: MIT License Copyright (c) [year] [fullname] Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Y a\u00f1adidos y confirmamos los cambios: $ git add LICENSE $ git commit -m \"A\u00f1adida licencia\" [master 3f5cb1c] A\u00f1adida licencia 1 file changed, 21 insertions(+) create mode 100644 LICENSE $ git hist --all * 3f5cb1c 2013-06-16 | A\u00f1adida licencia (HEAD -> master) [Sergio G\u00f3mez] * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola (origin/master) [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Viendo la historia podemos ver como nuestro master no est\u00e1 en el mismo punto que origin/master . Si vamos a la web de Github veremos que LICENSE no aparece a\u00fan. As\u00ed que vamos a enviar los cambios con la primera de las acciones que vimos git push : $ git push -u origin master Counting objects: 3, done. Delta compression using up to 4 threads. Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 941 bytes | 0 bytes/s, done. Total 3 (delta 0), reused 0 (delta 0) To git@github.com:sgomez/taller-de-git.git 2eab8ca..3f5cb1c master -> master Branch master set up to track remote branch master from origin. Info La orden git push necesita dos par\u00e1metros para funcionar: el repositorio y la rama destino. As\u00ed que realmente lo que ten\u00edamos que haber escrito es: $ git push origin master Para ahorrar tiempo escribiendo git nos deja vincular nuestra rama local con una rama remota, de tal manera que no tengamos que estar siempre indic\u00e1ndolo. Eso es posible con el par\u00e1metro --set-upstream o -u en forma abreviada. $ git push -u origin master Si repasas las \u00f3rdenes que te indic\u00f3 Github que ejecutaras ver\u00e1s que el par\u00e1metro -u estaba presente y por eso no ha sido necesario indicar ning\u00fan par\u00e1metro al hacer push. Recibiendo actualizaciones Si trabajamos con m\u00e1s personas, o trabajamos desde dos ordenadores distintos, nos encontraremos con que nuestro repositorio local es m\u00e1s antiguo que el remoto. Necesitamos descargar los cambios para poder incorporarlos a nuestro directorio de trabajo. Para la prueba, Github nos permite editar archivos directamente desde la web. Pulsamos sobre el archivo README.md . En la vista del archivo, veremos que aparece el icono de un l\u00e1piz. Esto nos permite editar el archivo. Info Los archivos con extensi\u00f3n .md est\u00e1n en un formato denominado MarkDown . Se trata de un lenguaje de marca que nos permite escribir texto enriquecido de manera muy sencilla. Dispones de un tutorial aqu\u00ed: https://www.markdowntutorial.com/ Modificamos el archivo como queramos, por ejemplo, a\u00f1adiendo nuestro nombre: # Curso de GIT Este proyecto contiene el curso de introducci\u00f3n a GIT Desarrollado por Sergio G\u00f3mez. El cambio quedar\u00e1 incorporado al repositorio de Github, pero no al nuestro. Necesitamos traer la informaci\u00f3n desde el servidor remoto. La orden asociada es git fetch : $ git fetch $ git hist --all * cbaf831 2013-06-16 | Actualizado README.md (origin/master) [Sergio G\u00f3mez] * 3f5cb1c 2013-06-16 | A\u00f1adida licencia (HEAD -> master) [Sergio G\u00f3mez] * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Ahora vemos el caso contrario, tenemos que origin/master est\u00e1 por delante que HEAD y que la rama master local. Ahora necesitamos incorporar los cambios de la rama remota en la local. La forma de hacerlo lo vimos en el cap\u00edtulo anterior usando git merge o git rebase . Habitualmente se usa git merge : $ git merge origin/master Updating 3f5cb1c..cbaf831 Fast-forward README.md | 2 ++ 1 file changed, 2 insertions(+) $ git hist --all * cbaf831 2013-06-16 | Actualizado README.md (HEAD -> master, origin/master) [Sergio G\u00f3mez] * 3f5cb1c 2013-06-16 | A\u00f1adida licencia [Sergio G\u00f3mez] * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Como las operaciones de traer cambios ( git fetch ) y de mezclar ramas ( git merge o git rebase ) est\u00e1n muy asociadas, git nos ofrece una posibilidad para ahorrar pasos que es la orden git pull que realiza las dos acciones simult\u00e1neamente. Para probar, vamos a editar de nuevo el archivo README.md y a\u00f1adimos algo m\u00e1s: # Curso de GIT Este proyecto contiene el curso de introducci\u00f3n a GIT del Aula de Software Libre. Desarrollado por Sergio G\u00f3mez. Como mensaje del commit : 'Indicado que se realiza en el ASL' . Y ahora probamos a actualizar con git pull : $ git pull remote: Counting objects: 3, done. remote: Compressing objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. From github.com:sgomez/taller-de-git cbaf831..d8922e4 master -> origin/master First, rewinding head to replay your work on top of it... Fast-forwarded master to d8922e4ffa4f87553b03e77df6196b7e496bfec4. $ git hist --all * d8922e4 2013-06-16 | Indicado que se realiza en el ASL (HEAD -> master, origin/master) [Sergio G\u00f3mez] * cbaf831 2013-06-16 | Actualizado README.md [Sergio G\u00f3mez] * 3f5cb1c 2013-06-16 | A\u00f1adida licencia [Sergio G\u00f3mez] * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Vemos que los cambios se han incorporado y que las ramas remota y local de master est\u00e1n sincronizadas. Problemas de sincronizaci\u00f3n No puedo hacer push Al intentar subir cambios nos podemos encontrar un mensaje como este: $ git push git push To git@github.com:sgomez/taller-de-git.git ! [rejected] master -> master (fetch first) error: failed to push some refs to 'git@github.com:sgomez/taller-de-git.git' hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., 'git pull ...') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details. La causa es que el repositorio remoto tambi\u00e9n se ha actualizado y nosotros a\u00fan no hemos recibido esos cambios. Es decir, ambos repositorios se han actualizado y el remoto tiene preferencia. Hay un conflicto en ciernes y se debe resolver localmente antes de continuar. Vamos a provocar una situaci\u00f3n donde podamos ver esto en acci\u00f3n. Vamos a modificar el archivo README.md tanto en local como en remoto a trav\u00e9s del interfaz web. En el web vamos a cambiar el t\u00edtulo para que aparezca de la siguiente manera. Curso de GIT, 2020 En local vamos a cambiar el t\u00edtulo para que aparezca de la siguiente manera. Curso de GIT, febrero Question Haz el commit para guardar el cambio en local. Respuesta al ejercicio anterior A\u00f1adimos el fichero actualizado: $ git commit -am \"A\u00f1adido el mes al README\" [master 1e8c0b7] A\u00f1adido el mes al README 1 file changed, 1 insertion(+), 1 deletion(-) La forma de proceder en este caso es hacer un git fetch y un git rebase . Si hay conflictos deber\u00e1n resolverse. Cuando est\u00e9 todo solucionado ya podremos hacer git push . Info Por defecto git pull lo que hace es un git merge , si queremos hacer git rebase deberemos especificarlos con el par\u00e1metro -r : $ git pull --rebase Vamos a hacer el pull con rebase y ver qu\u00e9 sucede. $ git pull --rebase First, rewinding head to replay your work on top of it... Applying: A\u00f1adido el mes al README Using index info to reconstruct a base tree... M README.md Falling back to patching base and 3-way merge... Auto-merging README.md CONFLICT (content): Merge conflict in README.md error: Failed to merge in the changes. Patch failed at 0001 A\u00f1adido el mes al README hint: Use 'git am --show-current-patch' to see the failed patch Resolve all conflicts manually, mark them as resolved with \"git add/rm <conflicted_files>\", then run \"git rebase --continue\". You can instead skip this commit: run \"git rebase --skip\". To abort and get back to the state before \"git rebase\", run \"git rebase --abort\". Evidentemente hay un conflicto porque hemos tocado el mismo archivo. Se deja como ejercicio resolverlo. Respuesta al ejercicio anterior El contenido del fichero final podr\u00eda ser: Curso de GIT, febrero, 2020 A continuaci\u00f3n confirmamos los cambios y los enviamos al servidor $ git add README.md $ git rebase --continue $ git push Warning \u00bfPor qu\u00e9 hemos hecho rebase en master si a lo largo del curso hemos dicho que no se debe cambiar la linea principal? B\u00e1sicamente hemos dicho que lo que no debemos hacer es modificar la l\u00ednea temporal compartida . En este caso nuestros cambios en master solo estaban en nuestro repositorio, porque al fallar el env\u00edo nadie m\u00e1s ha visto nuestras actualizaciones. Al hacer rebase estamos deshaciendo nuestros cambios, bajarnos la \u00faltima actualizaci\u00f3n compartida de master y volvi\u00e9ndolos a aplicar. Con lo que realmente la historia compartida no se ha modificado. Este es un problema que debemos evitar en la medida de lo posible. La menor cantidad de gente posible debe tener acceso de escritura en master y las actualizaciones de dicha rama deben hacerse a trav\u00e9s de ramas secundarias y haciendo merge en master como hemos visto en el cap\u00edtulo de ramas. No puedo hacer pull Al intentar descargar cambios nos podemos encontrar un mensaje como este: $ git pull error: Cannot pull with rebase: You have unstaged changes. O como este: $ git pull error: Cannot pull with rebase: Your index contains uncommitted changes. B\u00e1sicamente lo que ocurre es que tenemos cambios sin confirmar en nuestro espacio de trabajo. Una opci\u00f3n es confirmar ( commit ) y entonces proceder como el caso anterior. Pero puede ocurrir que a\u00fan estemos trabajando todav\u00eda y no nos interese confirmar los cambios, solo queremos sincronizar y seguir trabajando. Para casos como estos git ofrece una pila para guardar cambios temporalmente. Esta pila se llama stash y nos permite restaurar el espacio de trabajo al \u00faltimo commit. De nuevo vamos a modificar nuestro proyecto para ver esta situaci\u00f3n en acci\u00f3n. Example En remoto borra el a\u00f1o de la fecha y en local borra el mes. Pero esta vez no hagas commit en local . El archivo solo debe quedar modificado. La forma de proceder es la siguiente: $ git stash save # Guardamos los cambios en la pila $ git pull # Sincronizamos con el repositorio remoto, -r para hacer rebase puede ser requerido $ git stash pop # Sacamos los cambios de la pila Info Como ocurre habitualmente, git nos proporciona una forma de hacer todos estos pasos de una sola vez. Para ello tenemos que ejecutar lo siguiente: $ git pull --autostash En general no es mala idea ejecutar lo siguiente si somos conscientes, adem\u00e1s, de que tenemos varios cambios sin sincronizar: $ git pull --autostash --rebase Podr\u00eda darse el caso de que al sacar los cambios de la pila hubiera alg\u00fan conflicto. En ese caso actuamos como con el caso de merge o rebase . De nuevo este tipo de problemas no deben suceder si nos acostumbramos a trabajar en ramas.","title":"Github"},{"location":"github/#github","text":"Github es lo que se denomina una forja, un repositorio de proyectos que usan Git como sistema de control de versiones. Es la forja m\u00e1s popular, ya que alberga m\u00e1s de 10 millones de repositorios. Debe su popularidad a sus funcionalidades sociales, principalmente dos: la posibilidad de hacer forks de otros proyectos y la posibilidad de cooperar aportando c\u00f3digo para arreglar errores o mejorar el c\u00f3digo. Si bien, no es que fuera una novedad, s\u00ed lo es lo f\u00e1cil que resulta hacerlo. A ra\u00edz de este proyecto han surgido otros como Gitorius o Gitlab , pero Github sigue siendo el m\u00e1s popular y el que tiene mejores y mayores caracter\u00edsticas. algunas de estas son: Un wiki para documentar el proyecto, que usa MarkDown como lenguaje de marca. Un portal web para cada proyecto. Funcionalidades de redes sociales como followers. Gr\u00e1ficos estad\u00edsticos. Revisi\u00f3n de c\u00f3digo y comentarios. Sistemas de seguimiento de incidencias. Lo primero es entrar en el portal ( https://github.com/ ) para crearnos una cuenta si no la tenemos a\u00fan.","title":"Github"},{"location":"github/#tu-clave-publicaprivada","text":"Muchos servidores Git utilizan la autentificaci\u00f3n a trav\u00e9s de claves p\u00fablicas SSH. Y, para ello, cada usuario del sistema ha de generarse una, si es que no la tiene ya. El proceso para hacerlo es similar en casi cualquier sistema operativo. Ante todo, asegurarte que no tengas ya una clave. (comprueba que el directorio $HOME/usuario/.ssh no tiene un archivo id_dsa.pub o id_rsa.pub). Para crear una nueva clave usamos la siguiente orden: $ ssh-keygen -t rsa -C \"Cuenta Thinstation\" Warning Tu clave RSA te identifica contra los repositorios remotos, aseg\u00farate de no compartir la clave privada con nadie. Por defecto la clave se crea como solo lectura .","title":"Tu clave p\u00fablica/privada"},{"location":"github/#configuracion","text":"Vamos a aprovechar para a\u00f1adir la clave RSA que generamos antes, para poder acceder desde git a los repositorios. Para ellos nos vamos al men\u00fa de configuraci\u00f3n de usuario ( Settings ) Nos vamos al men\u00fa 'SSH and GPG Keys' y a\u00f1adimos una nueva clave. En Title indicamos una descripci\u00f3n que nos ayude a saber de d\u00f3nde procede la clave y en key volcamos el contenido del archivo ~/.ssh/id_rsa.pub . Y guardamos la clave. Con esto ya tendriamos todo nuestro entorno para poder empezar a trabajar desde nuestro equipo.","title":"Configuraci\u00f3n"},{"location":"github/#clientes-graficos-para-github","text":"Adem\u00e1s, para Github existe un cliente propio tanto para Windows como para MacOSX: Cliente Windows: http://windows.github.com/ Cliente MacOSX: http://mac.github.com/ Para Linux no hay cliente propio, pero s\u00ed hay plugin para la mayor\u00eda de editores de texto como atom, netbeans, eclipe o los editores de jetbrains. De todas maneras, estos clientes solo tienen el fin de facilitar el uso de Github, pero no son necesarios para usarlo. Es perfectamente v\u00e1lido usar el cliente de consola de Git o cualquier otro cliente gen\u00e9rico para Git. Uno de los m\u00e1s usados actualmente es GitKraken .","title":"Clientes gr\u00e1ficos para GitHub"},{"location":"github/#crear-un-repositorio","text":"Vamos a crear un repositorio donde guardar nuestro proyecto. Para ello pulsamos el signo + que hay en la barra superior y seleccionamos New repository . Ahora tenemos que designar un nombre para nuestro repositorio, por ejemplo: ' taller-de-git '. Nada m\u00e1s crear el repositorio nos saldr\u00e1 una pantalla con instrucciones precisas de como proceder a continuaci\u00f3n. B\u00e1sicamente podemos partir de tres situaciones: Todav\u00eda no hemos creado ning\u00fan repositorio en nuestro equipo. Ya tenemos un repositorio creado y queremos sincronizarlo con Github. Queremos importar un repositorio de otro sistema de control de versiones distinto. Nuestra situaci\u00f3n es la segunda, as\u00ed que nos aseguramos de que hemos elegido SSH como protocolo. A continuaci\u00f3n pulsamos el icono del portapapeles y ejecutamos las dos ordenes que nos indica la web en nuestro terminal. $ git remote add origin git@github.com:sgomez/taller-de-git.git $ git push -u origin master Counting objects: 33, done. Delta compression using up to 4 threads. Compressing objects: 100% (24/24), done. Writing objects: 100% (33/33), 3.35 KiB | 1.12 MiB/s, done. Total 33 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), done. To github.com:sgomez/taller-de-git.git * [new branch] master -> master Branch master set up to track remote branch master from origin by rebasing. Si recargamos la p\u00e1gina veremos que ya aparece nuestro proyecto.","title":"Crear un repositorio"},{"location":"github/#clonar-un-repositorio","text":"Una vez que ya tengamos sincronizado el repositorio contra Github, eventualmente vamos a querer descargarlo en otro de nuestros ordenadores para poder trabajar en \u00e9l. Esta acci\u00f3n se denomina clonar y para ello usaremos la orden git clone . En la p\u00e1gina principal de nuestro proyecto podemos ver un bot\u00f3n que indica Clone or download . Si la pulsamos nos da, de nuevo, la opci\u00f3n de elegir entre clonar con ssh o https . Recordad que si est\u00e1is en otro equipo y quer\u00e9is seguir utilizando ssh deber\u00e9is generar otra para de claves privada/p\u00fablica como hicimos en la secci\u00f3n de Aspectos b\u00e1sicos de Git y instalarla en nuestro perfil de Github, como vimos anteriormente. Para clonar nuestro repositorio y poder trabajar con \u00e9l todo lo que debemos hacer es lo siguiente: $ git clone git@github.com:sgomez/taller-de-git.git $ cd taller-de-git","title":"Clonar un repositorio"},{"location":"github/#ramas-remotas","text":"Si ahora vemos el estado de nuestro proyecto veremos algo similar a esto: $ git hist --all * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola (HEAD -> master, origin/master) [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Aparece que hay una nueva rama llamada origin/master . Esta rama indica el estado de sincronizaci\u00f3n de nuestro repositorio con un repositorio remoto llamado origin . En este caso el de Github . Info Por norma se llama autom\u00e1ticamente origin al primer repositorio con el que sincronizamos nuestro repositorio. Podemos ver la configuraci\u00f3n de este repositorio remoto con la orden git remote : $ git remote show origin * remote origin Fetch URL: git@github.com:sgomez/taller-de-git.git Push URL: git@github.com:sgomez/taller-de-git.git HEAD branch: master Remote branch: master tracked Local ref configured for 'git push': master pushes to master (up to date) De la respuesta tenemos que fijarnos en las l\u00edneas que indican fetch y push puesto que son las acciones de sincronizaci\u00f3n de nuestro repositorio con el remoto. Mientras que fetch se encarga de traer los cambios desde el repositorio remoto al nuestro, push los env\u00eda.","title":"Ramas remotas"},{"location":"github/#enviando-actualizaciones","text":"Vamos a a\u00f1adir una licencia a nuestra aplicaci\u00f3n. Creamos un fichero LICENSE con el siguiente contenido: MIT License Copyright (c) [year] [fullname] Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Y a\u00f1adidos y confirmamos los cambios: $ git add LICENSE $ git commit -m \"A\u00f1adida licencia\" [master 3f5cb1c] A\u00f1adida licencia 1 file changed, 21 insertions(+) create mode 100644 LICENSE $ git hist --all * 3f5cb1c 2013-06-16 | A\u00f1adida licencia (HEAD -> master) [Sergio G\u00f3mez] * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola (origin/master) [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Viendo la historia podemos ver como nuestro master no est\u00e1 en el mismo punto que origin/master . Si vamos a la web de Github veremos que LICENSE no aparece a\u00fan. As\u00ed que vamos a enviar los cambios con la primera de las acciones que vimos git push : $ git push -u origin master Counting objects: 3, done. Delta compression using up to 4 threads. Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 941 bytes | 0 bytes/s, done. Total 3 (delta 0), reused 0 (delta 0) To git@github.com:sgomez/taller-de-git.git 2eab8ca..3f5cb1c master -> master Branch master set up to track remote branch master from origin. Info La orden git push necesita dos par\u00e1metros para funcionar: el repositorio y la rama destino. As\u00ed que realmente lo que ten\u00edamos que haber escrito es: $ git push origin master Para ahorrar tiempo escribiendo git nos deja vincular nuestra rama local con una rama remota, de tal manera que no tengamos que estar siempre indic\u00e1ndolo. Eso es posible con el par\u00e1metro --set-upstream o -u en forma abreviada. $ git push -u origin master Si repasas las \u00f3rdenes que te indic\u00f3 Github que ejecutaras ver\u00e1s que el par\u00e1metro -u estaba presente y por eso no ha sido necesario indicar ning\u00fan par\u00e1metro al hacer push.","title":"Enviando actualizaciones"},{"location":"github/#recibiendo-actualizaciones","text":"Si trabajamos con m\u00e1s personas, o trabajamos desde dos ordenadores distintos, nos encontraremos con que nuestro repositorio local es m\u00e1s antiguo que el remoto. Necesitamos descargar los cambios para poder incorporarlos a nuestro directorio de trabajo. Para la prueba, Github nos permite editar archivos directamente desde la web. Pulsamos sobre el archivo README.md . En la vista del archivo, veremos que aparece el icono de un l\u00e1piz. Esto nos permite editar el archivo. Info Los archivos con extensi\u00f3n .md est\u00e1n en un formato denominado MarkDown . Se trata de un lenguaje de marca que nos permite escribir texto enriquecido de manera muy sencilla. Dispones de un tutorial aqu\u00ed: https://www.markdowntutorial.com/ Modificamos el archivo como queramos, por ejemplo, a\u00f1adiendo nuestro nombre: # Curso de GIT Este proyecto contiene el curso de introducci\u00f3n a GIT Desarrollado por Sergio G\u00f3mez. El cambio quedar\u00e1 incorporado al repositorio de Github, pero no al nuestro. Necesitamos traer la informaci\u00f3n desde el servidor remoto. La orden asociada es git fetch : $ git fetch $ git hist --all * cbaf831 2013-06-16 | Actualizado README.md (origin/master) [Sergio G\u00f3mez] * 3f5cb1c 2013-06-16 | A\u00f1adida licencia (HEAD -> master) [Sergio G\u00f3mez] * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Ahora vemos el caso contrario, tenemos que origin/master est\u00e1 por delante que HEAD y que la rama master local. Ahora necesitamos incorporar los cambios de la rama remota en la local. La forma de hacerlo lo vimos en el cap\u00edtulo anterior usando git merge o git rebase . Habitualmente se usa git merge : $ git merge origin/master Updating 3f5cb1c..cbaf831 Fast-forward README.md | 2 ++ 1 file changed, 2 insertions(+) $ git hist --all * cbaf831 2013-06-16 | Actualizado README.md (HEAD -> master, origin/master) [Sergio G\u00f3mez] * 3f5cb1c 2013-06-16 | A\u00f1adida licencia [Sergio G\u00f3mez] * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Como las operaciones de traer cambios ( git fetch ) y de mezclar ramas ( git merge o git rebase ) est\u00e1n muy asociadas, git nos ofrece una posibilidad para ahorrar pasos que es la orden git pull que realiza las dos acciones simult\u00e1neamente. Para probar, vamos a editar de nuevo el archivo README.md y a\u00f1adimos algo m\u00e1s: # Curso de GIT Este proyecto contiene el curso de introducci\u00f3n a GIT del Aula de Software Libre. Desarrollado por Sergio G\u00f3mez. Como mensaje del commit : 'Indicado que se realiza en el ASL' . Y ahora probamos a actualizar con git pull : $ git pull remote: Counting objects: 3, done. remote: Compressing objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. From github.com:sgomez/taller-de-git cbaf831..d8922e4 master -> origin/master First, rewinding head to replay your work on top of it... Fast-forwarded master to d8922e4ffa4f87553b03e77df6196b7e496bfec4. $ git hist --all * d8922e4 2013-06-16 | Indicado que se realiza en el ASL (HEAD -> master, origin/master) [Sergio G\u00f3mez] * cbaf831 2013-06-16 | Actualizado README.md [Sergio G\u00f3mez] * 3f5cb1c 2013-06-16 | A\u00f1adida licencia [Sergio G\u00f3mez] * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Vemos que los cambios se han incorporado y que las ramas remota y local de master est\u00e1n sincronizadas.","title":"Recibiendo actualizaciones"},{"location":"github/#problemas-de-sincronizacion","text":"","title":"Problemas de sincronizaci\u00f3n"},{"location":"github/#no-puedo-hacer-push","text":"Al intentar subir cambios nos podemos encontrar un mensaje como este: $ git push git push To git@github.com:sgomez/taller-de-git.git ! [rejected] master -> master (fetch first) error: failed to push some refs to 'git@github.com:sgomez/taller-de-git.git' hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., 'git pull ...') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details. La causa es que el repositorio remoto tambi\u00e9n se ha actualizado y nosotros a\u00fan no hemos recibido esos cambios. Es decir, ambos repositorios se han actualizado y el remoto tiene preferencia. Hay un conflicto en ciernes y se debe resolver localmente antes de continuar. Vamos a provocar una situaci\u00f3n donde podamos ver esto en acci\u00f3n. Vamos a modificar el archivo README.md tanto en local como en remoto a trav\u00e9s del interfaz web. En el web vamos a cambiar el t\u00edtulo para que aparezca de la siguiente manera. Curso de GIT, 2020 En local vamos a cambiar el t\u00edtulo para que aparezca de la siguiente manera. Curso de GIT, febrero Question Haz el commit para guardar el cambio en local. Respuesta al ejercicio anterior A\u00f1adimos el fichero actualizado: $ git commit -am \"A\u00f1adido el mes al README\" [master 1e8c0b7] A\u00f1adido el mes al README 1 file changed, 1 insertion(+), 1 deletion(-) La forma de proceder en este caso es hacer un git fetch y un git rebase . Si hay conflictos deber\u00e1n resolverse. Cuando est\u00e9 todo solucionado ya podremos hacer git push . Info Por defecto git pull lo que hace es un git merge , si queremos hacer git rebase deberemos especificarlos con el par\u00e1metro -r : $ git pull --rebase Vamos a hacer el pull con rebase y ver qu\u00e9 sucede. $ git pull --rebase First, rewinding head to replay your work on top of it... Applying: A\u00f1adido el mes al README Using index info to reconstruct a base tree... M README.md Falling back to patching base and 3-way merge... Auto-merging README.md CONFLICT (content): Merge conflict in README.md error: Failed to merge in the changes. Patch failed at 0001 A\u00f1adido el mes al README hint: Use 'git am --show-current-patch' to see the failed patch Resolve all conflicts manually, mark them as resolved with \"git add/rm <conflicted_files>\", then run \"git rebase --continue\". You can instead skip this commit: run \"git rebase --skip\". To abort and get back to the state before \"git rebase\", run \"git rebase --abort\". Evidentemente hay un conflicto porque hemos tocado el mismo archivo. Se deja como ejercicio resolverlo. Respuesta al ejercicio anterior El contenido del fichero final podr\u00eda ser: Curso de GIT, febrero, 2020 A continuaci\u00f3n confirmamos los cambios y los enviamos al servidor $ git add README.md $ git rebase --continue $ git push Warning \u00bfPor qu\u00e9 hemos hecho rebase en master si a lo largo del curso hemos dicho que no se debe cambiar la linea principal? B\u00e1sicamente hemos dicho que lo que no debemos hacer es modificar la l\u00ednea temporal compartida . En este caso nuestros cambios en master solo estaban en nuestro repositorio, porque al fallar el env\u00edo nadie m\u00e1s ha visto nuestras actualizaciones. Al hacer rebase estamos deshaciendo nuestros cambios, bajarnos la \u00faltima actualizaci\u00f3n compartida de master y volvi\u00e9ndolos a aplicar. Con lo que realmente la historia compartida no se ha modificado. Este es un problema que debemos evitar en la medida de lo posible. La menor cantidad de gente posible debe tener acceso de escritura en master y las actualizaciones de dicha rama deben hacerse a trav\u00e9s de ramas secundarias y haciendo merge en master como hemos visto en el cap\u00edtulo de ramas.","title":"No puedo hacer push"},{"location":"github/#no-puedo-hacer-pull","text":"Al intentar descargar cambios nos podemos encontrar un mensaje como este: $ git pull error: Cannot pull with rebase: You have unstaged changes. O como este: $ git pull error: Cannot pull with rebase: Your index contains uncommitted changes. B\u00e1sicamente lo que ocurre es que tenemos cambios sin confirmar en nuestro espacio de trabajo. Una opci\u00f3n es confirmar ( commit ) y entonces proceder como el caso anterior. Pero puede ocurrir que a\u00fan estemos trabajando todav\u00eda y no nos interese confirmar los cambios, solo queremos sincronizar y seguir trabajando. Para casos como estos git ofrece una pila para guardar cambios temporalmente. Esta pila se llama stash y nos permite restaurar el espacio de trabajo al \u00faltimo commit. De nuevo vamos a modificar nuestro proyecto para ver esta situaci\u00f3n en acci\u00f3n. Example En remoto borra el a\u00f1o de la fecha y en local borra el mes. Pero esta vez no hagas commit en local . El archivo solo debe quedar modificado. La forma de proceder es la siguiente: $ git stash save # Guardamos los cambios en la pila $ git pull # Sincronizamos con el repositorio remoto, -r para hacer rebase puede ser requerido $ git stash pop # Sacamos los cambios de la pila Info Como ocurre habitualmente, git nos proporciona una forma de hacer todos estos pasos de una sola vez. Para ello tenemos que ejecutar lo siguiente: $ git pull --autostash En general no es mala idea ejecutar lo siguiente si somos conscientes, adem\u00e1s, de que tenemos varios cambios sin sincronizar: $ git pull --autostash --rebase Podr\u00eda darse el caso de que al sacar los cambios de la pila hubiera alg\u00fan conflicto. En ese caso actuamos como con el caso de merge o rebase . De nuevo este tipo de problemas no deben suceder si nos acostumbramos a trabajar en ramas.","title":"No puedo hacer pull"},{"location":"images/","text":"Im\u00e1genes Las im\u00e1genes son la base de Docker. Nuestros contenedores se iniciar\u00e1n a partir de ellas. Como se indic\u00f3 en la introducci\u00f3n, es una plantilla de solo lectura, que se crea incorporando los requisitos necesarios para cumplir el objetivo para el cual fue creada. Por ejemplo, si estamos creando un proyecto con PHP, incorporar\u00e1 el int\u00e9rprete del lenguaje de PHP. Si es una p\u00e1gina web, incorporar\u00e1 el servidor web ( apache , nginx , etc.). Buscar im\u00e1genes Crear una imagen desde cero supone un esfuerzo demasiado grande, as\u00ed que lo normal es partir o usar una ya creada. Para ellos buscaremos en los registros, el lugar donde se almacenan. Hay un registro oficial ( https://hub.docker.com ), pero nada impide a otras organizaciones, o a nosotros mismo, tener un registro propio. Estos registros pueden ser privados o p\u00fablicos. Imaginemos que queremos crear una web con WordPress . Si buscamos en el registro encontraremos una imagen llamada wordpress , con la etiqueta oficial. La recomendaci\u00f3n es que siempre busquemos im\u00e1genes oficiales, est\u00e1n mantenidas y bien documentadas. En la p\u00e1gina encontraremos las diferentes opciones que tiene esta imagen para configurarla, aunque las veremos con m\u00e1s detalle m\u00e1s adelante. Por ahora iniciemos la imagen como se indica: Example Iniciar una imagen de WordPress : docker run -p 8080:80 wordpress Y comprobaremos como se inicia el contenedor: $ docker run -p 8080 :80 wordpress Unable to find image 'wordpress:latest' locally latest: Pulling from library/wordpress 802b00ed6f79: Pull complete 59f5a5a895f8: Pull complete 6898b2dbcfeb: Pull complete 8e0903aaa47e: Pull complete 2961af1e196a: Pull complete 71f7016f79a0: Pull complete 5e1a48e5719c: Pull complete 7ae5291984f3: Pull complete 725b65166f31: Pull complete e90b121f9520: Pull complete b5a272809bbd: Pull complete f045f3ae0e2b: Pull complete 7f51c9ea2d8e: Pull complete 5aa9d0ed164a: Pull complete 8eea44e2bfc7: Pull complete 48918885026e: Pull complete 8ac3e8ada01a: Pull complete d3da911b920f: Pull complete 94c7e0af5b20: Pull complete e1f39ac90dec: Pull complete Digest: sha256:7121cdf8e9f01816653a3b2d2f4fc7bfe1dab956f00db5c7e7689e5f1454029a Status: Downloaded newer image for wordpress:latest WordPress not found in /var/www/html - copying now... Complete! WordPress has been successfully copied to /var/www/html AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 10.17.0.1. Set the 'ServerName' directive globally to suppress this message AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 10.17.0.1. Set the 'ServerName' directive globally to suppress this message [DDD mmm dd hh:mm:ss.iiiiii yyyy] [mpm_prefork:notice] [pid 1] AH00163: Apache/2.4.25 (Debian) PHP/7.2.10 configured -- resuming normal operations [DDD mmm dd hh:mm:ss.iiiiii yyyy] [core:notice] [pid 1] AH00094: Command line: 'apache2 -D FOREGROUND' Vemos en la l\u00ednea nueva un nuevo par\u00e1metro: -p 8080:80 . Por defecto, un contenedor est\u00e1 totalmente aislado. Pero si estamos montando un blog con WordPress vamos a necesitar acceder a \u00e9l desde el navegador. Con el par\u00e1metro -p , versi\u00f3n corta de --publish , podemos indicar que estamos enlazando un puerto de la m\u00e1quina anfitri\u00f3n con el contenedor. En este caso estamos enlazando el puerto 8080 de la m\u00e1quina anfitri\u00f3n con el 80 del contenedor. Info No vamos a explicar todas las opciones posibles, el tutorial ser\u00eda demasiado largo. Puedes consultar la p\u00e1gina del manual con man docker-run o llamando a la ayuda desde el cliente con docker run --help . En este caso, el formato de publish es: -p, --publish ip:[hostPort]:containerPort | [hostPort:]containerPort Publish a container's port, or range of ports, to the host. Both hostPort and containerPort can be specified as a range. When specifying ranges for both, the number of ports in ranges should be equal. Examples: -p 1234-1236:1222-1224, -p 127.0.0.1:$HOSTPORT:$CONTAINERPORT. Use docker port(1) to see the actual mapping, e.g. docker port CONTAINER $CONTAINERPORT. Vamos a abrir la siguiente p\u00e1gina web en nuestro navegador: http://localhost:8080 La cual nos mostrar\u00e1 el asistente de instalaci\u00f3n de WordPress , el cual no vamos a instalar porque necesitamos una base de datos que a\u00fan no tenemos. En su lugar vamos a la consola e interrumpimos la ejecuci\u00f3n del contenedor con Control+C : ^C[DDD mmm dd hh:mm:ss.iiiiii yyyy] [mpm_prefork:notice] [pid 1] AH00169: caught SIGTERM, shutting down Gesti\u00f3n de im\u00e1genes Descarga Las imagenes que nos descargamos se identifican, adem\u00e1s de por el nombre, por una versi\u00f3n. De esa manera podemos tener distintas versiones de una misma imagen. En la p\u00e1gina del registro de WordPress veremos una pesta\u00f1a con el nombre Tags , con las versiones disponibles. Para usar una en concreto se usa dos puntos seguido del nombre de la versi\u00f3n. Si no se indica nada, como hasta ahora, por defecto se descarga la etiquetada como latest . Podemos descargar im\u00e1genes con la orden docker pull : $ docker pull wordpress:latest latest: Pulling from library/wordpress Digest: sha256:7121cdf8e9f01816653a3b2d2f4fc7bfe1dab956f00db5c7e7689e5f1454029a Status: Image is up to date for wordpress:latest $ docker pull wordpress:php7.1 php7.1: Pulling from library/wordpress 802b00ed6f79: Already exists 59f5a5a895f8: Already exists 6898b2dbcfeb: Already exists 8e0903aaa47e: Already exists 2961af1e196a: Already exists 71f7016f79a0: Already exists 5e1a48e5719c: Already exists 7ae5291984f3: Already exists 725b65166f31: Already exists a2d738459b49: Pull complete 24830994a3eb: Pull complete b3807dc98c17: Pull complete 59365c2968b5: Pull complete 36bea53859bb: Pull complete a777908b01b4: Pull complete bd3efa4fff20: Pull complete 662f2add84f7: Pull complete 4340a5e4d9f8: Pull complete 2dbeaf456768: Pull complete Digest: sha256:2cc529d3d4ac538f8565d18a893bd1308d6f5522422f4696d87267695f69702c Status: Downloaded newer image for wordpress:php7.1 En el primer caso no hay descarga porque esa versi\u00f3n ya estaba descargada, en la segunda nos descargamos la versi\u00f3n de la imagen que usa php7.1 en vez de php7.2 Listado Para ver el listado de images disponibles usamos docker images : $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE wordpress latest ca0fefec932b 7 days ago 409MB wordpress php7.1 37664bd9863e 7 days ago 400MB hello-world latest 4ab4c602aa5e 2 weeks ago 1.84kB Borrado Si queremos dejar de usar alguna imagen usaremos docker rmi : $ docker rmi wordpress:php7.1 Untagged: wordpress:php7.1 Untagged: wordpress@sha256:2cc529d3d4ac538f8565d18a893bd1308d6f5522422f4696d87267695f69702c Deleted: sha256:37664bd9863efe67a83cb2ff293f1816a9b5f918668ae19ca36b2af3d3b9f62d Deleted: sha256:77c97f008777c89455c8e5f248a626b192b62cf07ed1993c9acdfab73be210ee Deleted: sha256:14f58345b0bb2efaede03f9424412dce141ea275343305a79952c9c8bda3d1ba Deleted: sha256:5902e2becea5be6d672e8a6a84cc66a2f3b8e1b209302a9995de2b9afac8535f Deleted: sha256:a5b592bce0a767eed15cce29e5e4a941341a0b8de1633ab8836079c03af31b9e Deleted: sha256:6cc2318a4f6975aa87358d9f2852d8b91b335515a1d42ef141af368ee0b6fc05 Deleted: sha256:c3c8b98ead26315e76cd9625fd59f67cab81afa7810b84a229f4e612097a3db4 Deleted: sha256:a641d6d5a4f43b035946f9a82c9f126189e8502567bb17c41d25e922a5b314a3 Deleted: sha256:a7338078acb6f6e8b1a152dabd6e7e47b3e530e1f2e2169b8b69127c9578f8fe Deleted: sha256:8f416a21cdea7d5b42d6b799ab4ade2dffe1f6a3b9d83dd02be47a82699922de Deleted: sha256:53862f425fbc706f70bd1238a0e929bf6d648547481acfad4910c4c1bde39b95 Warning Si una imagen est\u00e1 en uso por alg\u00fan contenedor, no nos dejar\u00e1 eliminarla. $ docker rmi hello-world:latest Error response from daemon: conflict: unable to remove repository reference \"hello-world:latest\" (must force) - container 5ae8bbb8768d is using its referenced image 4ab4c602aa5e","title":"Im\u00e1genes"},{"location":"images/#imagenes","text":"Las im\u00e1genes son la base de Docker. Nuestros contenedores se iniciar\u00e1n a partir de ellas. Como se indic\u00f3 en la introducci\u00f3n, es una plantilla de solo lectura, que se crea incorporando los requisitos necesarios para cumplir el objetivo para el cual fue creada. Por ejemplo, si estamos creando un proyecto con PHP, incorporar\u00e1 el int\u00e9rprete del lenguaje de PHP. Si es una p\u00e1gina web, incorporar\u00e1 el servidor web ( apache , nginx , etc.).","title":"Im\u00e1genes"},{"location":"images/#buscar-imagenes","text":"Crear una imagen desde cero supone un esfuerzo demasiado grande, as\u00ed que lo normal es partir o usar una ya creada. Para ellos buscaremos en los registros, el lugar donde se almacenan. Hay un registro oficial ( https://hub.docker.com ), pero nada impide a otras organizaciones, o a nosotros mismo, tener un registro propio. Estos registros pueden ser privados o p\u00fablicos. Imaginemos que queremos crear una web con WordPress . Si buscamos en el registro encontraremos una imagen llamada wordpress , con la etiqueta oficial. La recomendaci\u00f3n es que siempre busquemos im\u00e1genes oficiales, est\u00e1n mantenidas y bien documentadas. En la p\u00e1gina encontraremos las diferentes opciones que tiene esta imagen para configurarla, aunque las veremos con m\u00e1s detalle m\u00e1s adelante. Por ahora iniciemos la imagen como se indica: Example Iniciar una imagen de WordPress : docker run -p 8080:80 wordpress Y comprobaremos como se inicia el contenedor: $ docker run -p 8080 :80 wordpress Unable to find image 'wordpress:latest' locally latest: Pulling from library/wordpress 802b00ed6f79: Pull complete 59f5a5a895f8: Pull complete 6898b2dbcfeb: Pull complete 8e0903aaa47e: Pull complete 2961af1e196a: Pull complete 71f7016f79a0: Pull complete 5e1a48e5719c: Pull complete 7ae5291984f3: Pull complete 725b65166f31: Pull complete e90b121f9520: Pull complete b5a272809bbd: Pull complete f045f3ae0e2b: Pull complete 7f51c9ea2d8e: Pull complete 5aa9d0ed164a: Pull complete 8eea44e2bfc7: Pull complete 48918885026e: Pull complete 8ac3e8ada01a: Pull complete d3da911b920f: Pull complete 94c7e0af5b20: Pull complete e1f39ac90dec: Pull complete Digest: sha256:7121cdf8e9f01816653a3b2d2f4fc7bfe1dab956f00db5c7e7689e5f1454029a Status: Downloaded newer image for wordpress:latest WordPress not found in /var/www/html - copying now... Complete! WordPress has been successfully copied to /var/www/html AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 10.17.0.1. Set the 'ServerName' directive globally to suppress this message AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 10.17.0.1. Set the 'ServerName' directive globally to suppress this message [DDD mmm dd hh:mm:ss.iiiiii yyyy] [mpm_prefork:notice] [pid 1] AH00163: Apache/2.4.25 (Debian) PHP/7.2.10 configured -- resuming normal operations [DDD mmm dd hh:mm:ss.iiiiii yyyy] [core:notice] [pid 1] AH00094: Command line: 'apache2 -D FOREGROUND' Vemos en la l\u00ednea nueva un nuevo par\u00e1metro: -p 8080:80 . Por defecto, un contenedor est\u00e1 totalmente aislado. Pero si estamos montando un blog con WordPress vamos a necesitar acceder a \u00e9l desde el navegador. Con el par\u00e1metro -p , versi\u00f3n corta de --publish , podemos indicar que estamos enlazando un puerto de la m\u00e1quina anfitri\u00f3n con el contenedor. En este caso estamos enlazando el puerto 8080 de la m\u00e1quina anfitri\u00f3n con el 80 del contenedor. Info No vamos a explicar todas las opciones posibles, el tutorial ser\u00eda demasiado largo. Puedes consultar la p\u00e1gina del manual con man docker-run o llamando a la ayuda desde el cliente con docker run --help . En este caso, el formato de publish es: -p, --publish ip:[hostPort]:containerPort | [hostPort:]containerPort Publish a container's port, or range of ports, to the host. Both hostPort and containerPort can be specified as a range. When specifying ranges for both, the number of ports in ranges should be equal. Examples: -p 1234-1236:1222-1224, -p 127.0.0.1:$HOSTPORT:$CONTAINERPORT. Use docker port(1) to see the actual mapping, e.g. docker port CONTAINER $CONTAINERPORT. Vamos a abrir la siguiente p\u00e1gina web en nuestro navegador: http://localhost:8080 La cual nos mostrar\u00e1 el asistente de instalaci\u00f3n de WordPress , el cual no vamos a instalar porque necesitamos una base de datos que a\u00fan no tenemos. En su lugar vamos a la consola e interrumpimos la ejecuci\u00f3n del contenedor con Control+C : ^C[DDD mmm dd hh:mm:ss.iiiiii yyyy] [mpm_prefork:notice] [pid 1] AH00169: caught SIGTERM, shutting down","title":"Buscar im\u00e1genes"},{"location":"images/#gestion-de-imagenes","text":"","title":"Gesti\u00f3n de im\u00e1genes"},{"location":"images/#descarga","text":"Las imagenes que nos descargamos se identifican, adem\u00e1s de por el nombre, por una versi\u00f3n. De esa manera podemos tener distintas versiones de una misma imagen. En la p\u00e1gina del registro de WordPress veremos una pesta\u00f1a con el nombre Tags , con las versiones disponibles. Para usar una en concreto se usa dos puntos seguido del nombre de la versi\u00f3n. Si no se indica nada, como hasta ahora, por defecto se descarga la etiquetada como latest . Podemos descargar im\u00e1genes con la orden docker pull : $ docker pull wordpress:latest latest: Pulling from library/wordpress Digest: sha256:7121cdf8e9f01816653a3b2d2f4fc7bfe1dab956f00db5c7e7689e5f1454029a Status: Image is up to date for wordpress:latest $ docker pull wordpress:php7.1 php7.1: Pulling from library/wordpress 802b00ed6f79: Already exists 59f5a5a895f8: Already exists 6898b2dbcfeb: Already exists 8e0903aaa47e: Already exists 2961af1e196a: Already exists 71f7016f79a0: Already exists 5e1a48e5719c: Already exists 7ae5291984f3: Already exists 725b65166f31: Already exists a2d738459b49: Pull complete 24830994a3eb: Pull complete b3807dc98c17: Pull complete 59365c2968b5: Pull complete 36bea53859bb: Pull complete a777908b01b4: Pull complete bd3efa4fff20: Pull complete 662f2add84f7: Pull complete 4340a5e4d9f8: Pull complete 2dbeaf456768: Pull complete Digest: sha256:2cc529d3d4ac538f8565d18a893bd1308d6f5522422f4696d87267695f69702c Status: Downloaded newer image for wordpress:php7.1 En el primer caso no hay descarga porque esa versi\u00f3n ya estaba descargada, en la segunda nos descargamos la versi\u00f3n de la imagen que usa php7.1 en vez de php7.2","title":"Descarga"},{"location":"images/#listado","text":"Para ver el listado de images disponibles usamos docker images : $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE wordpress latest ca0fefec932b 7 days ago 409MB wordpress php7.1 37664bd9863e 7 days ago 400MB hello-world latest 4ab4c602aa5e 2 weeks ago 1.84kB","title":"Listado"},{"location":"images/#borrado","text":"Si queremos dejar de usar alguna imagen usaremos docker rmi : $ docker rmi wordpress:php7.1 Untagged: wordpress:php7.1 Untagged: wordpress@sha256:2cc529d3d4ac538f8565d18a893bd1308d6f5522422f4696d87267695f69702c Deleted: sha256:37664bd9863efe67a83cb2ff293f1816a9b5f918668ae19ca36b2af3d3b9f62d Deleted: sha256:77c97f008777c89455c8e5f248a626b192b62cf07ed1993c9acdfab73be210ee Deleted: sha256:14f58345b0bb2efaede03f9424412dce141ea275343305a79952c9c8bda3d1ba Deleted: sha256:5902e2becea5be6d672e8a6a84cc66a2f3b8e1b209302a9995de2b9afac8535f Deleted: sha256:a5b592bce0a767eed15cce29e5e4a941341a0b8de1633ab8836079c03af31b9e Deleted: sha256:6cc2318a4f6975aa87358d9f2852d8b91b335515a1d42ef141af368ee0b6fc05 Deleted: sha256:c3c8b98ead26315e76cd9625fd59f67cab81afa7810b84a229f4e612097a3db4 Deleted: sha256:a641d6d5a4f43b035946f9a82c9f126189e8502567bb17c41d25e922a5b314a3 Deleted: sha256:a7338078acb6f6e8b1a152dabd6e7e47b3e530e1f2e2169b8b69127c9578f8fe Deleted: sha256:8f416a21cdea7d5b42d6b799ab4ade2dffe1f6a3b9d83dd02be47a82699922de Deleted: sha256:53862f425fbc706f70bd1238a0e929bf6d648547481acfad4910c4c1bde39b95 Warning Si una imagen est\u00e1 en uso por alg\u00fan contenedor, no nos dejar\u00e1 eliminarla. $ docker rmi hello-world:latest Error response from daemon: conflict: unable to remove repository reference \"hello-world:latest\" (must force) - container 5ae8bbb8768d is using its referenced image 4ab4c602aa5e","title":"Borrado"},{"location":"index_docker/","text":"Reconocimiento Aviso Los apuntes del Tema 6 - Contenedores est\u00e1n basados \u00edntegramente en el taller m\u00e1s abajo nombrado y es justo su reconocimiento. Se reproduce pr\u00e1cticamente \u00edntegro (salvo cuestiones ajenas al objetivo del m\u00f3dulo) porque su licencia as\u00ed lo permite. Este taller forma parte de las actividades del Aula de Software Libre de la Universidad de C\u00f3rdoba . Info Se recomienda acudir al evento con tu propio port\u00e1til y con una distribuci\u00f3n GNU/Linux compatible con Docker : Ubuntu , Debian , Fedora o CentOS . Si no dispones de ordenador propio notif\u00edcalo a los organizaci\u00f3n en la web de la actividad. Warning Para evitar demoras innecesarias en el inicio de la actividad es necesario traer ya de casa Docker instalado. Para ello lee el cap\u00edtulo de Instalaci\u00f3n .","title":"Tema 6 - Contenedores"},{"location":"index_docker/#reconocimiento","text":"Aviso Los apuntes del Tema 6 - Contenedores est\u00e1n basados \u00edntegramente en el taller m\u00e1s abajo nombrado y es justo su reconocimiento. Se reproduce pr\u00e1cticamente \u00edntegro (salvo cuestiones ajenas al objetivo del m\u00f3dulo) porque su licencia as\u00ed lo permite. Este taller forma parte de las actividades del Aula de Software Libre de la Universidad de C\u00f3rdoba . Info Se recomienda acudir al evento con tu propio port\u00e1til y con una distribuci\u00f3n GNU/Linux compatible con Docker : Ubuntu , Debian , Fedora o CentOS . Si no dispones de ordenador propio notif\u00edcalo a los organizaci\u00f3n en la web de la actividad. Warning Para evitar demoras innecesarias en el inicio de la actividad es necesario traer ya de casa Docker instalado. Para ello lee el cap\u00edtulo de Instalaci\u00f3n .","title":"Reconocimiento"},{"location":"index_git/","text":"Inicio Atenci\u00f3n Los contenidos del Tema 5 - Control de versiones est\u00e1n basados integramente en el curso que se menciona a continuaci\u00f3n. Su licencia original permite la reutilizaci\u00f3n y difusi\u00f3n del mismo. Este taller forma parte de las actividades del Aula de Software Libre de la Universidad de C\u00f3rdoba . El contenido del mismo es en parte de producci\u00f3n propia, en parte de otros manuales libres que pueden encontrarse en la secci\u00f3n de Referencias . Contenido Inicio Sistemas de control de versiones Introducci\u00f3n a Git Aspectos b\u00e1sicos de Git Uso b\u00e1sico Uso avanzado Ramas Administraci\u00f3n de repositorios Flujo de trabajo con Git (git flow) Github Referencias Agradecimientos Este curso ha sido impartido por las siguientes personas: Adri\u00e1n L\u00f3pez H\u00e9ctor Romero Javier de Santiago Jos\u00e9 M\u00e1rquez Sergio G\u00f3mez Licencia El material est\u00e1 publicado con licencia Atribuci\u00f3n-NoComercial 4.0 Internacional (CC BY-NC 4.0)","title":"Tema 5 - Control de versiones"},{"location":"index_git/#inicio","text":"Atenci\u00f3n Los contenidos del Tema 5 - Control de versiones est\u00e1n basados integramente en el curso que se menciona a continuaci\u00f3n. Su licencia original permite la reutilizaci\u00f3n y difusi\u00f3n del mismo. Este taller forma parte de las actividades del Aula de Software Libre de la Universidad de C\u00f3rdoba . El contenido del mismo es en parte de producci\u00f3n propia, en parte de otros manuales libres que pueden encontrarse en la secci\u00f3n de Referencias .","title":"Inicio"},{"location":"index_git/#contenido","text":"Inicio Sistemas de control de versiones Introducci\u00f3n a Git Aspectos b\u00e1sicos de Git Uso b\u00e1sico Uso avanzado Ramas Administraci\u00f3n de repositorios Flujo de trabajo con Git (git flow) Github Referencias","title":"Contenido"},{"location":"index_git/#agradecimientos","text":"Este curso ha sido impartido por las siguientes personas: Adri\u00e1n L\u00f3pez H\u00e9ctor Romero Javier de Santiago Jos\u00e9 M\u00e1rquez Sergio G\u00f3mez","title":"Agradecimientos"},{"location":"index_git/#licencia","text":"El material est\u00e1 publicado con licencia Atribuci\u00f3n-NoComercial 4.0 Internacional (CC BY-NC 4.0)","title":"Licencia"},{"location":"installation/","text":"Instalaci\u00f3n Existe dos versiones de Docker, una libre y otra que no lo es. Nos ocuparemos exclusivamente de la primera: Docker CE (Community Edition) . Disponibilidad Docker CE est\u00e1 disponible para los siguientes sistemas GNU/Linux: CentOS, Debian, Fedora y Ubuntu. No todas est\u00e1n en m\u00faltiples arquitecturas, pero s\u00ed todas soportan x86_64/amd64 . Si tienes otra arquitectura u otro sistema es mejor que uses una m\u00e1quina virtual para arrancar una distribuci\u00f3n compatible. Para m\u00e1s informaci\u00f3n sobre sistemas privativos soportados, leer la secci\u00f3n de plataformas soportadas de la documentaci\u00f3n oficial. Instalaci\u00f3n Debido a que, dependiendo de la distribuci\u00f3n, la forma de instalarlo difiere, es mejor consultar la documentaci\u00f3n oficial para saber como instalar Docker en tu m\u00e1quina. Ubuntu: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Debian: https://docs.docker.com/install/linux/docker-ce/debian/ CentOS: https://docs.docker.com/install/linux/docker-ce/centos/ Fedora: https://docs.docker.com/install/linux/docker-ce/fedora/ Si quieres instalar y probar Linux por primera vez, te recomendamos que uses una versi\u00f3n LTS de Ubuntu , por ser f\u00e1cil de instalar y tener un ciclo de mantenimiento de seguridad ampliado. Obviamente necesitas tener conexi\u00f3n a Internet para instalar y probar Docker. Para saber si tienes Docker bien instalado, los tutoriales oficiales siempre te indican inicies un contenedor de ejemplo. Esto es lo que sucede: Example Los c\u00f3digos de ejemplo ir\u00e1n acompa\u00f1ados de una caja como esta para poder copiar y pegar los comandos. sudo docker run hello-world El resultado es el siguiente: $ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world d1725b59e92d: Pull complete Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ En la l\u00ednea 1 estamos ejecutando el cliente de Docker, y estamos indicando que queremos ejecutar un contenedor a partir de la imagen hello-world del registro p\u00fablico de Docker. Si es la primera vez que hemos ejecutado esa imagen, nos aparecer\u00e1 la l\u00ednea 2, que indica que la imagen no puede ser encontrada y va a proceder a buscarla, por defecto, en el registro p\u00fablico. Si tenemos conexi\u00f3n a Internet se descargar\u00e1 la imagen (l\u00ednea 6) y autom\u00e1ticamente crear\u00e1 un contenedor. Tanto si se ha descargado la imagen o ya estaba descargada, el contenedor se ejecutar\u00e1, obteniendo el texto de bienvenida que se ve en el cuadro anterior. Configuraci\u00f3n del usuario Si estamos usando Docker en nuestro ordenador personal, podemos configurar nuestro usuario para usar el cliente sin tener que poner sudo delante. Para ello ejecuta lo siguiente: Example A\u00f1ade tu usuario al grupo de docker. sudo usermod -aG docker $USER Para que los nuevos permisos surtan efecto, debes cerrar y volver a abrir la sesi\u00f3n. Para problemas relacionados con los permisos visitad la p\u00e1gina del manual oficial . Requisitos del curso Im\u00e1genes Es necesario traer ya instaladas ciertas im\u00e1genes de contenedores. Ejecuta los siguientes comandos en tu equipo (si te da error de permisos aseg\u00farate que has hecho el apartado anterior y abierto y cerrado la sesi\u00f3n). Example Instalar WordPress : docker pull wordpress:latest Example Instalar MariaDB : docker pull mariadb:latest Herramientas Tambi\u00e9n es necesario traer una herramienta llamada Docker Compose . Puedes instalarla con las instrucciones que hay en la p\u00e1gina de Instalaci\u00f3n de Docker Compose . Sin embargo, si usas Ubuntu o Debian puedes instalarlo de forma m\u00e1s f\u00e1cil con apt : Example Instalaci\u00f3n de Docker Compose : sudo apt install docker-compose","title":"Instalaci\u00f3n"},{"location":"installation/#instalacion","text":"Existe dos versiones de Docker, una libre y otra que no lo es. Nos ocuparemos exclusivamente de la primera: Docker CE (Community Edition) .","title":"Instalaci\u00f3n"},{"location":"installation/#disponibilidad","text":"Docker CE est\u00e1 disponible para los siguientes sistemas GNU/Linux: CentOS, Debian, Fedora y Ubuntu. No todas est\u00e1n en m\u00faltiples arquitecturas, pero s\u00ed todas soportan x86_64/amd64 . Si tienes otra arquitectura u otro sistema es mejor que uses una m\u00e1quina virtual para arrancar una distribuci\u00f3n compatible. Para m\u00e1s informaci\u00f3n sobre sistemas privativos soportados, leer la secci\u00f3n de plataformas soportadas de la documentaci\u00f3n oficial.","title":"Disponibilidad"},{"location":"installation/#instalacion_1","text":"Debido a que, dependiendo de la distribuci\u00f3n, la forma de instalarlo difiere, es mejor consultar la documentaci\u00f3n oficial para saber como instalar Docker en tu m\u00e1quina. Ubuntu: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Debian: https://docs.docker.com/install/linux/docker-ce/debian/ CentOS: https://docs.docker.com/install/linux/docker-ce/centos/ Fedora: https://docs.docker.com/install/linux/docker-ce/fedora/ Si quieres instalar y probar Linux por primera vez, te recomendamos que uses una versi\u00f3n LTS de Ubuntu , por ser f\u00e1cil de instalar y tener un ciclo de mantenimiento de seguridad ampliado. Obviamente necesitas tener conexi\u00f3n a Internet para instalar y probar Docker. Para saber si tienes Docker bien instalado, los tutoriales oficiales siempre te indican inicies un contenedor de ejemplo. Esto es lo que sucede: Example Los c\u00f3digos de ejemplo ir\u00e1n acompa\u00f1ados de una caja como esta para poder copiar y pegar los comandos. sudo docker run hello-world El resultado es el siguiente: $ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world d1725b59e92d: Pull complete Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ En la l\u00ednea 1 estamos ejecutando el cliente de Docker, y estamos indicando que queremos ejecutar un contenedor a partir de la imagen hello-world del registro p\u00fablico de Docker. Si es la primera vez que hemos ejecutado esa imagen, nos aparecer\u00e1 la l\u00ednea 2, que indica que la imagen no puede ser encontrada y va a proceder a buscarla, por defecto, en el registro p\u00fablico. Si tenemos conexi\u00f3n a Internet se descargar\u00e1 la imagen (l\u00ednea 6) y autom\u00e1ticamente crear\u00e1 un contenedor. Tanto si se ha descargado la imagen o ya estaba descargada, el contenedor se ejecutar\u00e1, obteniendo el texto de bienvenida que se ve en el cuadro anterior.","title":"Instalaci\u00f3n"},{"location":"installation/#configuracion-del-usuario","text":"Si estamos usando Docker en nuestro ordenador personal, podemos configurar nuestro usuario para usar el cliente sin tener que poner sudo delante. Para ello ejecuta lo siguiente: Example A\u00f1ade tu usuario al grupo de docker. sudo usermod -aG docker $USER Para que los nuevos permisos surtan efecto, debes cerrar y volver a abrir la sesi\u00f3n. Para problemas relacionados con los permisos visitad la p\u00e1gina del manual oficial .","title":"Configuraci\u00f3n del usuario"},{"location":"installation/#requisitos-del-curso","text":"","title":"Requisitos del curso"},{"location":"installation/#imagenes","text":"Es necesario traer ya instaladas ciertas im\u00e1genes de contenedores. Ejecuta los siguientes comandos en tu equipo (si te da error de permisos aseg\u00farate que has hecho el apartado anterior y abierto y cerrado la sesi\u00f3n). Example Instalar WordPress : docker pull wordpress:latest Example Instalar MariaDB : docker pull mariadb:latest","title":"Im\u00e1genes"},{"location":"installation/#herramientas","text":"Tambi\u00e9n es necesario traer una herramienta llamada Docker Compose . Puedes instalarla con las instrucciones que hay en la p\u00e1gina de Instalaci\u00f3n de Docker Compose . Sin embargo, si usas Ubuntu o Debian puedes instalarlo de forma m\u00e1s f\u00e1cil con apt : Example Instalaci\u00f3n de Docker Compose : sudo apt install docker-compose","title":"Herramientas"},{"location":"introduccion/","text":"Introducci\u00f3n a git Git es un sistema de control de versiones distribuido que se diferencia del resto en el modo en que modela sus datos. La mayor\u00eda de los dem\u00e1s sistemas almacenan la informaci\u00f3n como una lista de cambios en los archivos, mientras que Git modela sus datos m\u00e1s como un conjunto de instant\u00e1neas de un mini sistema de archivos. Los tres estados Git tiene tres estados principales en los que se pueden encontrar tus archivos: confirmado (committed), modificado (modified), y preparado (staged). Confirmado significa que los datos est\u00e1n almacenados de manera segura en tu base de datos local. Modificado significa que has modificado el archivo pero todav\u00eda no lo has confirmado a tu base de datos. Preparado significa que has marcado un archivo modificado en su versi\u00f3n actual para que vaya en tu pr\u00f3xima confirmaci\u00f3n. Esto nos lleva a las tres secciones principales de un proyecto de Git: el directorio de Git (Git directory), el directorio de trabajo (working directory), y el \u00e1rea de preparaci\u00f3n (staging area). Flujos de trabajo distribuidos con git Hemos visto en qu\u00e9 consiste un entorno de control de versiones distribuido, pero m\u00e1s all\u00e1 de la simple definici\u00f3n, existe m\u00e1s de una manera de gestionar los repositorios. Estos son los flujos de trabajo m\u00e1s comunes en Git. Flujo de trabajo centralizado Existe un \u00fanico repositorio o punto central que guarda el c\u00f3digo y todo el mundo sincroniza su trabajo con \u00e9l. Si dos desarrolladores clonan desde el punto central, y ambos hacen cambios; tan solo el primero de ellos en enviar sus cambios de vuelta lo podr\u00e1 hacer limpiamente. El segundo desarrollador deber\u00e1 fusionar previamente su trabajo con el del primero, antes de enviarlo, para evitar el sobreescribir los cambios del primero Flujo de trabajo del Gestor-de-Integraciones Al permitir m\u00faltiples repositorios remotos, en Git es posible tener un flujo de trabajo donde cada desarrollador tenga acceso de escritura a su propio repositorio p\u00fablico y acceso de lectura a los repositorios de todos los dem\u00e1s. Habitualmente, este escenario suele incluir un repositorio can\u00f3nico, representante \"oficial\" del proyecto. Info Este modelo se puso muy de moda a ra\u00edz de la forja GitHub que se ver\u00e1 m\u00e1s adelante. Flujo de trabajo con Dictador y Tenientes Es una variante del flujo de trabajo con m\u00faltiples repositorios. Se utiliza generalmente en proyectos muy grandes, con cientos de colaboradores. Un ejemplo muy conocido es el del kernel de Linux. Unos gestores de integraci\u00f3n se encargan de partes concretas del repositorio; y se denominan tenientes. Todos los tenientes rinden cuentas a un gestor de integraci\u00f3n; conocido como el dictador benevolente. El repositorio del dictador benevolente es el repositorio de referencia, del que recuperan (pull) todos los colaboradores.","title":"Introducci\u00f3n a git"},{"location":"introduccion/#introduccion-a-git","text":"Git es un sistema de control de versiones distribuido que se diferencia del resto en el modo en que modela sus datos. La mayor\u00eda de los dem\u00e1s sistemas almacenan la informaci\u00f3n como una lista de cambios en los archivos, mientras que Git modela sus datos m\u00e1s como un conjunto de instant\u00e1neas de un mini sistema de archivos.","title":"Introducci\u00f3n a git"},{"location":"introduccion/#los-tres-estados","text":"Git tiene tres estados principales en los que se pueden encontrar tus archivos: confirmado (committed), modificado (modified), y preparado (staged). Confirmado significa que los datos est\u00e1n almacenados de manera segura en tu base de datos local. Modificado significa que has modificado el archivo pero todav\u00eda no lo has confirmado a tu base de datos. Preparado significa que has marcado un archivo modificado en su versi\u00f3n actual para que vaya en tu pr\u00f3xima confirmaci\u00f3n. Esto nos lleva a las tres secciones principales de un proyecto de Git: el directorio de Git (Git directory), el directorio de trabajo (working directory), y el \u00e1rea de preparaci\u00f3n (staging area).","title":"Los tres estados"},{"location":"introduccion/#flujos-de-trabajo-distribuidos-con-git","text":"Hemos visto en qu\u00e9 consiste un entorno de control de versiones distribuido, pero m\u00e1s all\u00e1 de la simple definici\u00f3n, existe m\u00e1s de una manera de gestionar los repositorios. Estos son los flujos de trabajo m\u00e1s comunes en Git.","title":"Flujos de trabajo distribuidos con git"},{"location":"introduccion/#flujo-de-trabajo-centralizado","text":"Existe un \u00fanico repositorio o punto central que guarda el c\u00f3digo y todo el mundo sincroniza su trabajo con \u00e9l. Si dos desarrolladores clonan desde el punto central, y ambos hacen cambios; tan solo el primero de ellos en enviar sus cambios de vuelta lo podr\u00e1 hacer limpiamente. El segundo desarrollador deber\u00e1 fusionar previamente su trabajo con el del primero, antes de enviarlo, para evitar el sobreescribir los cambios del primero","title":"Flujo de trabajo centralizado"},{"location":"introduccion/#flujo-de-trabajo-del-gestor-de-integraciones","text":"Al permitir m\u00faltiples repositorios remotos, en Git es posible tener un flujo de trabajo donde cada desarrollador tenga acceso de escritura a su propio repositorio p\u00fablico y acceso de lectura a los repositorios de todos los dem\u00e1s. Habitualmente, este escenario suele incluir un repositorio can\u00f3nico, representante \"oficial\" del proyecto. Info Este modelo se puso muy de moda a ra\u00edz de la forja GitHub que se ver\u00e1 m\u00e1s adelante.","title":"Flujo de trabajo del Gestor-de-Integraciones"},{"location":"introduccion/#flujo-de-trabajo-con-dictador-y-tenientes","text":"Es una variante del flujo de trabajo con m\u00faltiples repositorios. Se utiliza generalmente en proyectos muy grandes, con cientos de colaboradores. Un ejemplo muy conocido es el del kernel de Linux. Unos gestores de integraci\u00f3n se encargan de partes concretas del repositorio; y se denominan tenientes. Todos los tenientes rinden cuentas a un gestor de integraci\u00f3n; conocido como el dictador benevolente. El repositorio del dictador benevolente es el repositorio de referencia, del que recuperan (pull) todos los colaboradores.","title":"Flujo de trabajo con Dictador y Tenientes"},{"location":"introduction/","text":"Introducci\u00f3n Seg\u00fan la Wikipedia: \"Docker es un proyecto de c\u00f3digo abierto que automatiza el despliegue de aplicaciones dentro de contenedores de software, proporcionando una capa adicional de abstracci\u00f3n y automatizaci\u00f3n de virtualizaci\u00f3n de aplicaciones en m\u00faltiples sistemas operativos. Docker utiliza caracter\u00edsticas de aislamiento de recursos del kernel Linux, tales como cgroups y espacios de nombres (namespaces) para permitir que 'contenedores' independientes se ejecuten dentro de una sola instancia de Linux, evitando la sobrecarga de iniciar y mantener m\u00e1quinas virtuales.\" 1 . Esto es una descripci\u00f3n formal, pero para enterarte de qu\u00e9 es docker sigue leyendo: \u00bfA qui\u00e9n le puede interesar usar docker? Docker es \u00fatil a administradores de sistemas, pero tambi\u00e9n a desarrolladores. Uno de los problemas que se presentan durante el desarrollo y despliegue de aplicaciones es encontrarnos con sistemas heterog\u00e9neos, no ya entre los desarrolladores, tambi\u00e9n entre los sistemas de pruebas, pre-producci\u00f3n y producci\u00f3n. Es decir, que los desarrolladores y los sistemas donde se ejecuta la aplicaci\u00f3n tienen librer\u00edas y sistemas operativos diferentes. \u00bfY por qu\u00e9 es un problema? Pues porque la aplicaci\u00f3n puede funcionar bien en una distribuci\u00f3n de GNU/Linux pero no bien en otra, o ejecutarse bien con la versi\u00f3n de un lenguaje pero no con otra. Para asegurar la calidad de desarrollo tenemos que asegurar que todo el mundo usa las mismas versiones de todas las aplicaciones y librer\u00edas necesarios. Esto es m\u00e1s complicado de lo que parece, porque hay desarrolladores que prefieron una distribuci\u00f3n concreta, o incluso sistemas privativos. Incluso los sistemas de pruebas, pre-producci\u00f3n y producci\u00f3n suelen ser distintos. Los sistemas de producci\u00f3n suelen ser m\u00e1s nuevos y potentes y los antiguos se dejan para pruebas y pre-producci\u00f3n. Otro problema es que un mismo desarrollador o un mismo sistema de despliegue tenga que trabajar en m\u00e1s de un proyecto que requiera versiones distintas de librer\u00edas, complic\u00e1ndolo a\u00fan m\u00e1s. Docker viene a solucionar todos estos problemas, tanto para los desarrolladores como para los administradores de sistemas. Con Docker podemos crear entornos aislados con configuraciones que ser\u00e1n exactamente igual siempre. \u00bfDocker es virtualizaci\u00f3n? En GNU/Linux Docker no es virtualizado, no hay un hipervisor. Los procesos que corren dentro de un contenedor de docker se ejecutan con el mismo kernel que la m\u00e1quina anfitri\u00f3n. Linux lo que hace es aislar esos procesos del resto de procesos del sistema, ya sean los propios de la m\u00e1quina anfitri\u00f3n o procesos de otros contenedores. Adem\u00e1s, es capaz de controlar los recursos que se le asignan a esos contenedores (cpu, memoria, red, etc.). Internamente, el contenedor no sabe que lo es y a todos los efectos es una distribuci\u00f3n GNU/Linux independiente, pero sin la penalizaci\u00f3n de rendimiento que tienen los sistemas virtualizados. As\u00ed que, cuando ejecutamos un contenedor, estamos ejecutando un servicio dentro de una distribuci\u00f3n construida a partir de una \"receta\". Esa receta permite que el sistema que se ejecuta sea siempre el mismo, independientemente de si estamos usando Docker en Ubuntu, Fedora o, incluso, sistemas privativos compatibles con Docker. De esa manera podemos garantizar que estamos desarrollando o desplegando nuestra aplicaci\u00f3n, siempre con la misma versi\u00f3n de todas las dependencias. Obviamente, si ejecutamos contenedores GNU/Linux dentro de sistemas privativos, s\u00ed habr\u00e1 virtualizaci\u00f3n. Conceptos b\u00e1sicos Antes de comenzar a instalar y usar docker es importante tener una serie de conceptos claros 2 : Demonio de docker (docker daemon) : Es el proceso principal de docker. Escucha peticiones a la API y maneja los objetos de docker: im\u00e1genes, contenedores, redes, vol\u00famenes. Tambi\u00e9n es capaz de comunicarse con otros demonios para controlar servicios docker. Cliente de docker (docker client) : Es la principal herramienta que usan los administradores de sistema para interaccionar con el sistema Docker. Registro de docker (docker registry) : Es el lugar donde se almacenan las im\u00e1genes de Docker y poder descargarlas para reutilizarlas. Docker Hub es el principal registro p\u00fablico de Docker y contiene ya un mont\u00f3n de im\u00e1genes listas para ser usadas de multitud de servicios (mysql, wordpress, etc). Objetos de docker Cuando usamos Docker, estamos creando y usando im\u00e1genes, contenedores, redes o vol\u00famenes, entre otros. A todo esto se le denominan objetos. Veamos los m\u00e1s importantes: Imagen (image) : Plantilla de solo lectura que contiene las instrucciones para crear un contenedor Docker. Pueden estar basadas en otras imagenes, lo cual es habitual. Contenedor (container) : Es una instancia ejecutable de una imagen. Esta instancia puede ser creada, iniciada, detenida, movida o eliminada a trav\u00e9s del cliente de Docker o de la API. Las instancias se pueden conectar a una o m\u00e1s redes, sistemas de almacenamiento, o incluso se puede crear una imagen a partir del estado de un contenedor. Se puede controlar c\u00f3mo de aislado est\u00e1 el contenedor del sistema anfitri\u00f3n y del resto de contenedores. El contenedor est\u00e1 definido tanto por la imagen de la que procede como de las opciones de configuraci\u00f3n que permita. Por ejemplo, la imagen oficial de MariaDb permite configurar a trav\u00e9s de opciones la contrase\u00f1a del administrador, de la primera base de datos que se cree, del usuario que la maneja, etc. Servicios (services) : Los servicios permiten escalar contenedor a trav\u00e9s de m\u00faltiples demonios de Docker, los cuales trabajar\u00e1n conjuntamente como un enjambre (swarm). Objetivos del taller En este taller aprenderemos: A usar el registro oficial de Docker, a descargar y usar im\u00e1genes del mismo creando contenedores que nos puedan ser \u00fatiles. A crear una imagen a partir de otra. A automatizar la creaci\u00f3n de un conjunto de contenedores que interaccionan entre s\u00ed. En este taller no aprenderemos: A crear un cluster con Docker (Docker Swarm). A administrar sistemas. Se dar\u00e1n ya las instrucciones necesarias para crear servicios pero explicarlos est\u00e1 m\u00e1s alla del \u00e1mbito de este taller. Se recomienda cursar la asignatura: \"Programaci\u00f3n y Administraci\u00f3n de Sistemas\" del Grado de Ingenier\u00eda Inform\u00e1tica. Wikipedia: https://es.wikipedia.org/wiki/Docker_(software) \u21a9 Official Docker Documentation: Docker overview . \u21a9","title":"Introducci\u00f3n"},{"location":"introduction/#introduccion","text":"Seg\u00fan la Wikipedia: \"Docker es un proyecto de c\u00f3digo abierto que automatiza el despliegue de aplicaciones dentro de contenedores de software, proporcionando una capa adicional de abstracci\u00f3n y automatizaci\u00f3n de virtualizaci\u00f3n de aplicaciones en m\u00faltiples sistemas operativos. Docker utiliza caracter\u00edsticas de aislamiento de recursos del kernel Linux, tales como cgroups y espacios de nombres (namespaces) para permitir que 'contenedores' independientes se ejecuten dentro de una sola instancia de Linux, evitando la sobrecarga de iniciar y mantener m\u00e1quinas virtuales.\" 1 . Esto es una descripci\u00f3n formal, pero para enterarte de qu\u00e9 es docker sigue leyendo:","title":"Introducci\u00f3n"},{"location":"introduction/#a-quien-le-puede-interesar-usar-docker","text":"Docker es \u00fatil a administradores de sistemas, pero tambi\u00e9n a desarrolladores. Uno de los problemas que se presentan durante el desarrollo y despliegue de aplicaciones es encontrarnos con sistemas heterog\u00e9neos, no ya entre los desarrolladores, tambi\u00e9n entre los sistemas de pruebas, pre-producci\u00f3n y producci\u00f3n. Es decir, que los desarrolladores y los sistemas donde se ejecuta la aplicaci\u00f3n tienen librer\u00edas y sistemas operativos diferentes. \u00bfY por qu\u00e9 es un problema? Pues porque la aplicaci\u00f3n puede funcionar bien en una distribuci\u00f3n de GNU/Linux pero no bien en otra, o ejecutarse bien con la versi\u00f3n de un lenguaje pero no con otra. Para asegurar la calidad de desarrollo tenemos que asegurar que todo el mundo usa las mismas versiones de todas las aplicaciones y librer\u00edas necesarios. Esto es m\u00e1s complicado de lo que parece, porque hay desarrolladores que prefieron una distribuci\u00f3n concreta, o incluso sistemas privativos. Incluso los sistemas de pruebas, pre-producci\u00f3n y producci\u00f3n suelen ser distintos. Los sistemas de producci\u00f3n suelen ser m\u00e1s nuevos y potentes y los antiguos se dejan para pruebas y pre-producci\u00f3n. Otro problema es que un mismo desarrollador o un mismo sistema de despliegue tenga que trabajar en m\u00e1s de un proyecto que requiera versiones distintas de librer\u00edas, complic\u00e1ndolo a\u00fan m\u00e1s. Docker viene a solucionar todos estos problemas, tanto para los desarrolladores como para los administradores de sistemas. Con Docker podemos crear entornos aislados con configuraciones que ser\u00e1n exactamente igual siempre.","title":"\u00bfA qui\u00e9n le puede interesar usar docker?"},{"location":"introduction/#docker-es-virtualizacion","text":"En GNU/Linux Docker no es virtualizado, no hay un hipervisor. Los procesos que corren dentro de un contenedor de docker se ejecutan con el mismo kernel que la m\u00e1quina anfitri\u00f3n. Linux lo que hace es aislar esos procesos del resto de procesos del sistema, ya sean los propios de la m\u00e1quina anfitri\u00f3n o procesos de otros contenedores. Adem\u00e1s, es capaz de controlar los recursos que se le asignan a esos contenedores (cpu, memoria, red, etc.). Internamente, el contenedor no sabe que lo es y a todos los efectos es una distribuci\u00f3n GNU/Linux independiente, pero sin la penalizaci\u00f3n de rendimiento que tienen los sistemas virtualizados. As\u00ed que, cuando ejecutamos un contenedor, estamos ejecutando un servicio dentro de una distribuci\u00f3n construida a partir de una \"receta\". Esa receta permite que el sistema que se ejecuta sea siempre el mismo, independientemente de si estamos usando Docker en Ubuntu, Fedora o, incluso, sistemas privativos compatibles con Docker. De esa manera podemos garantizar que estamos desarrollando o desplegando nuestra aplicaci\u00f3n, siempre con la misma versi\u00f3n de todas las dependencias. Obviamente, si ejecutamos contenedores GNU/Linux dentro de sistemas privativos, s\u00ed habr\u00e1 virtualizaci\u00f3n.","title":"\u00bfDocker es virtualizaci\u00f3n?"},{"location":"introduction/#conceptos-basicos","text":"Antes de comenzar a instalar y usar docker es importante tener una serie de conceptos claros 2 : Demonio de docker (docker daemon) : Es el proceso principal de docker. Escucha peticiones a la API y maneja los objetos de docker: im\u00e1genes, contenedores, redes, vol\u00famenes. Tambi\u00e9n es capaz de comunicarse con otros demonios para controlar servicios docker. Cliente de docker (docker client) : Es la principal herramienta que usan los administradores de sistema para interaccionar con el sistema Docker. Registro de docker (docker registry) : Es el lugar donde se almacenan las im\u00e1genes de Docker y poder descargarlas para reutilizarlas. Docker Hub es el principal registro p\u00fablico de Docker y contiene ya un mont\u00f3n de im\u00e1genes listas para ser usadas de multitud de servicios (mysql, wordpress, etc).","title":"Conceptos b\u00e1sicos"},{"location":"introduction/#objetos-de-docker","text":"Cuando usamos Docker, estamos creando y usando im\u00e1genes, contenedores, redes o vol\u00famenes, entre otros. A todo esto se le denominan objetos. Veamos los m\u00e1s importantes: Imagen (image) : Plantilla de solo lectura que contiene las instrucciones para crear un contenedor Docker. Pueden estar basadas en otras imagenes, lo cual es habitual. Contenedor (container) : Es una instancia ejecutable de una imagen. Esta instancia puede ser creada, iniciada, detenida, movida o eliminada a trav\u00e9s del cliente de Docker o de la API. Las instancias se pueden conectar a una o m\u00e1s redes, sistemas de almacenamiento, o incluso se puede crear una imagen a partir del estado de un contenedor. Se puede controlar c\u00f3mo de aislado est\u00e1 el contenedor del sistema anfitri\u00f3n y del resto de contenedores. El contenedor est\u00e1 definido tanto por la imagen de la que procede como de las opciones de configuraci\u00f3n que permita. Por ejemplo, la imagen oficial de MariaDb permite configurar a trav\u00e9s de opciones la contrase\u00f1a del administrador, de la primera base de datos que se cree, del usuario que la maneja, etc. Servicios (services) : Los servicios permiten escalar contenedor a trav\u00e9s de m\u00faltiples demonios de Docker, los cuales trabajar\u00e1n conjuntamente como un enjambre (swarm).","title":"Objetos de docker"},{"location":"introduction/#objetivos-del-taller","text":"En este taller aprenderemos: A usar el registro oficial de Docker, a descargar y usar im\u00e1genes del mismo creando contenedores que nos puedan ser \u00fatiles. A crear una imagen a partir de otra. A automatizar la creaci\u00f3n de un conjunto de contenedores que interaccionan entre s\u00ed. En este taller no aprenderemos: A crear un cluster con Docker (Docker Swarm). A administrar sistemas. Se dar\u00e1n ya las instrucciones necesarias para crear servicios pero explicarlos est\u00e1 m\u00e1s alla del \u00e1mbito de este taller. Se recomienda cursar la asignatura: \"Programaci\u00f3n y Administraci\u00f3n de Sistemas\" del Grado de Ingenier\u00eda Inform\u00e1tica. Wikipedia: https://es.wikipedia.org/wiki/Docker_(software) \u21a9 Official Docker Documentation: Docker overview . \u21a9","title":"Objetivos del taller"},{"location":"ramas/","text":"Ramas Administraci\u00f3n de ramas Crear una nueva rama Cuando vamos a trabajar en una nueva funcionalidad, es conveniente hacerlo en una nueva rama, para no modificar la rama principal y dejarla inestable. Aunque la orden para manejar ramas es git branch podemos usar tambi\u00e9n git checkout . Vamos a crear una nueva rama: git branch hola Info Si usamos git branch sin ning\u00fan argumento, nos devolver\u00e1 la lista de ramas disponibles. La orden anterior no devuelve ning\u00fan resultado y tampoco nos cambia de rama, para eso debemos usar checkout : $ git checkout hola Switched to branch 'hola' Tip Hay una forma m\u00e1s rapida de hacer ambas acciones en un solo paso. Con el par\u00e1metro -b de git checkout podemos cambiarnos a una rama que, si no existe, se crea instant\u00e1neamente. $ git checkout -b hola Switched to a new branch 'hola' Modificaciones en la rama secundaria A\u00f1adimos un nuevo archivo en el directorio lib llamado HolaMundo.php : <?php class HolaMundo { private $nombre ; function __construct ( $nombre ) { $this -> nombre = $nombre ; } function __toString () { return sprintf ( \"Hola, %s. \\n \" , $this -> nombre ); } } Y modificamos hola.php : <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> // El nombre por defecto es Mundo require ( 'HolaMundo.php' ); $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; print new HolaMundo ( $nombre ); Podr\u00edamos confirmar los cambios todos de golpe, pero lo haremos de uno en uno, con su comentario. $ git add lib/HolaMundo.php $ git commit -m \"A\u00f1adida la clase HolaMundo\" [hola 6932156] A\u00f1adida la clase HolaMundo 1 file changed, 16 insertions(+) create mode 100644 lib/HolaMundo.php $ git add lib/hola.php $ git commit -m \"hola usa la clase HolaMundo\" [hola 9862f33] hola usa la clase HolaMundo 1 file changed, 3 insertions(+), 1 deletion(-) Y ahora con la orden git checkout podemos movernos entre ramas: $ git checkout master Switched to branch 'master' $ git checkout hola Switched to branch 'hola' Modificaciones en la rama master Podemos volver y a\u00f1adir un nuevo archivo a la rama principal: $ git checkout master Switched to branch 'master' Creamos un archivo llamado README.md en la ra\u00edz de nuestro proyecto con el siguiente contenido: # Curso de GIT Este proyecto contiene el curso de introducci\u00f3n a GIT Y lo a\u00f1adimos a nuestro repositorio en la rama en la que estamos: $ git add README.md $ git commit -m \"A\u00f1adido README.md\" [master c3e65d0] A\u00f1adido README.md 1 file changed, 3 insertions(+) create mode 100644 README.md $ git hist --all * c3e65d0 2013-06-16 | A\u00f1adido README.md (HEAD, master) [Sergio G\u00f3mez] | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Y vemos como git hist muestra la bifurcaci\u00f3n en nuestro c\u00f3digo. Fusi\u00f3n de ramas y resoluci\u00f3n de conflictos Mezclar ramas Podemos incorporar los cambios de una rama a otra con la orden git merge $ git checkout hola Switched to branch 'hola' $ git merge master Merge made by the 'recursive' strategy. README.md | 3 +++ 1 file changed, 3 insertions(+) create mode 100644 README.md $ git hist --all * 9c6ac06 2013-06-16 | Merge commit 'c3e65d0' into hola (HEAD, hola) [Sergio G\u00f3mez] |\\ * | 9862f33 2013-06-16 | hola usa la clase HolaMundo [Sergio G\u00f3mez] * | 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] | | | * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] De esa forma se puede trabajar en una rama secundaria incorporando los cambios de la rama principal o de otra rama. Resolver conflictos Un conflicto es cuando se produce una fusi\u00f3n que Git no es capaz de resolver. Vamos a modificar la rama master para crear uno con la rama hola. $ git checkout master Switched to branch 'master' Modificamos nuestro archivo hola.php de nuevo: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> print \"Introduce tu nombre:\" ; $nombre = trim ( fgets ( STDIN )); @ print \"Hola, { $nombre } \\n \" ; Y guardamos los cambios: $ git add lib/hola.php $ git commit -m \"Programa interactivo\" [master 9c85275] Programa interactivo 1 file changed, 2 insertions(+), 2 deletions(-) $ git hist --all * 9c6ac06 2013-06-16 | Merge commit 'c3e65d0' into hola (hola) [Sergio G\u00f3mez] |\\ * | 9862f33 2013-06-16 | hola usa la clase HolaMundo [Sergio G\u00f3mez] * | 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] | | * 9c85275 2013-06-16 | Programa interactivo (HEAD, master) [Sergio G\u00f3mez] | |/ | * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Volvemos a la rama hola y fusionamos: $ git checkout hola Switched to branch 'hola' $ git merge master Auto-merging lib/hola.php CONFLICT (content): Merge conflict in lib/hola.php Automatic merge failed; fix conflicts and then commit the result. Si editamos nuestro archivo lib/hola.php obtendremos algo similar a esto: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> <<<<<<< HEAD // El nombre por defecto es Mundo require ( 'HolaMundo.php' ); $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; print new HolaMundo ( $nombre ); ======= print \"Introduce tu nombre:\" ; $nombre = trim ( fgets ( STDIN )); @ print \"Hola, { $nombre } \\n \" ; >>>>>>> master La primera parte marca el c\u00f3digo que estaba en la rama donde trabaj\u00e1bamos (HEAD) y la parte final el c\u00f3digo de donde fusion\u00e1bamos. Resolvemos el conflicto, dejando el archivo como sigue: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> require ( 'HolaMundo.php' ); print \"Introduce tu nombre:\" ; $nombre = trim ( fgets ( STDIN )); print new HolaMundo ( $nombre ); Y resolvemos el conflicto confirmando los cambios: $ git add lib/hola.php $ git commit -m \"Solucionado el conflicto al fusionar con la rama master\" [hola a36af04] Solucionado el conflicto al fusionar con la rama master Rebasing vs Merging Rebasing es otra t\u00e9cnica para fusionar distinta a merge y usa la orden git rebase . Vamos a dejar nuestro proyecto como estaba antes del fusionado. Para ello necesitamos anotar el hash anterior al de la acci\u00f3n de merge . El que tiene la anotaci\u00f3n \"hola usa la clase HolaMundo\" . Para ello podemos usar la orden git reset que nos permite mover HEAD donde queramos. $ git checkout hola Switched to branch 'hola' $ git hist * a36af04 2013-06-16 | Solucionado el conflicto al fusionar con la rama master (HEAD, hola) [Sergio G\u00f3mez] |\\ | * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * | 9c6ac06 2013-06-16 | Merge commit 'c3e65d0' into hola [Sergio G\u00f3mez] |\\ \\ | |/ | * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * | 9862f33 2013-06-16 | hola usa la clase HolaMundo [Sergio G\u00f3mez] * | 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] $ git reset --hard 9862f33 HEAD is now at 9862f33 hola usa la clase HolaMundo Y nuestro estado ser\u00e1: $ git hist --all * 9862f33 2013-06-16 | hola usa la clase HolaMundo (HEAD, hola) [Sergio G\u00f3mez] * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] | * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] | * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Hemos desecho todos los merge y nuestro \u00e1rbol est\u00e1 \"limpio\" . Vamos a probar ahora a hacer un rebase. Continuamos en la rama hola y ejecutamos lo siguiente: $ git rebase master First, rewinding head to replay your work on top of it... Applying: A\u00f1adida la clase HolaMundo Applying: hola usa la clase HolaMundo Using index info to reconstruct a base tree... M lib/hola.php Falling back to patching base and 3-way merge... Auto-merging lib/hola.php CONFLICT (content): Merge conflict in lib/hola.php error: Failed to merge in the changes. Patch failed at 0002 hola usa la clase HolaMundo The copy of the patch that failed is found in: .git/rebase-apply/patch When you have resolved this problem, run \"git rebase --continue\". If you prefer to skip this patch, run \"git rebase --skip\" instead. To check out the original branch and stop rebasing, run \"git rebase --abort\". El conflicto, por supuesto, se sigue dando. Resolvemos guardando el archivo hola.php como en los casos anteriores: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> require ( 'HolaMundo.php' ); print \"Introduce tu nombre:\" ; $nombre = trim ( fgets ( STDIN )); print new HolaMundo ( $nombre ); A\u00f1adimos los cambios en staging y en esta ocasi\u00f3n, y tal como nos indicaba en el mensaje anterior, no tenemos que hacer git commit sino continuar con el rebase : $ git add lib/hola.php $ git status rebase in progress; onto 269eaca You are currently rebasing branch 'hola' on '269eaca'. (all conflicts fixed: run \"git rebase --continue\") Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) modified: lib/hola.php $ git rebase --continue Applying: hola usa la clase HolaMundo Y ahora vemos que nuestro \u00e1rbol tiene un aspecto distinto, mucho m\u00e1s limpio: $ git hist --all * 9862f33 2013-06-16 | hola usa la clase HolaMundo (HEAD -> hola) [Sergio G\u00f3mez] * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Lo que hace rebase es volver a aplicar todos los cambios a la rama m\u00e1ster, desde su nodo m\u00e1s reciente. Eso significa que se modifica el orden o la historia de creaci\u00f3n de los cambios. Por eso rebase no debe usarse si el orden es importante o si la rama es compartida. Mezclando con la rama master Ya hemos terminado de implementar los cambios en nuestra rama secundaria y es hora de llevar los cambios a la rama principal. Usamos git merge para hacer una fusi\u00f3n normal: $ git checkout master Switched to branch 'master' $ git merge hola Updating c3e65d0..491f1d2 Fast-forward lib/HolaMundo.php | 16 ++++++++++++++++ lib/hola.php | 4 +++- 2 files changed, 19 insertions(+), 1 deletion(-) create mode 100644 lib/HolaMundo.php $ git hist --all * 9862f33 2013-06-16 | hola usa la clase HolaMundo (HEAD -> master, hola) [Sergio G\u00f3mez] * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] * 9c85275 2013-06-16 | Programa interactivo [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Vemos que indica que el tipo de fusi\u00f3n es fast-forward . Este tipo de fusi\u00f3n tiene el problema que no deja rastro de la fusi\u00f3n, por eso suele ser recomendable usar el par\u00e1metro --no-ff para que quede constancia siempre de que se ha fusionado una rama con otra. Vamos a volver a probar ahora sin hacer fast-forward . Reseteamos master al estado \"Programa interactivo\" . $ git reset --hard 9c85275 $ git hist --all * 9862f33 2013-06-16 | hola usa la clase HolaMundo (HEAD -> hola) [Sergio G\u00f3mez] * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Vemos que estamos como en el final de la secci\u00f3n anterior, as\u00ed que ahora mezclamos: $ git merge -m \"Aplicando los cambios de la rama hola\" --no-ff hola Merge made by the 'recursive' strategy. lib/HolaMundo.php | 16 ++++++++++++++++ lib/hola.php | 4 +++- 2 files changed, 19 insertions(+), 1 deletion(-) create mode 100644 lib/HolaMundo.php $ git hist --all * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola (HEAD -> master) [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] En la siguiente imagen se puede ver la diferencia:","title":"Ramas"},{"location":"ramas/#ramas","text":"","title":"Ramas"},{"location":"ramas/#administracion-de-ramas","text":"","title":"Administraci\u00f3n de ramas"},{"location":"ramas/#crear-una-nueva-rama","text":"Cuando vamos a trabajar en una nueva funcionalidad, es conveniente hacerlo en una nueva rama, para no modificar la rama principal y dejarla inestable. Aunque la orden para manejar ramas es git branch podemos usar tambi\u00e9n git checkout . Vamos a crear una nueva rama: git branch hola Info Si usamos git branch sin ning\u00fan argumento, nos devolver\u00e1 la lista de ramas disponibles. La orden anterior no devuelve ning\u00fan resultado y tampoco nos cambia de rama, para eso debemos usar checkout : $ git checkout hola Switched to branch 'hola' Tip Hay una forma m\u00e1s rapida de hacer ambas acciones en un solo paso. Con el par\u00e1metro -b de git checkout podemos cambiarnos a una rama que, si no existe, se crea instant\u00e1neamente. $ git checkout -b hola Switched to a new branch 'hola'","title":"Crear una nueva rama"},{"location":"ramas/#modificaciones-en-la-rama-secundaria","text":"A\u00f1adimos un nuevo archivo en el directorio lib llamado HolaMundo.php : <?php class HolaMundo { private $nombre ; function __construct ( $nombre ) { $this -> nombre = $nombre ; } function __toString () { return sprintf ( \"Hola, %s. \\n \" , $this -> nombre ); } } Y modificamos hola.php : <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> // El nombre por defecto es Mundo require ( 'HolaMundo.php' ); $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; print new HolaMundo ( $nombre ); Podr\u00edamos confirmar los cambios todos de golpe, pero lo haremos de uno en uno, con su comentario. $ git add lib/HolaMundo.php $ git commit -m \"A\u00f1adida la clase HolaMundo\" [hola 6932156] A\u00f1adida la clase HolaMundo 1 file changed, 16 insertions(+) create mode 100644 lib/HolaMundo.php $ git add lib/hola.php $ git commit -m \"hola usa la clase HolaMundo\" [hola 9862f33] hola usa la clase HolaMundo 1 file changed, 3 insertions(+), 1 deletion(-) Y ahora con la orden git checkout podemos movernos entre ramas: $ git checkout master Switched to branch 'master' $ git checkout hola Switched to branch 'hola'","title":"Modificaciones en la rama secundaria"},{"location":"ramas/#modificaciones-en-la-rama-master","text":"Podemos volver y a\u00f1adir un nuevo archivo a la rama principal: $ git checkout master Switched to branch 'master' Creamos un archivo llamado README.md en la ra\u00edz de nuestro proyecto con el siguiente contenido: # Curso de GIT Este proyecto contiene el curso de introducci\u00f3n a GIT Y lo a\u00f1adimos a nuestro repositorio en la rama en la que estamos: $ git add README.md $ git commit -m \"A\u00f1adido README.md\" [master c3e65d0] A\u00f1adido README.md 1 file changed, 3 insertions(+) create mode 100644 README.md $ git hist --all * c3e65d0 2013-06-16 | A\u00f1adido README.md (HEAD, master) [Sergio G\u00f3mez] | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Y vemos como git hist muestra la bifurcaci\u00f3n en nuestro c\u00f3digo.","title":"Modificaciones en la rama master"},{"location":"ramas/#fusion-de-ramas-y-resolucion-de-conflictos","text":"","title":"Fusi\u00f3n de ramas y resoluci\u00f3n de conflictos"},{"location":"ramas/#mezclar-ramas","text":"Podemos incorporar los cambios de una rama a otra con la orden git merge $ git checkout hola Switched to branch 'hola' $ git merge master Merge made by the 'recursive' strategy. README.md | 3 +++ 1 file changed, 3 insertions(+) create mode 100644 README.md $ git hist --all * 9c6ac06 2013-06-16 | Merge commit 'c3e65d0' into hola (HEAD, hola) [Sergio G\u00f3mez] |\\ * | 9862f33 2013-06-16 | hola usa la clase HolaMundo [Sergio G\u00f3mez] * | 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] | | | * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] De esa forma se puede trabajar en una rama secundaria incorporando los cambios de la rama principal o de otra rama.","title":"Mezclar ramas"},{"location":"ramas/#resolver-conflictos","text":"Un conflicto es cuando se produce una fusi\u00f3n que Git no es capaz de resolver. Vamos a modificar la rama master para crear uno con la rama hola. $ git checkout master Switched to branch 'master' Modificamos nuestro archivo hola.php de nuevo: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> print \"Introduce tu nombre:\" ; $nombre = trim ( fgets ( STDIN )); @ print \"Hola, { $nombre } \\n \" ; Y guardamos los cambios: $ git add lib/hola.php $ git commit -m \"Programa interactivo\" [master 9c85275] Programa interactivo 1 file changed, 2 insertions(+), 2 deletions(-) $ git hist --all * 9c6ac06 2013-06-16 | Merge commit 'c3e65d0' into hola (hola) [Sergio G\u00f3mez] |\\ * | 9862f33 2013-06-16 | hola usa la clase HolaMundo [Sergio G\u00f3mez] * | 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] | | * 9c85275 2013-06-16 | Programa interactivo (HEAD, master) [Sergio G\u00f3mez] | |/ | * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Volvemos a la rama hola y fusionamos: $ git checkout hola Switched to branch 'hola' $ git merge master Auto-merging lib/hola.php CONFLICT (content): Merge conflict in lib/hola.php Automatic merge failed; fix conflicts and then commit the result. Si editamos nuestro archivo lib/hola.php obtendremos algo similar a esto: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> <<<<<<< HEAD // El nombre por defecto es Mundo require ( 'HolaMundo.php' ); $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; print new HolaMundo ( $nombre ); ======= print \"Introduce tu nombre:\" ; $nombre = trim ( fgets ( STDIN )); @ print \"Hola, { $nombre } \\n \" ; >>>>>>> master La primera parte marca el c\u00f3digo que estaba en la rama donde trabaj\u00e1bamos (HEAD) y la parte final el c\u00f3digo de donde fusion\u00e1bamos. Resolvemos el conflicto, dejando el archivo como sigue: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> require ( 'HolaMundo.php' ); print \"Introduce tu nombre:\" ; $nombre = trim ( fgets ( STDIN )); print new HolaMundo ( $nombre ); Y resolvemos el conflicto confirmando los cambios: $ git add lib/hola.php $ git commit -m \"Solucionado el conflicto al fusionar con la rama master\" [hola a36af04] Solucionado el conflicto al fusionar con la rama master","title":"Resolver conflictos"},{"location":"ramas/#rebasing-vs-merging","text":"Rebasing es otra t\u00e9cnica para fusionar distinta a merge y usa la orden git rebase . Vamos a dejar nuestro proyecto como estaba antes del fusionado. Para ello necesitamos anotar el hash anterior al de la acci\u00f3n de merge . El que tiene la anotaci\u00f3n \"hola usa la clase HolaMundo\" . Para ello podemos usar la orden git reset que nos permite mover HEAD donde queramos. $ git checkout hola Switched to branch 'hola' $ git hist * a36af04 2013-06-16 | Solucionado el conflicto al fusionar con la rama master (HEAD, hola) [Sergio G\u00f3mez] |\\ | * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * | 9c6ac06 2013-06-16 | Merge commit 'c3e65d0' into hola [Sergio G\u00f3mez] |\\ \\ | |/ | * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * | 9862f33 2013-06-16 | hola usa la clase HolaMundo [Sergio G\u00f3mez] * | 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] $ git reset --hard 9862f33 HEAD is now at 9862f33 hola usa la clase HolaMundo Y nuestro estado ser\u00e1: $ git hist --all * 9862f33 2013-06-16 | hola usa la clase HolaMundo (HEAD, hola) [Sergio G\u00f3mez] * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] | * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] | * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] |/ * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Hemos desecho todos los merge y nuestro \u00e1rbol est\u00e1 \"limpio\" . Vamos a probar ahora a hacer un rebase. Continuamos en la rama hola y ejecutamos lo siguiente: $ git rebase master First, rewinding head to replay your work on top of it... Applying: A\u00f1adida la clase HolaMundo Applying: hola usa la clase HolaMundo Using index info to reconstruct a base tree... M lib/hola.php Falling back to patching base and 3-way merge... Auto-merging lib/hola.php CONFLICT (content): Merge conflict in lib/hola.php error: Failed to merge in the changes. Patch failed at 0002 hola usa la clase HolaMundo The copy of the patch that failed is found in: .git/rebase-apply/patch When you have resolved this problem, run \"git rebase --continue\". If you prefer to skip this patch, run \"git rebase --skip\" instead. To check out the original branch and stop rebasing, run \"git rebase --abort\". El conflicto, por supuesto, se sigue dando. Resolvemos guardando el archivo hola.php como en los casos anteriores: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> require ( 'HolaMundo.php' ); print \"Introduce tu nombre:\" ; $nombre = trim ( fgets ( STDIN )); print new HolaMundo ( $nombre ); A\u00f1adimos los cambios en staging y en esta ocasi\u00f3n, y tal como nos indicaba en el mensaje anterior, no tenemos que hacer git commit sino continuar con el rebase : $ git add lib/hola.php $ git status rebase in progress; onto 269eaca You are currently rebasing branch 'hola' on '269eaca'. (all conflicts fixed: run \"git rebase --continue\") Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) modified: lib/hola.php $ git rebase --continue Applying: hola usa la clase HolaMundo Y ahora vemos que nuestro \u00e1rbol tiene un aspecto distinto, mucho m\u00e1s limpio: $ git hist --all * 9862f33 2013-06-16 | hola usa la clase HolaMundo (HEAD -> hola) [Sergio G\u00f3mez] * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Lo que hace rebase es volver a aplicar todos los cambios a la rama m\u00e1ster, desde su nodo m\u00e1s reciente. Eso significa que se modifica el orden o la historia de creaci\u00f3n de los cambios. Por eso rebase no debe usarse si el orden es importante o si la rama es compartida.","title":"Rebasing vs Merging"},{"location":"ramas/#mezclando-con-la-rama-master","text":"Ya hemos terminado de implementar los cambios en nuestra rama secundaria y es hora de llevar los cambios a la rama principal. Usamos git merge para hacer una fusi\u00f3n normal: $ git checkout master Switched to branch 'master' $ git merge hola Updating c3e65d0..491f1d2 Fast-forward lib/HolaMundo.php | 16 ++++++++++++++++ lib/hola.php | 4 +++- 2 files changed, 19 insertions(+), 1 deletion(-) create mode 100644 lib/HolaMundo.php $ git hist --all * 9862f33 2013-06-16 | hola usa la clase HolaMundo (HEAD -> master, hola) [Sergio G\u00f3mez] * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] * 9c85275 2013-06-16 | Programa interactivo [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Vemos que indica que el tipo de fusi\u00f3n es fast-forward . Este tipo de fusi\u00f3n tiene el problema que no deja rastro de la fusi\u00f3n, por eso suele ser recomendable usar el par\u00e1metro --no-ff para que quede constancia siempre de que se ha fusionado una rama con otra. Vamos a volver a probar ahora sin hacer fast-forward . Reseteamos master al estado \"Programa interactivo\" . $ git reset --hard 9c85275 $ git hist --all * 9862f33 2013-06-16 | hola usa la clase HolaMundo (HEAD -> hola) [Sergio G\u00f3mez] * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Vemos que estamos como en el final de la secci\u00f3n anterior, as\u00ed que ahora mezclamos: $ git merge -m \"Aplicando los cambios de la rama hola\" --no-ff hola Merge made by the 'recursive' strategy. lib/HolaMundo.php | 16 ++++++++++++++++ lib/hola.php | 4 +++- 2 files changed, 19 insertions(+), 1 deletion(-) create mode 100644 lib/HolaMundo.php $ git hist --all * 2eab8ca 2013-06-16 | Aplicando los cambios de la rama hola (HEAD -> master) [Sergio Gomez] *\\ | * 9862f33 2013-06-16 | hola usa la clase HolaMundo (hola) [Sergio G\u00f3mez] | * 6932156 2013-06-16 | A\u00f1adida la clase HolaMundo [Sergio G\u00f3mez] |/ * 9c85275 2013-06-16 | Programa interactivo (master) [Sergio G\u00f3mez] * c3e65d0 2013-06-16 | A\u00f1adido README.md [Sergio G\u00f3mez] * 81c6e93 2013-06-16 | Movido hola.php a lib [Sergio G\u00f3mez] * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] En la siguiente imagen se puede ver la diferencia:","title":"Mezclando con la rama master"},{"location":"referencias/","text":"Referencias Documentaci\u00f3n oficial en ingl\u00e9s . Documentaci\u00f3n oficial en espa\u00f1ol (quiz\u00e1s incompleta) . Curso de Git (ingl\u00e9s) . La mayor\u00eda de la documentaci\u00f3n de este manual est\u00e1 basada en este curso. Curso interactivo de Git (ingl\u00e9s) . P\u00e1gina de referencia de todas las \u00f3rdenes de Git (ingl\u00e9s) . Chuleta con las \u00f3rdenes m\u00e1s usuales de Git . Gitmagic (ingles y espa\u00f1ol). Otro manual de Git Art\u00edculo t\u00e9cnico: Un modelo exitoso de ramificaci\u00f3n en Git . Curso detallado y gratuito sobre Git y github Otra guia r\u00e1pida de git Gu\u00eda de estilos seg\u00fan Udacity Flujo de trabajo de Gitflow","title":"Referencias"},{"location":"referencias/#referencias","text":"Documentaci\u00f3n oficial en ingl\u00e9s . Documentaci\u00f3n oficial en espa\u00f1ol (quiz\u00e1s incompleta) . Curso de Git (ingl\u00e9s) . La mayor\u00eda de la documentaci\u00f3n de este manual est\u00e1 basada en este curso. Curso interactivo de Git (ingl\u00e9s) . P\u00e1gina de referencia de todas las \u00f3rdenes de Git (ingl\u00e9s) . Chuleta con las \u00f3rdenes m\u00e1s usuales de Git . Gitmagic (ingles y espa\u00f1ol). Otro manual de Git Art\u00edculo t\u00e9cnico: Un modelo exitoso de ramificaci\u00f3n en Git . Curso detallado y gratuito sobre Git y github Otra guia r\u00e1pida de git Gu\u00eda de estilos seg\u00fan Udacity Flujo de trabajo de Gitflow","title":"Referencias"},{"location":"tips/","text":"Trucos Portainer Portainer es una gestor de contenedores a trav\u00e9s de una interfaz web. Para usarlo creamos un directorio donde guardar nuestro docker-compose.yaml . mkdir -p ~/Sites/portainer cd ~/Sites/portainer Guardamos el siguiente fichero como docker-compose.yaml en nuestro directorio: version : '2' services : portainer : image : portainer/portainer command : -H unix:///var/run/docker.sock volumes : - /var/run/docker.sock:/var/run/docker.sock - portainer_data:/data ports : - 127.0.0.1:9000:9000 volumes : portainer_data : Y ejecutamos el contenedor: docker-compose up -d Limpieza Para borrar objetos que no est\u00e1n en uso: docker system prune Para borrar vol\u00famenes que no est\u00e1n asociados a ning\u00fan contenedor: docker volume rm $(docker volume ls -q -f \"dangling=true\") Para borrar contenedores que han terminado su ejecuci\u00f3n: docker rm $(docker ps -q -f \"status=exited\") Para borrar im\u00e1genes que no est\u00e1n etiquetadas: docker rmi $(docker images -q -f \"dangling=true\") Copias de seguridad Para hacer una copia de seguridad: docker run --rm -v /tmp:/backup \\ --volumes-from <container-name> \\ busybox tar -cvf /backup/backup.tar <path-to-data> Para restaurar: docker run --rm -v /tmp:/backup \\ --volumes-from <container-name> busybox tar -xvf /backup/backup.tar <path-to-data> Fuentes de esta p\u00e1gina: https://codefresh.io/docker-tutorial/everyday-hacks-docker/ http://blog.labianchin.me/2016/02/15/docker-tips-and-tricks Im\u00e1genes base Son las im\u00e1genes m\u00e1s conocidas por las que podemos usar para no partir desde cero para crear la nuestra. phusion/baseimage : 209mb centos : 200mb debian : 101mb ubuntu : 84mb alpine : 4.4mb busybox : 1.16mb","title":"Trucos"},{"location":"tips/#trucos","text":"","title":"Trucos"},{"location":"tips/#portainer","text":"Portainer es una gestor de contenedores a trav\u00e9s de una interfaz web. Para usarlo creamos un directorio donde guardar nuestro docker-compose.yaml . mkdir -p ~/Sites/portainer cd ~/Sites/portainer Guardamos el siguiente fichero como docker-compose.yaml en nuestro directorio: version : '2' services : portainer : image : portainer/portainer command : -H unix:///var/run/docker.sock volumes : - /var/run/docker.sock:/var/run/docker.sock - portainer_data:/data ports : - 127.0.0.1:9000:9000 volumes : portainer_data : Y ejecutamos el contenedor: docker-compose up -d","title":"Portainer"},{"location":"tips/#limpieza","text":"Para borrar objetos que no est\u00e1n en uso: docker system prune Para borrar vol\u00famenes que no est\u00e1n asociados a ning\u00fan contenedor: docker volume rm $(docker volume ls -q -f \"dangling=true\") Para borrar contenedores que han terminado su ejecuci\u00f3n: docker rm $(docker ps -q -f \"status=exited\") Para borrar im\u00e1genes que no est\u00e1n etiquetadas: docker rmi $(docker images -q -f \"dangling=true\")","title":"Limpieza"},{"location":"tips/#copias-de-seguridad","text":"Para hacer una copia de seguridad: docker run --rm -v /tmp:/backup \\ --volumes-from <container-name> \\ busybox tar -cvf /backup/backup.tar <path-to-data> Para restaurar: docker run --rm -v /tmp:/backup \\ --volumes-from <container-name> busybox tar -xvf /backup/backup.tar <path-to-data>","title":"Copias de seguridad"},{"location":"tips/#fuentes-de-esta-pagina","text":"https://codefresh.io/docker-tutorial/everyday-hacks-docker/ http://blog.labianchin.me/2016/02/15/docker-tips-and-tricks","title":"Fuentes de esta p\u00e1gina:"},{"location":"tips/#imagenes-base","text":"Son las im\u00e1genes m\u00e1s conocidas por las que podemos usar para no partir desde cero para crear la nuestra. phusion/baseimage : 209mb centos : 200mb debian : 101mb ubuntu : 84mb alpine : 4.4mb busybox : 1.16mb","title":"Im\u00e1genes base"},{"location":"usoavanzado/","text":"Uso avanzado de Git Deshacer cambios Deshaciendo cambios antes de la fase de staging. Volvemos a la rama m\u00e1ster y vamos a modificar el comentario que pusimos: $ git checkout master Previous HEAD position was 3283e0d... Se a\u00f1ade un par\u00e1metro por defecto Switched to branch 'master' Modificamos hola.php de la siguiente manera: <?php // Este comentario est\u00e1 mal y hay que borrarlo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y comprobamos: $ git status # On branch master # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # no changes added to commit (use \"git add\" and/or \"git commit -a\") El mismo Git nos indica que debemos hacer para a\u00f1adir los cambios o para deshacerlos: $ git checkout hola.php $ git status # On branch master nothing to commit, working directory clean $ cat hola.php <?php // El nombre por defecto es Mundo $nombre = isset($argv[1]) ? $argv[1] : \"Mundo\"; @print \"Hola, {$nombre}\\n\"; Deshaciendo cambios antes del commit Vamos a hacer lo mismo que la vez anterior, pero esta vez s\u00ed a\u00f1adiremos el cambio al staging (sin hacer commit ). As\u00ed que volvemos a modificar hola.php igual que la anterior ocasi\u00f3n: <?php // Este comentario est\u00e1 mal y hay que borrarlo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y lo a\u00f1adimos al staging $ git add hola.php $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # modified: hola.php # De nuevo, Git nos indica qu\u00e9 debemos hacer para deshacer el cambio: $ git reset HEAD hola.php Unstaged changes after reset: M hola.php $ git status # On branch master # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # no changes added to commit (use \"git add\" and/or \"git commit -a\") $ git checkout hola.php Y ya tenemos nuestro repositorio limpio otra vez. Como vemos hay que hacerlo en dos pasos: uno para borrar los datos del staging y otro para restaurar la copia de trabajo. Deshaciendo commits no deseados. Si a pesar de todo hemos hecho un commit y nos hemos equivocado, podemos deshacerlo con la orden git revert . Modificamos otra vez el archivo como antes: <?php // Este comentario est\u00e1 mal y hay que borrarlo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Pero ahora s\u00ed hacemos commit: $ git add hola.php $ git commit -m \"Ups... este commit est\u00e1 mal.\" master 5a5d067] Ups... este commit est\u00e1 mal 1 file changed, 1 insertion(+), 1 deletion(-) Bien, una vez confirmado el cambio, vamos a deshacer el cambio con la orden git revert : $ git revert HEAD --no-edit [master 817407b] Revert \"Ups... este commit est\u00e1 mal\" 1 file changed, 1 insertion(+), 1 deletion(-) $ git hist * 817407b 2013-06-16 | Revert \"Ups... este commit est\u00e1 mal\" (HEAD, master) [Sergio G\u00f3mez] * 5a5d067 2013-06-16 | Ups... este commit est\u00e1 mal [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Borrar commits de una rama El anterior apartado revierte un commit, pero deja huella en el historial de cambios. Para hacer que no aparezca hay que usar la orden git reset . $ git reset --hard v1 HEAD is now at fd4da94 Se a\u00f1ade un comentario al cambio del valor por defecto $ git hist * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (HEAD, tag: v1, master) [Sergio G\u00f3me * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] El resto de cambios no se han borrado (a\u00fan), simplemente no est\u00e1n accesibles porque git no sabe como referenciarlos. Si sabemos su hash podemos acceder a\u00fan a ellos. Pasado un tiempo, eventualmente Git tiene un recolector de basura que los borrar\u00e1. Se puede evitar etiquetando el estado final. Danger La orden reset es una operaci\u00f3n delicada. Debe evitarse si no se sabe bien lo que se est\u00e1 haciendo, sobre todo cuando se trabaja en repositorios compartidos, porque podr\u00edamos alterar la historia de cambios lo cual puede provocar problemas de sincronizaci\u00f3n. Modificar un commit Esto se usa cuando hemos olvidado a\u00f1adir un cambio a un commit que acabamos de realizar. Tenemos nuestro archivo hola.php de la siguiente manera: <?php // Autor: Sergio G\u00f3mez // El nombre por defecto es Mundo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y lo confirmamos: $ git commit -a -m \"A\u00f1adido el autor del programa\" [master cf405c1] A\u00f1adido el autor del programa 1 file changed, 1 insertion(+) Tip El par\u00e1metro -a hace un git add antes de hacer commit de todos los archivos modificados o borrados (de los nuevos no), con lo que nos ahorramos un paso. Ahora nos percatamos que se nos ha olvidado poner el correo electr\u00f3nico. As\u00ed que volvemos a modificar nuestro archivo: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> // El nombre por defecto es Mundo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y en esta ocasi\u00f3n usamos commit --amend que nos permite modificar el \u00faltimo estado confirmado, sustituy\u00e9ndolo por el estado actual: $ git add hola.php $ git commit --amend -m \"A\u00f1adido el autor del programa y su email\" [master 96a39df] A\u00f1adido el autor del programa y su email 1 file changed, 1 insertion(+) $ git hist * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email (HEAD, master) [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Danger Nunca modifiques un commit que ya hayas sincronizado con otro repositorio o que hayas recibido de \u00e9l. Estar\u00edas alterando la historia de cambios y provocar\u00edas problemas de sincronizaci\u00f3n. Moviendo y borrando archivos Mover un archivo a otro directorio con git Para mover archivos usaremos la orden git mv : $ mkdir lib $ git mv hola.php lib $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # renamed: hola.php -> lib/hola.php # Mover y borrar archivos. Pod\u00edamos haber hecho el paso anterior con la \u00f3rden del sistema mv y el resultado hubiera sido el mismo. Lo siguiente es a modo de ejemplo y no es necesario que lo ejecutes: $ mkdir lib $ mv hola.php lib $ git add lib/hola.php $ git rm hola.php Y, ahora s\u00ed, ya podemos guardar los cambios: $ git commit -m \"Movido hola.php a lib.\" [master 8c2a509] Movido hola.php a lib. 1 file changed, 0 insertions(+), 0 deletions(-) rename hola.php => lib/hola.php (100%)","title":"Uso avanzado de Git"},{"location":"usoavanzado/#uso-avanzado-de-git","text":"","title":"Uso avanzado de Git"},{"location":"usoavanzado/#deshacer-cambios","text":"","title":"Deshacer cambios"},{"location":"usoavanzado/#deshaciendo-cambios-antes-de-la-fase-de-staging","text":"Volvemos a la rama m\u00e1ster y vamos a modificar el comentario que pusimos: $ git checkout master Previous HEAD position was 3283e0d... Se a\u00f1ade un par\u00e1metro por defecto Switched to branch 'master' Modificamos hola.php de la siguiente manera: <?php // Este comentario est\u00e1 mal y hay que borrarlo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y comprobamos: $ git status # On branch master # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # no changes added to commit (use \"git add\" and/or \"git commit -a\") El mismo Git nos indica que debemos hacer para a\u00f1adir los cambios o para deshacerlos: $ git checkout hola.php $ git status # On branch master nothing to commit, working directory clean $ cat hola.php <?php // El nombre por defecto es Mundo $nombre = isset($argv[1]) ? $argv[1] : \"Mundo\"; @print \"Hola, {$nombre}\\n\";","title":"Deshaciendo cambios antes de la fase de staging."},{"location":"usoavanzado/#deshaciendo-cambios-antes-del-commit","text":"Vamos a hacer lo mismo que la vez anterior, pero esta vez s\u00ed a\u00f1adiremos el cambio al staging (sin hacer commit ). As\u00ed que volvemos a modificar hola.php igual que la anterior ocasi\u00f3n: <?php // Este comentario est\u00e1 mal y hay que borrarlo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y lo a\u00f1adimos al staging $ git add hola.php $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # modified: hola.php # De nuevo, Git nos indica qu\u00e9 debemos hacer para deshacer el cambio: $ git reset HEAD hola.php Unstaged changes after reset: M hola.php $ git status # On branch master # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # no changes added to commit (use \"git add\" and/or \"git commit -a\") $ git checkout hola.php Y ya tenemos nuestro repositorio limpio otra vez. Como vemos hay que hacerlo en dos pasos: uno para borrar los datos del staging y otro para restaurar la copia de trabajo.","title":"Deshaciendo cambios antes del commit"},{"location":"usoavanzado/#deshaciendo-commits-no-deseados","text":"Si a pesar de todo hemos hecho un commit y nos hemos equivocado, podemos deshacerlo con la orden git revert . Modificamos otra vez el archivo como antes: <?php // Este comentario est\u00e1 mal y hay que borrarlo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Pero ahora s\u00ed hacemos commit: $ git add hola.php $ git commit -m \"Ups... este commit est\u00e1 mal.\" master 5a5d067] Ups... este commit est\u00e1 mal 1 file changed, 1 insertion(+), 1 deletion(-) Bien, una vez confirmado el cambio, vamos a deshacer el cambio con la orden git revert : $ git revert HEAD --no-edit [master 817407b] Revert \"Ups... este commit est\u00e1 mal\" 1 file changed, 1 insertion(+), 1 deletion(-) $ git hist * 817407b 2013-06-16 | Revert \"Ups... este commit est\u00e1 mal\" (HEAD, master) [Sergio G\u00f3mez] * 5a5d067 2013-06-16 | Ups... este commit est\u00e1 mal [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez]","title":"Deshaciendo commits no deseados."},{"location":"usoavanzado/#borrar-commits-de-una-rama","text":"El anterior apartado revierte un commit, pero deja huella en el historial de cambios. Para hacer que no aparezca hay que usar la orden git reset . $ git reset --hard v1 HEAD is now at fd4da94 Se a\u00f1ade un comentario al cambio del valor por defecto $ git hist * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (HEAD, tag: v1, master) [Sergio G\u00f3me * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] El resto de cambios no se han borrado (a\u00fan), simplemente no est\u00e1n accesibles porque git no sabe como referenciarlos. Si sabemos su hash podemos acceder a\u00fan a ellos. Pasado un tiempo, eventualmente Git tiene un recolector de basura que los borrar\u00e1. Se puede evitar etiquetando el estado final. Danger La orden reset es una operaci\u00f3n delicada. Debe evitarse si no se sabe bien lo que se est\u00e1 haciendo, sobre todo cuando se trabaja en repositorios compartidos, porque podr\u00edamos alterar la historia de cambios lo cual puede provocar problemas de sincronizaci\u00f3n.","title":"Borrar commits de una rama"},{"location":"usoavanzado/#modificar-un-commit","text":"Esto se usa cuando hemos olvidado a\u00f1adir un cambio a un commit que acabamos de realizar. Tenemos nuestro archivo hola.php de la siguiente manera: <?php // Autor: Sergio G\u00f3mez // El nombre por defecto es Mundo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y lo confirmamos: $ git commit -a -m \"A\u00f1adido el autor del programa\" [master cf405c1] A\u00f1adido el autor del programa 1 file changed, 1 insertion(+) Tip El par\u00e1metro -a hace un git add antes de hacer commit de todos los archivos modificados o borrados (de los nuevos no), con lo que nos ahorramos un paso. Ahora nos percatamos que se nos ha olvidado poner el correo electr\u00f3nico. As\u00ed que volvemos a modificar nuestro archivo: <?php // Autor: Sergio G\u00f3mez <sergio@uco.es> // El nombre por defecto es Mundo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y en esta ocasi\u00f3n usamos commit --amend que nos permite modificar el \u00faltimo estado confirmado, sustituy\u00e9ndolo por el estado actual: $ git add hola.php $ git commit --amend -m \"A\u00f1adido el autor del programa y su email\" [master 96a39df] A\u00f1adido el autor del programa y su email 1 file changed, 1 insertion(+) $ git hist * 96a39df 2013-06-16 | A\u00f1adido el autor del programa y su email (HEAD, master) [Sergio G\u00f3mez] * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Danger Nunca modifiques un commit que ya hayas sincronizado con otro repositorio o que hayas recibido de \u00e9l. Estar\u00edas alterando la historia de cambios y provocar\u00edas problemas de sincronizaci\u00f3n.","title":"Modificar un commit"},{"location":"usoavanzado/#moviendo-y-borrando-archivos","text":"","title":"Moviendo y borrando archivos"},{"location":"usoavanzado/#mover-un-archivo-a-otro-directorio-con-git","text":"Para mover archivos usaremos la orden git mv : $ mkdir lib $ git mv hola.php lib $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # renamed: hola.php -> lib/hola.php #","title":"Mover un archivo a otro directorio con git"},{"location":"usoavanzado/#mover-y-borrar-archivos","text":"Pod\u00edamos haber hecho el paso anterior con la \u00f3rden del sistema mv y el resultado hubiera sido el mismo. Lo siguiente es a modo de ejemplo y no es necesario que lo ejecutes: $ mkdir lib $ mv hola.php lib $ git add lib/hola.php $ git rm hola.php Y, ahora s\u00ed, ya podemos guardar los cambios: $ git commit -m \"Movido hola.php a lib.\" [master 8c2a509] Movido hola.php a lib. 1 file changed, 0 insertions(+), 0 deletions(-) rename hola.php => lib/hola.php (100%)","title":"Mover y borrar archivos."},{"location":"usobasico/","text":"Uso b\u00e1sico de Git Crear un proyecto Crear un programa \"Hola Mundo\" Creamos un directorio donde colocar el c\u00f3digo $ mkdir curso-de-git $ cd curso-de-git Creamos un fichero hola.php que muestre Hola Mundo. <?php echo \"Hola Mundo \\n \" ; Crear el repositorio Para crear un nuevo repositorio se usa la orden git init $ git init Initialized empty Git repository in /home/cc0gobas/git/curso-de-git/.git/ A\u00f1adir la aplicaci\u00f3n Vamos a almacenar el archivo que hemos creado en el repositorio para poder trabajar, despu\u00e9s explicaremos para qu\u00e9 sirve cada orden. $ git add hola.php $ git commit -m \"Creaci\u00f3n del proyecto\" [master (root-commit) e19f2c1] Creaci\u00f3n del proyecto 1 file changed, 2 insertions(+) create mode 100644 hola.php Comprobar el estado del repositorio Con la orden git status podemos ver en qu\u00e9 estado se encuentran los archivos de nuestro repositorio. $ git status # On branch master nothing to commit (working directory clean) Si modificamos el archivo hola.php : <?php @ print \"Hola { $argv [ 1 ] } \\n \" ; Y volvemos a comprobar el estado del repositorio: $ git status # On branch master # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # no changes added to commit (use \"git add\" and/or \"git commit -a\") A\u00f1adir cambios Con la orden git add indicamos a git que prepare los cambios para que sean almacenados. $ git add hola.php $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # modified: hola.php # Confirmar los cambios Con la orden git commit confirmamos los cambios definitivamente, lo que hace que se guarden permanentemente en nuestro repositorio. $ git commit -m \"Parametrizaci\u00f3n del programa\" [master efc252e] Parametrizaci\u00f3n del programa 1 file changed, 1 insertion(+), 1 deletion(-) $ git status # On branch master nothing to commit (working directory clean) Diferencias entre workdir y staging . Modificamos nuestra aplicaci\u00f3n para que soporte un par\u00e1metro por defecto y a\u00f1adimos los cambios. <?php $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Este vez a\u00f1adimos los cambios a la fase de staging pero sin confirmarlos ( commit ). git add hola.php Volvemos a modificar el programa para indicar con un comentario lo que hemos hecho. <?php // El nombre por defecto es Mundo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y vemos el estado en el que est\u00e1 el repositorio $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # modified: hola.php # # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # Podemos ver como aparecen el archivo hola.php dos veces. El primero est\u00e1 preparado para ser confirmado y est\u00e1 almacenado en la zona de staging . El segundo indica que el directorio hola.php est\u00e1 modificado otra vez en la zona de trabajo ( workdir ). Warning Si volvieramos a hacer un git add hola.php sobreescribir\u00edamos los cambios previos que hab\u00eda en la zona de staging . Almacenamos los cambios por separado: $ git commit -m \"Se a\u00f1ade un par\u00e1metro por defecto\" [master 3283e0d] Se a\u00f1ade un par\u00e1metro por defecto 1 file changed, 2 insertions(+), 1 deletion(-) $ git status # On branch master # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # no changes added to commit (use \"git add\" and/or \"git commit -a\") $ git add . $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # modified: hola.php # $ git commit -m \"Se a\u00f1ade un comentario al cambio del valor por defecto\" [master fd4da94] Se a\u00f1ade un comentario al cambio del valor por defecto 1 file changed, 1 insertion(+) Info El valor \".\" despues de git add indica que se a\u00f1adan todos los archivos de forma recursiva. Warning Cuidado cuando uses git add . aseg\u00farate de que no est\u00e1s a\u00f1adiendo archivos que no quieres a\u00f1adir. Ignorando archivos La orden git add . o git add nombre_directorio es muy c\u00f3moda, ya que nos permite a\u00f1adir todos los archivos del proyecto o todos los contenidos en un directorio y sus subdirectorios. Es mucho m\u00e1s r\u00e1pido que tener que ir a\u00f1adi\u00e9ndolos uno por uno. El problema es que, si no se tiene cuidado, se puede terminar por a\u00f1adir archivos innecesarios o con informaci\u00f3n sensible. Por lo general se debe evitar a\u00f1adir archivos que se hayan generado como producto de la compilaci\u00f3n del proyecto, los que generen los entornos de desarrollo (archivos de configuraci\u00f3n y temporales) y aquellos que contentan informaci\u00f3n sensible, como contrase\u00f1as o tokens de autenticaci\u00f3n. Por ejemplo, en un proyecto de C/C++ , los archivos objeto no deben incluirse, solo los que contengan c\u00f3digo fuente y los make que los generen. Para indicarle a git que debe ignorar un archivo, se puede crear un fichero llamado .gitignore , bien en la ra\u00edz del proyecto o en los subdirectorios que queramos. Dicho fichero puede contener patrones, uno en cada l\u00ednea, que especiquen qu\u00e9 archivos deben ignorarse. El formato es el siguiente: # .gitignore dir1/ # ignora todo lo que contenga el directorio dir1 !dir1/info.txt # El operador ! excluye del ignore a dir1/info.txt (s\u00ed se guardar\u00eda) dir2/*.txt # ignora todos los archivos txt que hay en el directorio dir2 dir3/**/*.txt # ignora todos los archivos txt que hay en el dir3 y sus subdirectorios *.o # ignora todos los archivos con extensi\u00f3n .o en todos los directorios Cada tipo de proyecto genera sus ficheros temporales, as\u00ed que para cada proyecto hay un .gitignore apropiado. Existen repositorios que ya tienen creadas plantillas. Pod\u00e9is encontrar uno en https://github.com/github/gitignore Ignorando archivos globalmente Si bien, los archivos que hemos metido en .gitignore , deben ser aquellos ficheros temporales o de configuraci\u00f3n que se pueden crear durante las fases de compilaci\u00f3n o ejecuci\u00f3n del programa, en ocasiones habr\u00e1 otros ficheros que tampoco debemos introducir en el repositorio y que son recurrentes en todos los proyectos. En dicho caso, es m\u00e1s \u00fatil tener un gitignore que sea global a todos nuestros proyectos. Esta configuraci\u00f3n ser\u00eda complementaria a la que ya tenemos. Ejemplos de lo que se puede ignorar de forma global son los ficheros temporales del sistema operativo ( *~ , .nfs* ) y los que generan los entornos de desarrollo. Para indicar a git que queremos tener un fichero de gitignore global, tenemos que configurarlo con la siguiente orden: git config --global core.excludesfile $HOME/.gitignore_global Ahora podemos crear un archivo llamado .gitignore_global en la ra\u00edz de nuestra cuenta con este contenido: # Compiled source # ################### *.com *.class *.dll *.exe *.o *.so # Packages # ############ # it's better to unpack these files and commit the raw source # git has its own built in compression methods *.7z *.dmg *.gz *.iso *.jar *.rar *.tar *.zip # Logs and databases # ###################### *.log *.sql *.sqlite # OS generated files # ###################### .DS_Store .DS_Store? ._* .Spotlight-V100 .Trashes ehthumbs.db Thumbs.db *~ *.swp # IDEs # ###################### .idea .settings/ .classpath .project Trabajando con el historial Observando los cambios Con la orden git log podemos ver todos los cambios que hemos hecho: $ git log commit fd4da946326fbe8b24e89282ad25a71721bf40f6 Author: Sergio G\u00f3mez <sergio@uco.es> Date: Sun Jun 16 12:51:01 2013 +0200 Se a\u00f1ade un comentario al cambio del valor por defecto commit 3283e0d306c8d42d55ffcb64e456f10510df8177 Author: Sergio G\u00f3mez <sergio@uco.es> Date: Sun Jun 16 12:50:00 2013 +0200 Se a\u00f1ade un par\u00e1metro por defecto commit efc252e11939351505a426a6e1aa5bb7dc1dd7c0 Author: Sergio G\u00f3mez <sergio@uco.es> Date: Sun Jun 16 12:13:26 2013 +0200 Parametrizaci\u00f3n del programa commit e19f2c1701069d9d1159e9ee21acaa1bbc47d264 Author: Sergio G\u00f3mez <sergio@uco.es> Date: Sun Jun 16 11:55:23 2013 +0200 Creaci\u00f3n del proyecto Tambi\u00e9n es posible ver versiones abreviadas o limitadas, dependiendo de los par\u00e1metros: $ git log --oneline fd4da94 Se a\u00f1ade un comentario al cambio del valor por defecto 3283e0d Se a\u00f1ade un par\u00e1metro por defecto efc252e Parametrizaci\u00f3n del programa e19f2c1 Creaci\u00f3n del proyecto git log --oneline --max-count=2 git log --oneline --since='5 minutes ago' git log --oneline --until='5 minutes ago' git log --oneline --author=sergio git log --oneline --all Una versi\u00f3n muy \u00fatil de git log es la siguiente, pues nos permite ver en que lugares est\u00e1 master y HEAD, entre otras cosas: $ git log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (HEAD, master) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Crear alias Como estas \u00f3rdenes son demasiado largas, Git nos permite crear alias para crear nuevas \u00f3rdenes parametrizadas. Para ello podemos configurar nuestro entorno con la orden git config de la siguiente manera: git config --global alias.hist \"log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short\" Example Puedes configurar incluso alias para abreviar comandos. Algunos ejemplos de alias \u00fatiles: git config --global alias.br branch git config --global alias.co checkout git config --global alias.ci commit git config --global alias.st \"status -u\" git config --global alias.cane \"commit --amend --no-edit\" Recuperando versiones anteriores Cada cambio es etiquetado por un hash, para poder regresar a ese momento del estado del proyecto se usa la orden git checkout . $ git checkout e19f2c1 Note: checking out 'e19f2c1'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b new_branch_name HEAD is now at e19f2c1... Creaci\u00f3n del proyecto $ cat hola.php <?php echo \"Hello, World\\n\"; El aviso que nos sale nos indica que estamos en un estado donde no trabajamos en ninguna rama concreta. Eso significa que los cambios que hagamos podr\u00edan \"perderse\" porque si no son guardados en una nueva rama, en principio no podr\u00edamos volver a recuperarlos. Hay que pensar que Git es como un \u00e1rbol donde un nodo tiene informaci\u00f3n de su nodo padre, no de sus nodos hijos, con lo que siempre necesitar\u00edamos informaci\u00f3n de d\u00f3nde se encuentran los nodos finales o de otra manera no podr\u00edamos acceder a ellos. Volver a la \u00faltima versi\u00f3n de la rama master. Usamos git checkout indicando el nombre de la rama: $ git checkout master Previous HEAD position was e19f2c1... Creaci\u00f3n del proyecto Etiquetando versiones Para poder recuperar versiones concretas en la historia del repositorio, podemos etiquetarlas, lo cual es m\u00e1s facil que usar un hash. Para eso usaremos la orden git tag . $ git tag v1 Ahora vamos a etiquetar la versi\u00f3n inmediatamente anterior como v1-beta. Para ello podemos usar los modificadores ^ o ~ que nos llevar\u00e1n a un ancestro determinado. Las siguientes dos \u00f3rdenes son equivalentes: $ git checkout v1^ $ git checkout v1~1 $ git tag v1-beta Si ejecutamos la orden sin par\u00e1metros nos mostrar\u00e1 todas las etiquetas existentes. $ git tag v1 v1-beta Y para verlas en el historial: $ git hist master --all * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1, master) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (HEAD, tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez] Borrar etiquetas Para borrar etiquetas: git tag -d nombre_etiqueta Visualizar cambios Para ver los cambios que se han realizado en el c\u00f3digo usamos la orden git diff . La orden sin especificar nada m\u00e1s, mostrar\u00e1 los cambios que no han sido a\u00f1adidos a\u00fan, es decir, todos los cambios que se han hecho antes de usar la orden git add . Despu\u00e9s se puede indicar un par\u00e1metro y dar\u00e1 los cambios entre la versi\u00f3n indicada y el estado actual. O para comparar dos versiones entre s\u00ed, se indica la m\u00e1s antigua y la m\u00e1s nueva. Ejemplo: $ git diff v1-beta v1 diff --git a/hola.php b/hola.php index a31e01f..25a35c0 100644 --- a/hola.php +++ b/hola.php @@ -1,3 +1,4 @@ <?php +// El nombre por defecto es Mundo $nombre = isset($argv[1]) ? $argv[1] : \"Mundo\"; @print \"Hola, {$nombre}\\n\";","title":"Uso b\u00e1sico de Git"},{"location":"usobasico/#uso-basico-de-git","text":"","title":"Uso b\u00e1sico de Git"},{"location":"usobasico/#crear-un-proyecto","text":"","title":"Crear un proyecto"},{"location":"usobasico/#crear-un-programa-hola-mundo","text":"Creamos un directorio donde colocar el c\u00f3digo $ mkdir curso-de-git $ cd curso-de-git Creamos un fichero hola.php que muestre Hola Mundo. <?php echo \"Hola Mundo \\n \" ;","title":"Crear un programa \"Hola Mundo\""},{"location":"usobasico/#crear-el-repositorio","text":"Para crear un nuevo repositorio se usa la orden git init $ git init Initialized empty Git repository in /home/cc0gobas/git/curso-de-git/.git/","title":"Crear el repositorio"},{"location":"usobasico/#anadir-la-aplicacion","text":"Vamos a almacenar el archivo que hemos creado en el repositorio para poder trabajar, despu\u00e9s explicaremos para qu\u00e9 sirve cada orden. $ git add hola.php $ git commit -m \"Creaci\u00f3n del proyecto\" [master (root-commit) e19f2c1] Creaci\u00f3n del proyecto 1 file changed, 2 insertions(+) create mode 100644 hola.php","title":"A\u00f1adir la aplicaci\u00f3n"},{"location":"usobasico/#comprobar-el-estado-del-repositorio","text":"Con la orden git status podemos ver en qu\u00e9 estado se encuentran los archivos de nuestro repositorio. $ git status # On branch master nothing to commit (working directory clean) Si modificamos el archivo hola.php : <?php @ print \"Hola { $argv [ 1 ] } \\n \" ; Y volvemos a comprobar el estado del repositorio: $ git status # On branch master # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # no changes added to commit (use \"git add\" and/or \"git commit -a\")","title":"Comprobar el estado del repositorio"},{"location":"usobasico/#anadir-cambios","text":"Con la orden git add indicamos a git que prepare los cambios para que sean almacenados. $ git add hola.php $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # modified: hola.php #","title":"A\u00f1adir cambios"},{"location":"usobasico/#confirmar-los-cambios","text":"Con la orden git commit confirmamos los cambios definitivamente, lo que hace que se guarden permanentemente en nuestro repositorio. $ git commit -m \"Parametrizaci\u00f3n del programa\" [master efc252e] Parametrizaci\u00f3n del programa 1 file changed, 1 insertion(+), 1 deletion(-) $ git status # On branch master nothing to commit (working directory clean)","title":"Confirmar los cambios"},{"location":"usobasico/#diferencias-entre-workdir-y-staging","text":"Modificamos nuestra aplicaci\u00f3n para que soporte un par\u00e1metro por defecto y a\u00f1adimos los cambios. <?php $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Este vez a\u00f1adimos los cambios a la fase de staging pero sin confirmarlos ( commit ). git add hola.php Volvemos a modificar el programa para indicar con un comentario lo que hemos hecho. <?php // El nombre por defecto es Mundo $nombre = isset ( $argv [ 1 ]) ? $argv [ 1 ] : \"Mundo\" ; @ print \"Hola, { $nombre } \\n \" ; Y vemos el estado en el que est\u00e1 el repositorio $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # modified: hola.php # # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # Podemos ver como aparecen el archivo hola.php dos veces. El primero est\u00e1 preparado para ser confirmado y est\u00e1 almacenado en la zona de staging . El segundo indica que el directorio hola.php est\u00e1 modificado otra vez en la zona de trabajo ( workdir ). Warning Si volvieramos a hacer un git add hola.php sobreescribir\u00edamos los cambios previos que hab\u00eda en la zona de staging . Almacenamos los cambios por separado: $ git commit -m \"Se a\u00f1ade un par\u00e1metro por defecto\" [master 3283e0d] Se a\u00f1ade un par\u00e1metro por defecto 1 file changed, 2 insertions(+), 1 deletion(-) $ git status # On branch master # Changes not staged for commit: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: hola.php # no changes added to commit (use \"git add\" and/or \"git commit -a\") $ git add . $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # modified: hola.php # $ git commit -m \"Se a\u00f1ade un comentario al cambio del valor por defecto\" [master fd4da94] Se a\u00f1ade un comentario al cambio del valor por defecto 1 file changed, 1 insertion(+) Info El valor \".\" despues de git add indica que se a\u00f1adan todos los archivos de forma recursiva. Warning Cuidado cuando uses git add . aseg\u00farate de que no est\u00e1s a\u00f1adiendo archivos que no quieres a\u00f1adir.","title":"Diferencias entre workdir y staging."},{"location":"usobasico/#ignorando-archivos","text":"La orden git add . o git add nombre_directorio es muy c\u00f3moda, ya que nos permite a\u00f1adir todos los archivos del proyecto o todos los contenidos en un directorio y sus subdirectorios. Es mucho m\u00e1s r\u00e1pido que tener que ir a\u00f1adi\u00e9ndolos uno por uno. El problema es que, si no se tiene cuidado, se puede terminar por a\u00f1adir archivos innecesarios o con informaci\u00f3n sensible. Por lo general se debe evitar a\u00f1adir archivos que se hayan generado como producto de la compilaci\u00f3n del proyecto, los que generen los entornos de desarrollo (archivos de configuraci\u00f3n y temporales) y aquellos que contentan informaci\u00f3n sensible, como contrase\u00f1as o tokens de autenticaci\u00f3n. Por ejemplo, en un proyecto de C/C++ , los archivos objeto no deben incluirse, solo los que contengan c\u00f3digo fuente y los make que los generen. Para indicarle a git que debe ignorar un archivo, se puede crear un fichero llamado .gitignore , bien en la ra\u00edz del proyecto o en los subdirectorios que queramos. Dicho fichero puede contener patrones, uno en cada l\u00ednea, que especiquen qu\u00e9 archivos deben ignorarse. El formato es el siguiente: # .gitignore dir1/ # ignora todo lo que contenga el directorio dir1 !dir1/info.txt # El operador ! excluye del ignore a dir1/info.txt (s\u00ed se guardar\u00eda) dir2/*.txt # ignora todos los archivos txt que hay en el directorio dir2 dir3/**/*.txt # ignora todos los archivos txt que hay en el dir3 y sus subdirectorios *.o # ignora todos los archivos con extensi\u00f3n .o en todos los directorios Cada tipo de proyecto genera sus ficheros temporales, as\u00ed que para cada proyecto hay un .gitignore apropiado. Existen repositorios que ya tienen creadas plantillas. Pod\u00e9is encontrar uno en https://github.com/github/gitignore","title":"Ignorando archivos"},{"location":"usobasico/#ignorando-archivos-globalmente","text":"Si bien, los archivos que hemos metido en .gitignore , deben ser aquellos ficheros temporales o de configuraci\u00f3n que se pueden crear durante las fases de compilaci\u00f3n o ejecuci\u00f3n del programa, en ocasiones habr\u00e1 otros ficheros que tampoco debemos introducir en el repositorio y que son recurrentes en todos los proyectos. En dicho caso, es m\u00e1s \u00fatil tener un gitignore que sea global a todos nuestros proyectos. Esta configuraci\u00f3n ser\u00eda complementaria a la que ya tenemos. Ejemplos de lo que se puede ignorar de forma global son los ficheros temporales del sistema operativo ( *~ , .nfs* ) y los que generan los entornos de desarrollo. Para indicar a git que queremos tener un fichero de gitignore global, tenemos que configurarlo con la siguiente orden: git config --global core.excludesfile $HOME/.gitignore_global Ahora podemos crear un archivo llamado .gitignore_global en la ra\u00edz de nuestra cuenta con este contenido: # Compiled source # ################### *.com *.class *.dll *.exe *.o *.so # Packages # ############ # it's better to unpack these files and commit the raw source # git has its own built in compression methods *.7z *.dmg *.gz *.iso *.jar *.rar *.tar *.zip # Logs and databases # ###################### *.log *.sql *.sqlite # OS generated files # ###################### .DS_Store .DS_Store? ._* .Spotlight-V100 .Trashes ehthumbs.db Thumbs.db *~ *.swp # IDEs # ###################### .idea .settings/ .classpath .project","title":"Ignorando archivos globalmente"},{"location":"usobasico/#trabajando-con-el-historial","text":"","title":"Trabajando con el historial"},{"location":"usobasico/#observando-los-cambios","text":"Con la orden git log podemos ver todos los cambios que hemos hecho: $ git log commit fd4da946326fbe8b24e89282ad25a71721bf40f6 Author: Sergio G\u00f3mez <sergio@uco.es> Date: Sun Jun 16 12:51:01 2013 +0200 Se a\u00f1ade un comentario al cambio del valor por defecto commit 3283e0d306c8d42d55ffcb64e456f10510df8177 Author: Sergio G\u00f3mez <sergio@uco.es> Date: Sun Jun 16 12:50:00 2013 +0200 Se a\u00f1ade un par\u00e1metro por defecto commit efc252e11939351505a426a6e1aa5bb7dc1dd7c0 Author: Sergio G\u00f3mez <sergio@uco.es> Date: Sun Jun 16 12:13:26 2013 +0200 Parametrizaci\u00f3n del programa commit e19f2c1701069d9d1159e9ee21acaa1bbc47d264 Author: Sergio G\u00f3mez <sergio@uco.es> Date: Sun Jun 16 11:55:23 2013 +0200 Creaci\u00f3n del proyecto Tambi\u00e9n es posible ver versiones abreviadas o limitadas, dependiendo de los par\u00e1metros: $ git log --oneline fd4da94 Se a\u00f1ade un comentario al cambio del valor por defecto 3283e0d Se a\u00f1ade un par\u00e1metro por defecto efc252e Parametrizaci\u00f3n del programa e19f2c1 Creaci\u00f3n del proyecto git log --oneline --max-count=2 git log --oneline --since='5 minutes ago' git log --oneline --until='5 minutes ago' git log --oneline --author=sergio git log --oneline --all Una versi\u00f3n muy \u00fatil de git log es la siguiente, pues nos permite ver en que lugares est\u00e1 master y HEAD, entre otras cosas: $ git log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (HEAD, master) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez]","title":"Observando los cambios"},{"location":"usobasico/#crear-alias","text":"Como estas \u00f3rdenes son demasiado largas, Git nos permite crear alias para crear nuevas \u00f3rdenes parametrizadas. Para ello podemos configurar nuestro entorno con la orden git config de la siguiente manera: git config --global alias.hist \"log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short\" Example Puedes configurar incluso alias para abreviar comandos. Algunos ejemplos de alias \u00fatiles: git config --global alias.br branch git config --global alias.co checkout git config --global alias.ci commit git config --global alias.st \"status -u\" git config --global alias.cane \"commit --amend --no-edit\"","title":"Crear alias"},{"location":"usobasico/#recuperando-versiones-anteriores","text":"Cada cambio es etiquetado por un hash, para poder regresar a ese momento del estado del proyecto se usa la orden git checkout . $ git checkout e19f2c1 Note: checking out 'e19f2c1'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b new_branch_name HEAD is now at e19f2c1... Creaci\u00f3n del proyecto $ cat hola.php <?php echo \"Hello, World\\n\"; El aviso que nos sale nos indica que estamos en un estado donde no trabajamos en ninguna rama concreta. Eso significa que los cambios que hagamos podr\u00edan \"perderse\" porque si no son guardados en una nueva rama, en principio no podr\u00edamos volver a recuperarlos. Hay que pensar que Git es como un \u00e1rbol donde un nodo tiene informaci\u00f3n de su nodo padre, no de sus nodos hijos, con lo que siempre necesitar\u00edamos informaci\u00f3n de d\u00f3nde se encuentran los nodos finales o de otra manera no podr\u00edamos acceder a ellos.","title":"Recuperando versiones anteriores"},{"location":"usobasico/#volver-a-la-ultima-version-de-la-rama-master","text":"Usamos git checkout indicando el nombre de la rama: $ git checkout master Previous HEAD position was e19f2c1... Creaci\u00f3n del proyecto","title":"Volver a la \u00faltima versi\u00f3n de la rama master."},{"location":"usobasico/#etiquetando-versiones","text":"Para poder recuperar versiones concretas en la historia del repositorio, podemos etiquetarlas, lo cual es m\u00e1s facil que usar un hash. Para eso usaremos la orden git tag . $ git tag v1 Ahora vamos a etiquetar la versi\u00f3n inmediatamente anterior como v1-beta. Para ello podemos usar los modificadores ^ o ~ que nos llevar\u00e1n a un ancestro determinado. Las siguientes dos \u00f3rdenes son equivalentes: $ git checkout v1^ $ git checkout v1~1 $ git tag v1-beta Si ejecutamos la orden sin par\u00e1metros nos mostrar\u00e1 todas las etiquetas existentes. $ git tag v1 v1-beta Y para verlas en el historial: $ git hist master --all * fd4da94 2013-06-16 | Se a\u00f1ade un comentario al cambio del valor por defecto (tag: v1, master) [Sergio G\u00f3mez] * 3283e0d 2013-06-16 | Se a\u00f1ade un par\u00e1metro por defecto (HEAD, tag: v1-beta) [Sergio G\u00f3mez] * efc252e 2013-06-16 | Parametrizaci\u00f3n del programa [Sergio G\u00f3mez] * e19f2c1 2013-06-16 | Creaci\u00f3n del proyecto [Sergio G\u00f3mez]","title":"Etiquetando versiones"},{"location":"usobasico/#borrar-etiquetas","text":"Para borrar etiquetas: git tag -d nombre_etiqueta","title":"Borrar etiquetas"},{"location":"usobasico/#visualizar-cambios","text":"Para ver los cambios que se han realizado en el c\u00f3digo usamos la orden git diff . La orden sin especificar nada m\u00e1s, mostrar\u00e1 los cambios que no han sido a\u00f1adidos a\u00fan, es decir, todos los cambios que se han hecho antes de usar la orden git add . Despu\u00e9s se puede indicar un par\u00e1metro y dar\u00e1 los cambios entre la versi\u00f3n indicada y el estado actual. O para comparar dos versiones entre s\u00ed, se indica la m\u00e1s antigua y la m\u00e1s nueva. Ejemplo: $ git diff v1-beta v1 diff --git a/hola.php b/hola.php index a31e01f..25a35c0 100644 --- a/hola.php +++ b/hola.php @@ -1,3 +1,4 @@ <?php +// El nombre por defecto es Mundo $nombre = isset($argv[1]) ? $argv[1] : \"Mundo\"; @print \"Hola, {$nombre}\\n\";","title":"Visualizar cambios"},{"location":"wordpress/","text":"Levantar un WordPress con Docker Para crear un blog con WordPress necesitamos tener una base de datos d\u00f3nde almacenar las entradas. As\u00ed que empezaremos cre\u00e1ndola y despu\u00e9s crearemos el contenedor de nuestro blog . Crear un contenedor con MariaDB . WordPress soporta los motores relaciones MySQL y MariaDB . Usaremos este \u00faltimo. Example Vamos a crear nuestra base de datos usando este volumen. docker run -d --name wordpress-db \\ --mount source = wordpress-db,target = /var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD = secret \\ -e MYSQL_DATABASE = wordpress \\ -e MYSQL_USER = manager \\ -e MYSQL_PASSWORD = secret mariadb:10.3.9 La imagen se descargar\u00e1, si no lo estaba ya, y se iniciar\u00e1 nuestro contenedor de MariaDB : $ docker run -d --name wordpress-db \\ --mount source = wordpress-db,target = /var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD = secret \\ -e MYSQL_DATABASE = wordpress \\ -e MYSQL_USER = manager \\ -e MYSQL_PASSWORD = secret mariadb:10.3.9 Unable to find image 'mariadb:10.3.9' locally 10.3.9: Pulling from library/mariadb 124c757242f8: Pull complete 9d866f8bde2a: Pull complete fa3f2f277e67: Pull complete 398d32b153e8: Pull complete afde35469481: Pull complete 31f2ae82b3e3: Pull complete 3eeaf7e45ea6: Pull complete 716982328e17: Pull complete 34ce605c9036: Pull complete 4502ed9073c0: Pull complete 2afafbdf5a96: Pull complete 43d52b11dd31: Pull complete 30c7b70556f3: Pull complete 8b1b39f2f89a: Pull complete 41480b9319d7: Pull complete Digest: sha256:b7894bd08e5752acdd41fea654cb89467c99e67b8293975bb5d787b27e66ce1a Status: Downloaded newer image for mariadb:10.3.9 30634831d17108aa553a5774e27f398760bdbdf32debc3179843e73aa5957956 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 30634831d171 mariadb:10.3.9 \"docker-entrypoint.s\u2026\" 20 seconds ago Up 16 seconds 3306/tcp wordpress-db El principal cambio en docker run con respecto a la \u00faltima vez es que no hemos usado -p (el par\u00e1metro para publicar puertos) y hemos a\u00f1adido el par\u00e1metro -d . Lo primero que habremos notado es que el contenedor ya no se queda en primer plano. El par\u00e1metro -d indica que debe ejecutarse como un proceso en segundo plano. As\u00ed no podremos pararlo por accidente con Control+C . Lo segundo es que vemos que el contenedor usa un puerto, el 3306/tcp , pero no est\u00e1 linkado a la m\u00e1quina anfitri\u00f3n. No tenemos forma de acceder a la base de datos directamente. Nuestra intenci\u00f3n es que solo el contenedor de WordPress pueda acceder. Luego una serie de par\u00e1metros -e que nos permite configurar nuestra base de datos. Info Los contenedores se configuran a trav\u00e9s de variables de ambiente, que podemos configurar con el par\u00e1metro -e que vemos en la orden anterior. Gracias a ellos hemos creado una base de datos, un usuario y configurado las contrase\u00f1as. Se recomienda buscar en el registro de Docker la imagen oficial de MariaDB para entender el uso de los par\u00e1metros. Por \u00faltimo, el par\u00e1metro --mount nos permite enlazar el volumen que creamos en el paso anterior con el directorio /var/lib/mysql del contenedor. Ese directorio es donde se guardan los datos de MariaDB . Eso significa que si borramos el contenedor, o actualizamos el contenedor a una nueva versi\u00f3n, no perderemos los datos porque ya no se encuentran en \u00e9l, si no en el volumen. Solo lo perder\u00edamos si borramos expl\u00edcitamente el volumen. Warning Cada contendor que usemos tendr\u00e1 uno o varios directorios donde se deben guardar los datos no vol\u00e1tiles. Nos corresponde a nosotros conocer la herramienta y saber de qu\u00e9 directorios se tratan. Usualmente est\u00e1n en la documentaci\u00f3n del contenedor, pero no siempre. Info El par\u00e1metro --mount se empez\u00f3 a utilizar desde la versi\u00f3n 17.06 para contenedores independientes (los que no pertenecen a un enjambre o swarm ). Los que conozcan Docker de versiones m\u00e1s antiguas estar\u00e1n m\u00e1s acostumbrados a usar el par\u00e1metro --volume que hace algo similar. Sin embargo la documentaci\u00f3n aconseja usar ya --mount , sobre todo para nuevos usuarios. Nosotros somos muy obedientes as\u00ed que en este taller usaremos --mount . Creando nuestro blog Vamos a crear otra vez nuestro contenedor de WordPress , pero esta vez vamos a conectarlo con nuestra base de datos. Adem\u00e1s, queremos poder editar los ficheros de las plantillas, por si tenemos que modificar algo, as\u00ed que necesitaremos montar el directorio del contenedor donde est\u00e1 instalado WordPress con nuestra cuenta de usuario en la m\u00e1quina anfitri\u00f3n. Example Vamos a crear el espacio de trabajo: mkdir -p ~/Sites/wordpress/target && cd ~/Sites/wordpress Example Y dentro de este directorio arrancamos el contenedor: docker run -d --name wordpress \\ --link wordpress-db:mysql \\ --mount type = bind,source = \" $( pwd ) \" /target,target = /var/www/html \\ -e WORDPRESS_DB_USER = manager \\ -e WORDPRESS_DB_PASSWORD = secret \\ -p 8080 :80 \\ wordpress:4.9.8 Cuando termine la ejecuci\u00f3n, si accedemos a la direcci\u00f3n http://localhost:8080/ , ahora s\u00ed podremos acabar el proceso de instalaci\u00f3n de nuestro WordPress. Si listamos el directorio target comprobaremos que tenemos todos los archivos de instalaci\u00f3n accesibles desde el directorio anfitri\u00f3n. Note Ejercicios: Para los contenedores, tanto el de WordPress como el MariaDB . Borra ambos. Vuelve a crearlos y mira como ya no es necesario volver a instalar WordPress . Vuelve a borrarlos y borra tambi\u00e9n el volumen. Vuelve a crear el volumen y los contenedores y comprueba que ahora s\u00ed hay que volver a instalar WordPress .","title":"Levantar un WordPress con Docker"},{"location":"wordpress/#levantar-un-wordpress-con-docker","text":"Para crear un blog con WordPress necesitamos tener una base de datos d\u00f3nde almacenar las entradas. As\u00ed que empezaremos cre\u00e1ndola y despu\u00e9s crearemos el contenedor de nuestro blog .","title":"Levantar un WordPress con Docker"},{"location":"wordpress/#crear-un-contenedor-con-mariadb","text":"WordPress soporta los motores relaciones MySQL y MariaDB . Usaremos este \u00faltimo. Example Vamos a crear nuestra base de datos usando este volumen. docker run -d --name wordpress-db \\ --mount source = wordpress-db,target = /var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD = secret \\ -e MYSQL_DATABASE = wordpress \\ -e MYSQL_USER = manager \\ -e MYSQL_PASSWORD = secret mariadb:10.3.9 La imagen se descargar\u00e1, si no lo estaba ya, y se iniciar\u00e1 nuestro contenedor de MariaDB : $ docker run -d --name wordpress-db \\ --mount source = wordpress-db,target = /var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD = secret \\ -e MYSQL_DATABASE = wordpress \\ -e MYSQL_USER = manager \\ -e MYSQL_PASSWORD = secret mariadb:10.3.9 Unable to find image 'mariadb:10.3.9' locally 10.3.9: Pulling from library/mariadb 124c757242f8: Pull complete 9d866f8bde2a: Pull complete fa3f2f277e67: Pull complete 398d32b153e8: Pull complete afde35469481: Pull complete 31f2ae82b3e3: Pull complete 3eeaf7e45ea6: Pull complete 716982328e17: Pull complete 34ce605c9036: Pull complete 4502ed9073c0: Pull complete 2afafbdf5a96: Pull complete 43d52b11dd31: Pull complete 30c7b70556f3: Pull complete 8b1b39f2f89a: Pull complete 41480b9319d7: Pull complete Digest: sha256:b7894bd08e5752acdd41fea654cb89467c99e67b8293975bb5d787b27e66ce1a Status: Downloaded newer image for mariadb:10.3.9 30634831d17108aa553a5774e27f398760bdbdf32debc3179843e73aa5957956 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 30634831d171 mariadb:10.3.9 \"docker-entrypoint.s\u2026\" 20 seconds ago Up 16 seconds 3306/tcp wordpress-db El principal cambio en docker run con respecto a la \u00faltima vez es que no hemos usado -p (el par\u00e1metro para publicar puertos) y hemos a\u00f1adido el par\u00e1metro -d . Lo primero que habremos notado es que el contenedor ya no se queda en primer plano. El par\u00e1metro -d indica que debe ejecutarse como un proceso en segundo plano. As\u00ed no podremos pararlo por accidente con Control+C . Lo segundo es que vemos que el contenedor usa un puerto, el 3306/tcp , pero no est\u00e1 linkado a la m\u00e1quina anfitri\u00f3n. No tenemos forma de acceder a la base de datos directamente. Nuestra intenci\u00f3n es que solo el contenedor de WordPress pueda acceder. Luego una serie de par\u00e1metros -e que nos permite configurar nuestra base de datos. Info Los contenedores se configuran a trav\u00e9s de variables de ambiente, que podemos configurar con el par\u00e1metro -e que vemos en la orden anterior. Gracias a ellos hemos creado una base de datos, un usuario y configurado las contrase\u00f1as. Se recomienda buscar en el registro de Docker la imagen oficial de MariaDB para entender el uso de los par\u00e1metros. Por \u00faltimo, el par\u00e1metro --mount nos permite enlazar el volumen que creamos en el paso anterior con el directorio /var/lib/mysql del contenedor. Ese directorio es donde se guardan los datos de MariaDB . Eso significa que si borramos el contenedor, o actualizamos el contenedor a una nueva versi\u00f3n, no perderemos los datos porque ya no se encuentran en \u00e9l, si no en el volumen. Solo lo perder\u00edamos si borramos expl\u00edcitamente el volumen. Warning Cada contendor que usemos tendr\u00e1 uno o varios directorios donde se deben guardar los datos no vol\u00e1tiles. Nos corresponde a nosotros conocer la herramienta y saber de qu\u00e9 directorios se tratan. Usualmente est\u00e1n en la documentaci\u00f3n del contenedor, pero no siempre. Info El par\u00e1metro --mount se empez\u00f3 a utilizar desde la versi\u00f3n 17.06 para contenedores independientes (los que no pertenecen a un enjambre o swarm ). Los que conozcan Docker de versiones m\u00e1s antiguas estar\u00e1n m\u00e1s acostumbrados a usar el par\u00e1metro --volume que hace algo similar. Sin embargo la documentaci\u00f3n aconseja usar ya --mount , sobre todo para nuevos usuarios. Nosotros somos muy obedientes as\u00ed que en este taller usaremos --mount .","title":"Crear un contenedor con MariaDB."},{"location":"wordpress/#creando-nuestro-blog","text":"Vamos a crear otra vez nuestro contenedor de WordPress , pero esta vez vamos a conectarlo con nuestra base de datos. Adem\u00e1s, queremos poder editar los ficheros de las plantillas, por si tenemos que modificar algo, as\u00ed que necesitaremos montar el directorio del contenedor donde est\u00e1 instalado WordPress con nuestra cuenta de usuario en la m\u00e1quina anfitri\u00f3n. Example Vamos a crear el espacio de trabajo: mkdir -p ~/Sites/wordpress/target && cd ~/Sites/wordpress Example Y dentro de este directorio arrancamos el contenedor: docker run -d --name wordpress \\ --link wordpress-db:mysql \\ --mount type = bind,source = \" $( pwd ) \" /target,target = /var/www/html \\ -e WORDPRESS_DB_USER = manager \\ -e WORDPRESS_DB_PASSWORD = secret \\ -p 8080 :80 \\ wordpress:4.9.8 Cuando termine la ejecuci\u00f3n, si accedemos a la direcci\u00f3n http://localhost:8080/ , ahora s\u00ed podremos acabar el proceso de instalaci\u00f3n de nuestro WordPress. Si listamos el directorio target comprobaremos que tenemos todos los archivos de instalaci\u00f3n accesibles desde el directorio anfitri\u00f3n. Note Ejercicios: Para los contenedores, tanto el de WordPress como el MariaDB . Borra ambos. Vuelve a crearlos y mira como ya no es necesario volver a instalar WordPress . Vuelve a borrarlos y borra tambi\u00e9n el volumen. Vuelve a crear el volumen y los contenedores y comprueba que ahora s\u00ed hay que volver a instalar WordPress .","title":"Creando nuestro blog"}]}